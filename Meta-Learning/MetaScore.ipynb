{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> MetaScore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 数据加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import hydra\n",
    "import higher\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch_geometric.data import Batch\n",
    "\n",
    "# predefined model\n",
    "from models.gatedgcn import GenScore_GGCN\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './dataset/pdbbind/v2020_train_dict.pt'\n",
    "data = torch.load(data_path,weights_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看一下数据组织格式：所有数据是一个大的嵌套字典，它的键是`pdb_id`，值是一个字典，其键`prot`,`lig`,`label`分别对应了蛋白质图、配体图和亲和力数据:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prot': Data(x=[83, 41], edge_index=[2, 2014], edge_attr=[2014, 5], pos=[83, 24, 3]),\n",
       " 'lig': Data(x=[33, 41], edge_index=[2, 68], edge_attr=[68, 10], pos=[33, 3]),\n",
       " 'label': np.str_('6.4')}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['10gs']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "与传统机器学习不同，元学习训练的数据单元是任务而非单个数据点，因此需要对pdbbind中的蛋白-配体对数据进行任务划分，在MetaScore中表现为对蛋白质结构相似度进行层次聚类，以聚类得到的簇为单个任务进行训练。对应的，`DataLoader`的写法也相应的改变：\n",
    "\n",
    "- `DataLoader`一次从所有任务中采样`batch_size`个任务\n",
    "- 每个任务应当采样`num_classes_per_task`个类别，即选取不同的聚类用于构建一个任务\n",
    "- 每个聚类中随机采样`k-shot`个样本用于构建支持集，`q-query`个样本用于构建查询集\n",
    "- 将一个任务中所有聚类中被选中的支持集样本和查询集样本聚合后，即可视为`meta-batch`中的一个任务\n",
    "- 考虑到显存问题，无法再将`meta-batch`中的任务也做一遍聚合然后一次算完（显存会爆炸），只能在一个批次中逐任务累计梯度\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "聚类的结果已经拿到，加载一下:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PDB_ID</th>\n",
       "      <th>Cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2z7i</td>\n",
       "      <td>257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3i7g</td>\n",
       "      <td>333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2v88</td>\n",
       "      <td>197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3wav</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6qlu</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  PDB_ID  Cluster\n",
       "0   2z7i      257\n",
       "1   3i7g      333\n",
       "2   2v88      197\n",
       "3   3wav      150\n",
       "4   6qlu        0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_path = './dataset/cluster/clustering_results.csv'\n",
    "cluster_df = pd.read_csv(cluster_path)\n",
    "cluster_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "而后我们根据聚类的结果来构建一个新的数据字典，在这个字典中，键应当改为`Cluster`，值应为列表，每个列表的元素代表位于该聚类下的一个数据点，以原数据中字典的格式存储（在原数据字典的基础上额外增加了`pdb_id`作为数据标识符）:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19145/19145 [00:09<00:00, 1936.29it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'prot': Data(x=[38, 41], edge_index=[2, 794], edge_attr=[794, 5], pos=[38, 24, 3]),\n",
       "  'lig': Data(x=[35, 41], edge_index=[2, 72], edge_attr=[72, 10], pos=[35, 3]),\n",
       "  'label': np.str_('5.7'),\n",
       "  'pdb_id': np.str_('4z0u')},\n",
       " {'prot': Data(x=[83, 41], edge_index=[2, 2264], edge_attr=[2264, 5], pos=[83, 24, 3]),\n",
       "  'lig': Data(x=[21, 41], edge_index=[2, 46], edge_attr=[46, 10], pos=[21, 3]),\n",
       "  'label': np.str_('6.0'),\n",
       "  'pdb_id': np.str_('3dxm')}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_with_cluster = {}\n",
    "for pdb_id, value in tqdm.tqdm(data.items()):\n",
    "    value['pdb_id'] = pdb_id\n",
    "    row = cluster_df[cluster_df['PDB_ID'] == pdb_id]\n",
    "    cluster_id = row['Cluster'].values[0]\n",
    "    if f'class_{cluster_id}' not in dict_with_cluster:\n",
    "        dict_with_cluster[f'class_{cluster_id}'] = []\n",
    "    dict_with_cluster[f'class_{cluster_id}'].append(value)\n",
    "dict_with_cluster['class_1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在此结果基础上，考虑划分元学习的`meta-train`和`meta-test`集，在`MetaScore`中，考虑到降低实现难度，该集的划分以每个聚类中数据点的数量划分，具体的，不少于17个数据点的聚类会被分类为训练集，不少于7个数据点的聚类分类为测试集，其他聚类暂时丢弃，或考虑用于模型`zeroshot`性能测试（这部分聚类数量很多但是数据量很小，可以丢弃而不造成严重影响）。为此可以写一个`dict_with_split`，存储每个聚类属于的集合:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 186 classes, Val: 153 classes, Test: 808 classes\n",
      "data length of each split {'train': np.int64(15810), 'val': np.int64(1574), 'test': np.int64(1761)}\n"
     ]
    }
   ],
   "source": [
    "# split the data into train, val, test according to the amount of data in each class\n",
    "train_set_min_size = 17\n",
    "val_set_min_size = 7\n",
    "dict_with_split = {'train': [], 'val': [], 'test': []}\n",
    "for class_idx, value in dict_with_cluster.items():\n",
    "    data_count = len(value)\n",
    "    if data_count >= train_set_min_size:\n",
    "        dict_with_split['train'].append(class_idx)\n",
    "    elif data_count >= val_set_min_size and data_count < train_set_min_size:\n",
    "        dict_with_split['val'].append(class_idx)\n",
    "    else:\n",
    "        dict_with_split['test'].append(class_idx)\n",
    "print(f\"Train: {len(dict_with_split['train'])} classes, Val: {len(dict_with_split['val'])} classes, Test: {len(dict_with_split['test'])} classes\")\n",
    "\n",
    "# get the size of each split\n",
    "dataset_size_dict = {\n",
    "    \"train\": {key: len(dict_with_cluster[key]) for key in dict_with_split[\"train\"]},\n",
    "    \"val\": {key: len(dict_with_cluster[key]) for key in dict_with_split[\"val\"]},\n",
    "    \"test\": {key: len(dict_with_cluster[key]) for key in dict_with_split[\"test\"]},\n",
    "}\n",
    "data_length = {name: np.sum(np.array(list(dataset_size_dict[name].values()))) for name in dict_with_split.keys()}\n",
    "print(\"data length of each split\", data_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "到此为止完成了数据单元（单个聚类）的数据集划分，接下来要考虑如何使用数据单元来构建`meta-batch`，这涉及到:\n",
    "- 任务构建逻辑，如何在所有聚类中选择`batch_size`个聚类构建一个任务\n",
    "- 聚类中的样本采样逻辑，如何在一个聚类中选取样本构建支持集和查询集\n",
    "这两点可以通过一个`TaskSampler`类来完成:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskSampler:\n",
    "    \"\"\"\n",
    "    Samples tasks based on specified rules.\n",
    "    Can select classes uniformly or based on class size using softmax probability.\n",
    "    Samples support/query items uniformly from within selected classes.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 dataset_size_dict: dict,\n",
    "                 train_num_classes_per_set: int,\n",
    "                 val_num_classes_per_set: int,\n",
    "                 train_num_support: int,\n",
    "                 val_num_support: int,\n",
    "                 train_num_query: int,\n",
    "                 val_num_query: int,\n",
    "                 sampling_rule: str = \"uniform\"): # Added sampling_rule parameter\n",
    "        \"\"\"\n",
    "        Initializes the TaskSampler.\n",
    "\n",
    "        Args:\n",
    "            dataset_size_dict: Dictionary mapping split names ('train', 'val') to dictionaries\n",
    "                               of {class_name: class_size}.\n",
    "            train_num_classes_per_set: Number of classes per task in the training set.\n",
    "            val_num_classes_per_set: Number of classes per task in the validation set.\n",
    "            train_num_support: Number of support examples per class in the training set.\n",
    "            val_num_support: Number of support examples per class in the validation set.\n",
    "            train_num_query: Number of query examples per class in the training set.\n",
    "            val_num_query: Number of query examples per class in the validation set.\n",
    "            sampling_rule: Rule for sampling classes ('uniform' or 'softmax'). Defaults to 'uniform'.\n",
    "        \"\"\"\n",
    "        self.dataset_size_dict = dataset_size_dict\n",
    "        self.sampling_rule = sampling_rule # Store the rule, for now only uniform sampling is implemented\n",
    "        \n",
    "        # determine the number of classes to sample for each task\n",
    "        self.num_classes_per_set = {\"train\": train_num_classes_per_set, \"val\": val_num_classes_per_set}\n",
    "        self.num_support = {\"train\": train_num_support, \"val\": val_num_support}\n",
    "        self.num_query = {\"train\": train_num_query, \"val\": val_num_query}\n",
    "\n",
    "    def sample_task(self, dataset_name: str, seed: int) -> dict:\n",
    "        \"\"\"\n",
    "        Samples a task based on the configured sampling rule.\n",
    "\n",
    "        Args:\n",
    "            dataset_name: 'train' or 'val'.\n",
    "            seed: Random seed for reproducibility.\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with sampled class names and their support/query indices.\n",
    "            Example: {'class_A': {'support_indices': [...], 'query_indices': [...]}, ...}\n",
    "        \"\"\"\n",
    "        rng = np.random.RandomState(seed)\n",
    "        num_classes = self.num_classes_per_set[dataset_name]\n",
    "        num_support = self.num_support[dataset_name]\n",
    "        num_query = self.num_query[dataset_name]\n",
    "        num_samples_per_class = num_support + num_query\n",
    "        available_classes_dict = self.dataset_size_dict[dataset_name]\n",
    "        available_class_names = list(available_classes_dict.keys())\n",
    "        if len(available_class_names) < num_classes:\n",
    "            raise ValueError(\n",
    "                f\"Not enough classes in {dataset_name} split ({len(available_class_names)}) \"\n",
    "                f\"to sample {num_classes} classes.\"\n",
    "            )\n",
    "        \n",
    "        # select classes based on the sampling rule\n",
    "        assert self.sampling_rule == \"uniform\"\n",
    "        selected_classes = rng.choice(\n",
    "            available_class_names,\n",
    "            size=num_classes,\n",
    "            replace=False,\n",
    "        )\n",
    "        \n",
    "        task_info = {}\n",
    "        for class_name in selected_classes:\n",
    "            class_size = available_classes_dict[class_name] \n",
    "            if class_size < num_samples_per_class:\n",
    "                 raise ValueError(\n",
    "                    f\"Class '{class_name}' in '{dataset_name}' has only {class_size} samples, \"\n",
    "                    f\"but {num_samples_per_class} are required for support+query.\"\n",
    "                )\n",
    "            # sample indices from 0 to class_size - 1\n",
    "            selected_indices = rng.choice(class_size, size=num_samples_per_class, replace=False)\n",
    "            task_info[class_name] = {\n",
    "                'support_indices': selected_indices[:num_support],\n",
    "                'query_indices': selected_indices[num_support : num_support + num_query]\n",
    "            }\n",
    "        return task_info\n",
    "\n",
    "task_sampler = TaskSampler(\n",
    "    dataset_size_dict=dataset_size_dict,\n",
    "    train_num_classes_per_set=5,\n",
    "    val_num_classes_per_set=16,\n",
    "    train_num_support=5,\n",
    "    val_num_support=3,\n",
    "    train_num_query=5,\n",
    "    val_num_query=3,\n",
    "    sampling_rule=\"uniform\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这是一个非常简单的采样实现（也是`MAML`论文中的实现），基于这个`TaskSampler`尝试采样一批任务:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 5, 3, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_support_set_data = [];train_query_set_data = [];val_support_set_data = [];val_query_set_data = []\n",
    "sampled_train_task_info = task_sampler.sample_task(dataset_name='train', seed=1227)\n",
    "sampled_val_task_info = task_sampler.sample_task(dataset_name='val', seed=1227)\n",
    "\n",
    "# sample the data for train set\n",
    "for sampled_train_class_name in sampled_train_task_info.keys():\n",
    "    train_class_data_list = dict_with_cluster[sampled_train_class_name]\n",
    "    support_indices = sampled_train_task_info[sampled_train_class_name]['support_indices']\n",
    "    query_indices = sampled_train_task_info[sampled_train_class_name]['query_indices']\n",
    "    per_class_support_data = [train_class_data_list[i] for i in support_indices]\n",
    "    train_support_set_data.append(per_class_support_data)\n",
    "    per_class_query_data = [train_class_data_list[i] for i in query_indices]\n",
    "    train_query_set_data.append(per_class_query_data)\n",
    "\n",
    "# do the same for val set\n",
    "for sampled_val_class_name in sampled_val_task_info.keys():\n",
    "    val_class_data_list = dict_with_cluster[sampled_val_class_name]\n",
    "    support_indices = sampled_val_task_info[sampled_val_class_name]['support_indices']\n",
    "    query_indices = sampled_val_task_info[sampled_val_class_name]['query_indices']\n",
    "    per_class_support_data = [val_class_data_list[i] for i in support_indices]\n",
    "    val_support_set_data.append(per_class_support_data)\n",
    "    per_class_query_data = [val_class_data_list[i] for i in query_indices]\n",
    "    val_query_set_data.append(per_class_query_data)\n",
    "\n",
    "# thus a task is sampled, data is organized as follows:\n",
    "# (5,5,16,16), refer to the number of classes sampled \n",
    "len(train_support_set_data),len(train_query_set_data),len(val_support_set_data),len(val_query_set_data)\n",
    "# (5,5,3,3), refer to the number of support and query samples sampled for each class\n",
    "len(train_support_set_data[0]),len(train_query_set_data[0]),len(val_support_set_data[0]),len(val_query_set_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目前采样得到的任务组织比较混乱，考虑用字典使任务变得更加清晰，可以定义一个`transform`函数实现这一点，同时，该函数可以用于定义`__getitem__`方法，即获得一个数据单元，在此我们通过复制任务来得到一个`meta-batch`方便后续演示:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(data):\n",
    "    \"\"\"Transforms the raw data list from get_set into separate lists.\"\"\"\n",
    "    pdb_ids = [];labels = [];prots = [];ligs = []\n",
    "    for class_data_list in data:\n",
    "        for per_pdb_data_dict in class_data_list:\n",
    "            pdb_ids.append(per_pdb_data_dict['pdb_id'])\n",
    "            labels.append(float(per_pdb_data_dict['label']))\n",
    "            prots.append(per_pdb_data_dict['prot'])\n",
    "            ligs.append(per_pdb_data_dict['lig'])\n",
    "    return {\n",
    "        'pdb_ids': pdb_ids,\n",
    "        'labels': labels,\n",
    "        'prots': prots,\n",
    "        'ligs': ligs,\n",
    "    }\n",
    "\n",
    "# (25,25,48,48), 25 = 5*5, 48 = 16*3\n",
    "# train_support_set_data['pdb_ids'].__len__(),train_query_set_data['pdb_ids'].__len__(),val_support_set_data['pdb_ids'].__len__(),val_query_set_data['pdb_ids'].__len__()\n",
    "train_support_set_data = transform(train_support_set_data);train_query_set_data = transform(train_query_set_data)\n",
    "val_support_set_data = transform(val_support_set_data);val_query_set_data = transform(val_query_set_data)\n",
    "train_task = (train_support_set_data,train_query_set_data,1227); val_task = (val_support_set_data,val_query_set_data,1227)\n",
    "\n",
    "# simply repeat the task for 4 times to get a batch\n",
    "train_batch = [train_task for _ in range(4)];val_batch = [val_task for _ in range(4)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了让数据格式更加工整，考虑写一个自定义的`task_collate_fn`来实现这一点，最后整合好的`meta-batch`是一个长度为`batch_size`的列表，列表的每个元素是一个字典:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Collate function for creating task batches with graph data.\n",
    "    Processes and collates support and query sets into batches.\n",
    "    Input `batch`: A list of tuples, where each tuple is (support_set_dict, query_set_dict, seed)\n",
    "                    returned by FewShotLearningDatasetParallel.__getitem__.\n",
    "    \"\"\"\n",
    "    all_tasks = []\n",
    "    \n",
    "    for task in batch:\n",
    "        # Unpack the task tuple\n",
    "        support_set_dict, query_set_dict, seed = task\n",
    "\n",
    "        # Process Support Set\n",
    "        support_ligs = support_set_dict['ligs'] # List of ligand graph objects\n",
    "        support_prots = support_set_dict['prots'] # List of protein graph objects\n",
    "        support_labels = support_set_dict['labels']\n",
    "        support_pdb_ids = support_set_dict['pdb_ids']\n",
    "        \n",
    "        query_ligs = query_set_dict['ligs'] # List of ligand graph objects\n",
    "        query_prots = query_set_dict['prots'] # List of protein graph objects\n",
    "        query_labels = query_set_dict['labels']\n",
    "        query_pdb_ids = query_set_dict['pdb_ids']\n",
    "        \n",
    "        task_data = {\n",
    "            \"support\": {\n",
    "                \"prots\": Batch.from_data_list(support_prots),\n",
    "                \"ligs\": Batch.from_data_list(support_ligs),\n",
    "                \"labels\": torch.tensor(support_labels, dtype=torch.float).reshape(-1),\n",
    "                \"pdb_ids\": support_pdb_ids,\n",
    "            },\n",
    "            \"query\": {\n",
    "                \"prots\": Batch.from_data_list(query_prots),\n",
    "                \"ligs\": Batch.from_data_list(query_ligs),\n",
    "                \"labels\": torch.tensor(query_labels, dtype=torch.float).reshape(-1),\n",
    "                \"pdb_ids\": query_pdb_ids,\n",
    "            },\n",
    "            \"seed\": seed,\n",
    "        }\n",
    "        all_tasks.append(task_data)\n",
    "    return all_tasks\n",
    "\n",
    "# successfully get a meta-batch for training and validation\n",
    "train_batch = task_collate_fn(train_batch);val_batch = task_collate_fn(val_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 基础模型的定义\n",
    "\n",
    "该部分定义了神经网络的具体架构，实现与传统机器学习并无不同，后续考虑收录一些基于`MAML`设计模型的技巧在里面，目前相关的工作有:\n",
    "\n",
    "- [MetaMolGen: A Neural Graph Motif Generation Model for De Novo Molecular Design](https://arxiv.org/abs/2504.15587)\n",
    "- [Meta-MGNN: Few-Shot Graph Learning for Molecular Property Prediction](https://arxiv.org/abs/2102.07916)\n",
    "  \n",
    "该部分工作持续收录中~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此处我们就用预先写好的`./models/gatedgcn.py`中定义好的权重以及参数`./models/gatedgcn.pt`进行模型初始化，并且简单测试一下基础模型是否能够工作:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenScore_GGCN(\n",
       "  (ligand_model): GatedGCN(\n",
       "    (node_encoder): Linear(in_features=41, out_features=128, bias=True)\n",
       "    (edge_encoder): Linear(in_features=10, out_features=128, bias=True)\n",
       "    (gt_block): ModuleList(\n",
       "      (0-5): 6 x GatedGCNLayer()\n",
       "    )\n",
       "  )\n",
       "  (target_model): GatedGCN(\n",
       "    (node_encoder): Linear(in_features=41, out_features=128, bias=True)\n",
       "    (edge_encoder): Linear(in_features=5, out_features=128, bias=True)\n",
       "    (gt_block): ModuleList(\n",
       "      (0-5): 6 x GatedGCNLayer()\n",
       "    )\n",
       "  )\n",
       "  (MLP): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ELU(alpha=1.0)\n",
       "    (3): Dropout(p=0.15, inplace=False)\n",
       "  )\n",
       "  (z_pi): Linear(in_features=128, out_features=10, bias=True)\n",
       "  (z_sigma): Linear(in_features=128, out_features=10, bias=True)\n",
       "  (z_mu): Linear(in_features=128, out_features=10, bias=True)\n",
       "  (atom_types): Linear(in_features=128, out_features=17, bias=True)\n",
       "  (bond_types): Linear(in_features=256, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load a pretrained meta model\n",
    "state_dict = torch.load('./models/gatedgcn.pt',weights_only=False,map_location='cpu')\n",
    "meta_model_state_dict = state_dict['network'] # meta_model_state_dict, with a 'regressor.' prefix\n",
    "\n",
    "# strip the prefix, ensuring the new key is not empty\n",
    "model_state_dict = {\n",
    "    new_key: v \n",
    "    for k, v in meta_model_state_dict.items() \n",
    "    if k.startswith('regressor.') and (new_key := k[len('regressor.'):])\n",
    "}\n",
    "base_model = GenScore_GGCN()\n",
    "base_model.load_state_dict(model_state_dict)\n",
    "base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Gaussian Mixture Weights: torch.Size([64953, 10])\n",
      "Predicted Gaussian Mixture Standard Deviations: torch.Size([64953, 10])\n",
      "Predicted Gaussian Mixture Means: torch.Size([64953, 10])\n",
      "Precalculated Closest Distance: torch.Size([64953, 1])\n",
      "Predicted Atom Types: torch.Size([763, 17])\n",
      "Predicted Bond Types: torch.Size([1638, 4])\n",
      "Batch Index: torch.Size([64953])\n",
      "Total Loss: 0.9430026737227443\n",
      "MDN Loss: 0.9428443908691406\n",
      "Affinity Pearson Correlation: 0.6915146708488464\n",
      "Atom Loss: 0.002030132105574012\n",
      "Bond Loss: 0.15622907876968384\n",
      "Predicted Scores: tensor([210.0679, 219.5144, 210.9504, 133.8276, 220.4948, 106.6906, 161.8000,\n",
      "         51.3203, 165.5737, 172.4626,  81.7168,  54.9609,  70.7099,  89.0071,\n",
      "        204.1539, 119.4976, 173.1818, 150.1528, 118.9367, 168.6175,  57.8180,\n",
      "        121.2643, 184.5635,  77.7820, 120.7989], dtype=torch.float64)\n",
      "Batch Index: tensor([ 0,  0,  0,  ..., 24, 24, 24])\n",
      "All Test Passed Successfully!\n"
     ]
    }
   ],
   "source": [
    "# Get sample data\n",
    "support_train_prot_eg = train_batch[0]['support']['prots']\n",
    "support_train_lig_eg = train_batch[0]['support']['ligs']\n",
    "\n",
    "# Test network forward pass\n",
    "pi, sigma, mu, dist, atom_types, bond_types, C_batch = base_model.net_forward(\n",
    "    support_train_lig_eg, support_train_prot_eg\n",
    ")\n",
    "\n",
    "print(f\"Predicted Gaussian Mixture Weights: {pi.shape}\\n\"\n",
    "      f\"Predicted Gaussian Mixture Standard Deviations: {sigma.shape}\\n\"\n",
    "      f\"Predicted Gaussian Mixture Means: {mu.shape}\\n\"\n",
    "      f\"Precalculated Closest Distance: {dist.shape}\\n\"\n",
    "      f\"Predicted Atom Types: {atom_types.shape}\\n\"\n",
    "      f\"Predicted Bond Types: {bond_types.shape}\\n\"\n",
    "      f\"Batch Index: {C_batch.shape}\")\n",
    "\n",
    "# Test full model forward pass\n",
    "total_loss, mdn_loss, affi_loss, atom_loss, bond_loss, y, batch = base_model.forward(\n",
    "    train_batch[0]['support']\n",
    ")\n",
    "\n",
    "print(f\"Total Loss: {total_loss}\\n\"\n",
    "      f\"MDN Loss: {mdn_loss}\\n\"\n",
    "      f\"Affinity Pearson Correlation: {affi_loss}\\n\"\n",
    "      f\"Atom Loss: {atom_loss}\\n\"\n",
    "      f\"Bond Loss: {bond_loss}\\n\"\n",
    "      f\"Predicted Scores: {y}\\n\"\n",
    "      f\"Batch Index: {batch}\")\n",
    "\n",
    "# end\n",
    "print('All Test Passed Successfully!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent! 接下来就要上元学习的核心逻辑了，MetaScore采用的元学习方法是基于优化的`MAML`及其变体`MAML++`（后续考虑继续做变体，比如`Task Based Attention Mechanism`等），其介绍可以详细看同目录下的`MetaLearning.md`，下面主要强调其工程上的实现（主要基于`higher`库，比较方便，所有已经写好的并非用于元学习的模型通过`higher`的帮助均可以轻松改造为元模型）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 元学习框架设计"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了将元学习框架套用到`GenScore_GGCN`模型上，我们可以额外定义一个类`MAMLRegressor`.在具体的训练实现中,我们使用了`higher`库以简化开发流程.\n",
    "\n",
    "该设计基于`MAML++`,同时可以通过修改配置逐步退化为传统的`MAML`框架,有一定灵活性.`MAML++`相比于`MAML`的主要改进点有:\n",
    "\n",
    "- `MSL`机制引入.一般而言`MAML`在内部存在多步循环的情况下只考虑通过最后一步循环得到的损失对元模型求导,而`MAML++`中则是通过对多步循环中的每一步的损失进行加权求和,然后再对元模型进行求导.权重根据内循环适应的步数获取.\n",
    "- `learnable inner-loop optimizer`.为模型的每个参数维护一个学习率,这个学习率是可以通过外循环的`meta-update`进行同步更新的,旨在加速模型收敛.\n",
    "- `first-order to second-order`.在一般的`MAML`中,训练流程计算了二阶导会增加内存开销和时间开销,且模型训练容易产生剧烈震荡;而其近似`FOMAML`训练所得结果并不如`MAML`优秀,为此`MAML++`提出,可以在训练的初期先采用一阶近似稳定模型的行为,并在损失基本稳定后开启二阶导训练增强模型的性能."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_torch_seed(seed):\n",
    "    \"\"\"\n",
    "    Sets the pytorch seeds for current experiment run\n",
    "    :param seed: The seed (int)\n",
    "    :return: A random number generator to use\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(seed=seed)\n",
    "    torch_seed = rng.randint(0, 999999)\n",
    "    torch.manual_seed(seed=torch_seed)\n",
    "\n",
    "    return rng\n",
    "\n",
    "class MAMLRegressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes a MAML few shot learning system\n",
    "        :param device: The device to use to use the model on.\n",
    "        :param args: A namedtuple of arguments specifying various hyperparameters.\n",
    "        :param logs_filepath: Path to the logs directory for storing csv files\n",
    "        \"\"\"\n",
    "        super(MAMLRegressor, self).__init__()\n",
    "\n",
    "        # base configs\n",
    "        self.device = torch.device('cpu')\n",
    "        self.batch_size = 4\n",
    "        self.current_epoch = 0\n",
    "        self.logs_filepath = './tmp'\n",
    "        self.learnable_inner_opt_params = True\n",
    "        self.rng = set_torch_seed(seed=1227)\n",
    "        self.regressor = GenScore_GGCN()\n",
    "        self.training_per_iter = 1\n",
    "        self.evaluation_per_iter = 1\n",
    "        self.multi_step_loss_num = 10\n",
    "        self.enable_inner_loop_optimizable_bn_params = True\n",
    "        self.first_order_to_second_order_epoch = 100\n",
    "        self.use_multi_step_loss_optimization = True\n",
    "        self.second_order = True\n",
    "        \n",
    "        # use SGD as the inner loop optimizer according to the original paper\n",
    "        inner_opt_class = hydra.utils.get_class('torch.optim.SGD')\n",
    "        kwargs = {'lr':0.01}\n",
    "\n",
    "        # learnable inner loop optimizer parameters\n",
    "        if self.learnable_inner_opt_params:\n",
    "            param_groups = [\n",
    "                {'params': p, 'lr': 0.01} for p in self.regressor.parameters()\n",
    "            ]\n",
    "            self.inner_opt = inner_opt_class(param_groups, **kwargs)\n",
    "            t = higher.optim.get_trainable_opt_params(self.inner_opt)\n",
    "            self.lrs = nn.ParameterList(map(\n",
    "                nn.Parameter,\n",
    "                t['lr']\n",
    "            ))\n",
    "        else:\n",
    "            params = self.regressor.parameters()\n",
    "            self.inner_opt = inner_opt_class(params, **kwargs)\n",
    "\n",
    "        # print the parameters of the model, which should include inner loop optimizer parameters\n",
    "        print(\"Outer Loop parameters\")\n",
    "        param_shapes = []\n",
    "        for name, param in self.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                print(name, param.shape, param.device, param.requires_grad)\n",
    "                param_shapes.append(param.shape)\n",
    "        print(f'n_params: {sum(map(np.prod, param_shapes))}')\n",
    "\n",
    "        # set Adam as outer loop optimizer\n",
    "        self.optimizer = optim.Adam(self.trainable_parameters(), lr=0.001, amsgrad=False)\n",
    "        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer=self.optimizer, \n",
    "            T_max=100, # 100 epochs\n",
    "            eta_min=1.0e-5 # minimum outer loop learning rate\n",
    "        )\n",
    "\n",
    "    def get_per_step_loss_importance_vector(self):\n",
    "        \"\"\"\n",
    "        Generates a tensor of dimensionality (num_inner_loop_steps) indicating the importance of each step's target\n",
    "        loss towards the optimization loss.\n",
    "        :return: A tensor to be used to compute the weighted average of the loss, useful for\n",
    "        the MSL (Multi Step Loss) mechanism.\n",
    "        \"\"\"\n",
    "        loss_weights = np.ones(shape=(self.training_per_iter)) * (1.0 / self.training_per_iter)\n",
    "        decay_rate = 1.0 / self.training_per_iter / self.multi_step_loss_num\n",
    "        min_value_for_non_final_losses = 0.03 / self.training_per_iter\n",
    "        for i in range(len(loss_weights) - 1):\n",
    "            curr_value = np.maximum(loss_weights[i] - (self.current_epoch * decay_rate), min_value_for_non_final_losses)\n",
    "            loss_weights[i] = curr_value\n",
    "        curr_value = np.minimum(\n",
    "            loss_weights[-1] + (self.current_epoch * (self.training_per_iter - 1) * decay_rate),\n",
    "            1.0 - ((self.training_per_iter - 1) * min_value_for_non_final_losses))\n",
    "        loss_weights[-1] = curr_value\n",
    "        loss_weights = torch.Tensor(loss_weights).to(device=self.device)\n",
    "        return loss_weights\n",
    "\n",
    "    def get_inner_loop_parameter_dict(self, params):\n",
    "        \"\"\"\n",
    "        Returns a dictionary with the parameters to use for inner loop updates.\n",
    "        :param params: A dictionary of the network's parameters.\n",
    "        :return: A dictionary of the parameters to use for the inner loop optimization process.\n",
    "        \"\"\"\n",
    "        param_dict = dict()\n",
    "        for name, param in params:\n",
    "            #print(name, param.shape, param.device, param.requires_grad)\n",
    "            if param.requires_grad:\n",
    "                if self.enable_inner_loop_optimizable_bn_params:\n",
    "                    param_dict[name] = param.to(device=self.device)\n",
    "                else:\n",
    "                    if \"norm_layer\" not in name:\n",
    "                        param_dict[name] = param.to(device=self.device)\n",
    "\n",
    "        return param_dict\n",
    "\n",
    "    def forward(self, data_batch, epoch, \n",
    "                use_second_order, use_multi_step_loss_optimization, \n",
    "                num_steps, training_phase, dist_threshold = 7.0):\n",
    "        # zero the gradients\n",
    "        self.regressor.zero_grad()\n",
    "\n",
    "        # initialize the lists to store the losses and predictions\n",
    "        total_losses = [];mdn_losses = []\n",
    "        atom_losses = [];bond_losses = [];affi_coeffs = []\n",
    "        per_task_query_preds = [];per_task_query_labels = []\n",
    "        \n",
    "        for task_id, task in enumerate(data_batch):\n",
    "            task_losses = [];task_mdn_losses = [];task_affi_coeffs = []\n",
    "            task_atom_losses = [];task_bond_losses = []\n",
    "            \n",
    "            # We only need the predictions after the final inner loop step\n",
    "            final_query_preds_for_task = None \n",
    "            \n",
    "            # get the support and query tasks\n",
    "            support_task = task[\"support\"];query_task = task[\"query\"]\n",
    "            per_task_query_labels.append(query_task[\"labels\"])\n",
    "\n",
    "            # get the per step loss importance vector\n",
    "            per_step_loss_importance_vectors = self.get_per_step_loss_importance_vector()\n",
    "\n",
    "            # use higher to track the gradients\n",
    "            with higher.innerloop_ctx(\n",
    "                self.regressor, self.inner_opt, \n",
    "                copy_initial_weights=False,\n",
    "                track_higher_grads=training_phase,\n",
    "            ) as (fnet, diffopt):\n",
    "                # a trick, but it is obviously wrong, because the inner loop optimizer is not the same as the outer loop optimizer\n",
    "                # for p in self.regressor.parameters():\n",
    "                #    self.inner_opt.state[p] = copy.deepcopy(self.optimizer.state[p])\n",
    "                for num_step in range(num_steps):\n",
    "                    (support_loss, support_mdn_loss, support_affi_coeff, \n",
    "                     support_atom_loss, support_bond_loss, support_preds, \n",
    "                     support_batch) = fnet(task=support_task,dist_threshold=dist_threshold) # run the inner loop  \n",
    "                    \n",
    "                    # higher provide a differentiable optimizer\n",
    "                    # we can use dict to override the parameters of the inner loop optimizer\n",
    "                    if self.learnable_inner_opt_params:\n",
    "                        diffopt.step(support_loss, override={'lr': self.lrs})\n",
    "                    else:\n",
    "                        diffopt.step(support_loss)\n",
    "                    \n",
    "                    # use the multi step loss optimization\n",
    "                    if use_multi_step_loss_optimization and training_phase and \\\n",
    "                            epoch < self.multi_step_loss_num:\n",
    "                        (query_loss, query_mdn_loss, query_affi_coeff, \n",
    "                         query_atom_loss, query_bond_loss, query_preds, \n",
    "                         query_batch) = fnet(query_task,dist_threshold=dist_threshold)\n",
    "                        \n",
    "                        task_losses.append(per_step_loss_importance_vectors[num_step] * query_loss)\n",
    "                        task_mdn_losses.append(per_step_loss_importance_vectors[num_step] * query_mdn_loss)\n",
    "                        task_affi_coeffs.append(per_step_loss_importance_vectors[num_step] * query_affi_coeff)\n",
    "                        task_atom_losses.append(per_step_loss_importance_vectors[num_step] * query_atom_loss)\n",
    "                        task_bond_losses.append(per_step_loss_importance_vectors[num_step] * query_bond_loss)\n",
    "                        \n",
    "                        # store the prediction only from the last step\n",
    "                        if num_step == (num_steps - 1):\n",
    "                            final_query_preds_for_task = query_preds\n",
    "\n",
    "                    # if not use the multi step loss optimization\n",
    "                    # use the last step's prediction and loss\n",
    "                    else:\n",
    "                        if num_step == (self.training_per_iter - 1):\n",
    "                            (query_loss, query_mdn_loss, query_affi_coeff, \n",
    "                             query_atom_loss, query_bond_loss, query_preds, \n",
    "                             query_batch) = fnet(query_task,dist_threshold=dist_threshold)\n",
    "\n",
    "                            task_losses.append(query_loss)\n",
    "                            task_mdn_losses.append(query_mdn_loss)\n",
    "                            task_affi_coeffs.append(query_affi_coeff)\n",
    "                            task_atom_losses.append(query_atom_loss)\n",
    "                            task_bond_losses.append(query_bond_loss)\n",
    "                            \n",
    "                            # store the prediction from the last step\n",
    "                            final_query_preds_for_task = query_preds\n",
    "\n",
    "            # sum the losses of all tasks\n",
    "            total_losses.append(torch.sum(torch.stack(task_losses)))\n",
    "            mdn_losses.append(torch.sum(torch.stack(task_mdn_losses)))\n",
    "            atom_losses.append(torch.sum(torch.stack(task_atom_losses)))\n",
    "            bond_losses.append(torch.sum(torch.stack(task_bond_losses)))\n",
    "            affi_coeffs.append(torch.sum(torch.stack(task_affi_coeffs)))\n",
    "            \n",
    "            # append the final prediction for this task\n",
    "            per_task_query_preds.append(final_query_preds_for_task) \n",
    "\n",
    "        # concatenate the final predictions from all tasks\n",
    "        all_query_preds = torch.cat(per_task_query_preds, dim=0) \n",
    "        all_query_labels = torch.cat(per_task_query_labels, dim=0)\n",
    "        all_query_labels = all_query_labels.to(device=self.device)\n",
    "\n",
    "        # now shapes should match: [batch_size * num_query]\n",
    "        total_affi_coeff = torch.corrcoef(torch.stack([all_query_preds, all_query_labels]))[1, 0]\n",
    "\n",
    "        losses = {\n",
    "            \"total_loss\": torch.mean(torch.stack(total_losses)),\n",
    "            \"mdn_loss\": torch.mean(torch.stack(mdn_losses)),\n",
    "            \"affi_coeffs\": torch.mean(torch.stack(affi_coeffs)),\n",
    "            \"atom_loss\": torch.mean(torch.stack(atom_losses)),\n",
    "            \"bond_loss\": torch.mean(torch.stack(bond_losses)),\n",
    "            \"total_affi_coeff\": total_affi_coeff,\n",
    "        }\n",
    "        \n",
    "        # return the losses and the predictions\n",
    "        return losses, all_query_preds.cpu().numpy()\n",
    "\n",
    "    def trainable_parameters(self):\n",
    "        \"\"\"\n",
    "        Returns an iterator over the trainable parameters of the model.\n",
    "        \"\"\"\n",
    "        for param in self.parameters():\n",
    "            if param.requires_grad:\n",
    "                yield param\n",
    "\n",
    "    def train_forward_prop(self, data_batch, epoch):\n",
    "        \"\"\"\n",
    "        Runs an outer loop forward prop using the meta-model and base-model.\n",
    "        :param data_batch: A data batch containing the support set and target set input, output pairs.\n",
    "        :param epoch: The training epoch's index\n",
    "        :return: A dictionary of losses for the current step.\n",
    "        \"\"\"\n",
    "        losses, all_query_preds = self.forward(\n",
    "            data_batch=data_batch, epoch=epoch,\n",
    "            use_second_order=self.second_order and\n",
    "            epoch > self.first_order_to_second_order_epoch,\n",
    "            use_multi_step_loss_optimization=self.use_multi_step_loss_optimization,\n",
    "            num_steps=self.training_per_iter,\n",
    "            training_phase=True)\n",
    "        return losses, all_query_preds\n",
    "\n",
    "    def evaluation_forward_prop(self, data_batch, epoch):\n",
    "        \"\"\"\n",
    "        Runs an outer loop evaluation forward prop using the meta-model and base-model.\n",
    "        :param data_batch: A data batch containing the support set and target set input, output pairs.\n",
    "        :param epoch: The training epoch's index\n",
    "        :return: A dictionary of losses for the current step.\n",
    "        \"\"\"\n",
    "        losses, all_query_preds = self.forward(\n",
    "            data_batch=data_batch, epoch=epoch, use_second_order=False,\n",
    "            use_multi_step_loss_optimization=self.use_multi_step_loss_optimization,\n",
    "            num_steps=self.evaluation_per_iter,\n",
    "            training_phase=False)\n",
    "        return losses, all_query_preds\n",
    "\n",
    "    def meta_update(self, loss):\n",
    "        \"\"\"\n",
    "        Applies an outer loop update on the meta-parameters of the model.\n",
    "        :param loss: The current loss.\n",
    "        \"\"\"\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        for name, param in self.regressor.named_parameters():\n",
    "            if param.requires_grad and param.grad is not None:\n",
    "                param.grad.data.clamp_(-10, 10)  # not sure if this is necessary, more experiments are needed\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # set the minimum learning rate for the inner loop optimizer\n",
    "        if self.learnable_inner_opt_params:\n",
    "            for lr in self.lrs:\n",
    "                lr.data[lr < 1e-4] = 1e-4\n",
    "\n",
    "    def run_train_iter(self, data_batch, epoch):\n",
    "        \"\"\"\n",
    "        Runs an outer loop update step on the meta-model's parameters.\n",
    "        :param data_batch: input data batch containing the support set and target set input, output pairs\n",
    "        :param epoch: the index of the current epoch\n",
    "        :return: The losses of the ran iteration.\n",
    "        \"\"\"\n",
    "        epoch = int(epoch)\n",
    "        self.scheduler.step(epoch=epoch)\n",
    "        if self.current_epoch != epoch:\n",
    "            self.current_epoch = epoch\n",
    "\n",
    "        # set the model to training mode\n",
    "        self.train()\n",
    "\n",
    "        # run the forward prop\n",
    "        losses, all_query_preds = self.train_forward_prop(data_batch=data_batch, epoch=epoch)\n",
    "\n",
    "        # update the meta-model\n",
    "        self.meta_update(loss=losses['total_loss'])\n",
    "\n",
    "        # get the learning rate for the outer loop optimizer\n",
    "        losses['learning_rate'] = self.scheduler.get_lr()[0]\n",
    "\n",
    "        # zero the gradients\n",
    "        self.optimizer.zero_grad()\n",
    "        self.zero_grad()\n",
    "        \n",
    "        # return the losses and the predictions\n",
    "        return losses, all_query_preds\n",
    "\n",
    "    def run_validation_iter(self, data_batch):\n",
    "        \"\"\"\n",
    "        Runs an outer loop evaluation step on the meta-model's parameters.\n",
    "        :param data_batch: input data batch containing the support set and target set input, output pairs\n",
    "        :param epoch: the index of the current epoch\n",
    "        :return: The losses of the ran iteration.\n",
    "        \"\"\"\n",
    "        # still need to set the model to training mode\n",
    "        # to track the gradients of the base model\n",
    "        self.train()\n",
    "\n",
    "        # run the forward prop\n",
    "        losses, all_query_preds = self.evaluation_forward_prop(data_batch=data_batch, epoch=self.current_epoch)\n",
    "\n",
    "        # return the losses and the predictions\n",
    "        return losses, all_query_preds\n",
    "\n",
    "    def save_model(self, model_save_dir, state):\n",
    "        \"\"\"\n",
    "        Save the network parameter state and experiment state dictionary.\n",
    "        :param model_save_dir: The directory to store the state at.\n",
    "        :param state: The state containing the experiment state and the network. It's in the form of a dictionary\n",
    "        object.\n",
    "        \"\"\"\n",
    "        state['network'] = self.state_dict()\n",
    "        torch.save(state, f=model_save_dir)\n",
    "\n",
    "    def load_model(self, model_save_dir):\n",
    "        \"\"\"\n",
    "        Load checkpoint and return the state dictionary containing the network state params and experiment state.\n",
    "        :param model_save_dir: The directory from which to load the files.\n",
    "        :param model_name: The model_name to be loaded from the direcotry.\n",
    "        :param model_idx: The index of the model (i.e. epoch number or 'latest' for the latest saved model of the current\n",
    "        experiment)\n",
    "        :return: A dictionary containing the experiment state and the saved model parameters.\n",
    "        \"\"\"\n",
    "        filepath = os.path.join(model_save_dir)\n",
    "        state = torch.load(filepath, map_location=self.device, weights_only=False)\n",
    "        state_dict_loaded = state['network']\n",
    "        self.load_state_dict(state_dict=state_dict_loaded)\n",
    "        return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 4. 元学习训练演示\n",
    "\n",
    "现在我们使用定义好的`MAMLRegressor`和准备好的`train_batch`、`val_batch`来演示元学习的训练和验证过程：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check the data batch format:\n",
      "Train batch size: 4\n",
      "Val batch size: 4\n",
      "First task in train batch support set size: torch.Size([25])\n",
      "First task in train batch query set size: torch.Size([25])\n",
      "First task in val batch support set size: torch.Size([48])\n",
      "First task in val batch query set size: torch.Size([48])\n",
      "\n",
      "Initializing MAMLRegressor...\n",
      "Outer Loop parameters\n",
      "regressor.ligand_model.node_encoder.weight torch.Size([128, 41]) cpu True\n",
      "regressor.ligand_model.node_encoder.bias torch.Size([128]) cpu True\n",
      "regressor.ligand_model.edge_encoder.weight torch.Size([128, 10]) cpu True\n",
      "regressor.ligand_model.edge_encoder.bias torch.Size([128]) cpu True\n",
      "regressor.ligand_model.gt_block.0.A.weight torch.Size([128, 128]) cpu True\n",
      "regressor.ligand_model.gt_block.0.A.bias torch.Size([128]) cpu True\n",
      "regressor.ligand_model.gt_block.0.B.weight torch.Size([128, 128]) cpu True\n",
      "regressor.ligand_model.gt_block.0.B.bias torch.Size([128]) cpu True\n",
      "regressor.ligand_model.gt_block.0.C.weight torch.Size([128, 128]) cpu True\n",
      "regressor.ligand_model.gt_block.0.C.bias torch.Size([128]) cpu True\n",
      "regressor.ligand_model.gt_block.0.D.weight torch.Size([128, 128]) cpu True\n",
      "regressor.ligand_model.gt_block.0.D.bias torch.Size([128]) cpu True\n",
      "regressor.ligand_model.gt_block.0.E.weight torch.Size([128, 128]) cpu True\n",
      "regressor.ligand_model.gt_block.0.E.bias torch.Size([128]) cpu True\n",
      "regressor.ligand_model.gt_block.0.bn_node_x.weight torch.Size([128]) cpu True\n",
      "regressor.ligand_model.gt_block.0.bn_node_x.bias torch.Size([128]) cpu True\n",
      "regressor.ligand_model.gt_block.0.bn_edge_e.weight torch.Size([128]) cpu True\n",
      "regressor.ligand_model.gt_block.0.bn_edge_e.bias torch.Size([128]) cpu True\n",
      "regressor.ligand_model.gt_block.1.A.weight torch.Size([128, 128]) cpu True\n",
      "regressor.ligand_model.gt_block.1.A.bias torch.Size([128]) cpu True\n",
      "regressor.ligand_model.gt_block.1.B.weight torch.Size([128, 128]) cpu True\n",
      "regressor.ligand_model.gt_block.1.B.bias torch.Size([128]) cpu True\n",
      "regressor.ligand_model.gt_block.1.C.weight torch.Size([128, 128]) cpu True\n",
      "regressor.ligand_model.gt_block.1.C.bias torch.Size([128]) cpu True\n",
      "regressor.ligand_model.gt_block.1.D.weight torch.Size([128, 128]) cpu True\n",
      "regressor.ligand_model.gt_block.1.D.bias torch.Size([128]) cpu True\n",
      "regressor.ligand_model.gt_block.1.E.weight torch.Size([128, 128]) cpu True\n",
      "regressor.ligand_model.gt_block.1.E.bias torch.Size([128]) cpu True\n",
      "regressor.ligand_model.gt_block.1.bn_node_x.weight torch.Size([128]) cpu True\n",
      "regressor.ligand_model.gt_block.1.bn_node_x.bias torch.Size([128]) cpu True\n",
      "regressor.ligand_model.gt_block.1.bn_edge_e.weight torch.Size([128]) cpu True\n",
      "regressor.ligand_model.gt_block.1.bn_edge_e.bias torch.Size([128]) cpu True\n",
      "regressor.ligand_model.gt_block.2.A.weight torch.Size([128, 128]) cpu True\n",
      "regressor.ligand_model.gt_block.2.A.bias torch.Size([128]) cpu True\n",
      "regressor.ligand_model.gt_block.2.B.weight torch.Size([128, 128]) cpu True\n",
      "regressor.ligand_model.gt_block.2.B.bias torch.Size([128]) cpu True\n",
      "regressor.ligand_model.gt_block.2.C.weight torch.Size([128, 128]) cpu True\n",
      "regressor.ligand_model.gt_block.2.C.bias torch.Size([128]) cpu True\n",
      "regressor.ligand_model.gt_block.2.D.weight torch.Size([128, 128]) cpu True\n",
      "regressor.ligand_model.gt_block.2.D.bias torch.Size([128]) cpu True\n",
      "regressor.ligand_model.gt_block.2.E.weight torch.Size([128, 128]) cpu True\n",
      "regressor.ligand_model.gt_block.2.E.bias torch.Size([128]) cpu True\n",
      "regressor.ligand_model.gt_block.2.bn_node_x.weight torch.Size([128]) cpu True\n",
      "regressor.ligand_model.gt_block.2.bn_node_x.bias torch.Size([128]) cpu True\n",
      "regressor.ligand_model.gt_block.2.bn_edge_e.weight torch.Size([128]) cpu True\n",
      "regressor.ligand_model.gt_block.2.bn_edge_e.bias torch.Size([128]) cpu True\n",
      "regressor.ligand_model.gt_block.3.A.weight torch.Size([128, 128]) cpu True\n",
      "regressor.ligand_model.gt_block.3.A.bias torch.Size([128]) cpu True\n",
      "regressor.ligand_model.gt_block.3.B.weight torch.Size([128, 128]) cpu True\n",
      "regressor.ligand_model.gt_block.3.B.bias torch.Size([128]) cpu True\n",
      "regressor.ligand_model.gt_block.3.C.weight torch.Size([128, 128]) cpu True\n",
      "regressor.ligand_model.gt_block.3.C.bias torch.Size([128]) cpu True\n",
      "regressor.ligand_model.gt_block.3.D.weight torch.Size([128, 128]) cpu True\n",
      "regressor.ligand_model.gt_block.3.D.bias torch.Size([128]) cpu True\n",
      "regressor.ligand_model.gt_block.3.E.weight torch.Size([128, 128]) cpu True\n",
      "regressor.ligand_model.gt_block.3.E.bias torch.Size([128]) cpu True\n",
      "regressor.ligand_model.gt_block.3.bn_node_x.weight torch.Size([128]) cpu True\n",
      "regressor.ligand_model.gt_block.3.bn_node_x.bias torch.Size([128]) cpu True\n",
      "regressor.ligand_model.gt_block.3.bn_edge_e.weight torch.Size([128]) cpu True\n",
      "regressor.ligand_model.gt_block.3.bn_edge_e.bias torch.Size([128]) cpu True\n",
      "regressor.ligand_model.gt_block.4.A.weight torch.Size([128, 128]) cpu True\n",
      "regressor.ligand_model.gt_block.4.A.bias torch.Size([128]) cpu True\n",
      "regressor.ligand_model.gt_block.4.B.weight torch.Size([128, 128]) cpu True\n",
      "regressor.ligand_model.gt_block.4.B.bias torch.Size([128]) cpu True\n",
      "regressor.ligand_model.gt_block.4.C.weight torch.Size([128, 128]) cpu True\n",
      "regressor.ligand_model.gt_block.4.C.bias torch.Size([128]) cpu True\n",
      "regressor.ligand_model.gt_block.4.D.weight torch.Size([128, 128]) cpu True\n",
      "regressor.ligand_model.gt_block.4.D.bias torch.Size([128]) cpu True\n",
      "regressor.ligand_model.gt_block.4.E.weight torch.Size([128, 128]) cpu True\n",
      "regressor.ligand_model.gt_block.4.E.bias torch.Size([128]) cpu True\n",
      "regressor.ligand_model.gt_block.4.bn_node_x.weight torch.Size([128]) cpu True\n",
      "regressor.ligand_model.gt_block.4.bn_node_x.bias torch.Size([128]) cpu True\n",
      "regressor.ligand_model.gt_block.4.bn_edge_e.weight torch.Size([128]) cpu True\n",
      "regressor.ligand_model.gt_block.4.bn_edge_e.bias torch.Size([128]) cpu True\n",
      "regressor.ligand_model.gt_block.5.A.weight torch.Size([128, 128]) cpu True\n",
      "regressor.ligand_model.gt_block.5.A.bias torch.Size([128]) cpu True\n",
      "regressor.ligand_model.gt_block.5.B.weight torch.Size([128, 128]) cpu True\n",
      "regressor.ligand_model.gt_block.5.B.bias torch.Size([128]) cpu True\n",
      "regressor.ligand_model.gt_block.5.C.weight torch.Size([128, 128]) cpu True\n",
      "regressor.ligand_model.gt_block.5.C.bias torch.Size([128]) cpu True\n",
      "regressor.ligand_model.gt_block.5.D.weight torch.Size([128, 128]) cpu True\n",
      "regressor.ligand_model.gt_block.5.D.bias torch.Size([128]) cpu True\n",
      "regressor.ligand_model.gt_block.5.E.weight torch.Size([128, 128]) cpu True\n",
      "regressor.ligand_model.gt_block.5.E.bias torch.Size([128]) cpu True\n",
      "regressor.ligand_model.gt_block.5.bn_node_x.weight torch.Size([128]) cpu True\n",
      "regressor.ligand_model.gt_block.5.bn_node_x.bias torch.Size([128]) cpu True\n",
      "regressor.ligand_model.gt_block.5.bn_edge_e.weight torch.Size([128]) cpu True\n",
      "regressor.ligand_model.gt_block.5.bn_edge_e.bias torch.Size([128]) cpu True\n",
      "regressor.target_model.node_encoder.weight torch.Size([128, 41]) cpu True\n",
      "regressor.target_model.node_encoder.bias torch.Size([128]) cpu True\n",
      "regressor.target_model.edge_encoder.weight torch.Size([128, 5]) cpu True\n",
      "regressor.target_model.edge_encoder.bias torch.Size([128]) cpu True\n",
      "regressor.target_model.gt_block.0.A.weight torch.Size([128, 128]) cpu True\n",
      "regressor.target_model.gt_block.0.A.bias torch.Size([128]) cpu True\n",
      "regressor.target_model.gt_block.0.B.weight torch.Size([128, 128]) cpu True\n",
      "regressor.target_model.gt_block.0.B.bias torch.Size([128]) cpu True\n",
      "regressor.target_model.gt_block.0.C.weight torch.Size([128, 128]) cpu True\n",
      "regressor.target_model.gt_block.0.C.bias torch.Size([128]) cpu True\n",
      "regressor.target_model.gt_block.0.D.weight torch.Size([128, 128]) cpu True\n",
      "regressor.target_model.gt_block.0.D.bias torch.Size([128]) cpu True\n",
      "regressor.target_model.gt_block.0.E.weight torch.Size([128, 128]) cpu True\n",
      "regressor.target_model.gt_block.0.E.bias torch.Size([128]) cpu True\n",
      "regressor.target_model.gt_block.0.bn_node_x.weight torch.Size([128]) cpu True\n",
      "regressor.target_model.gt_block.0.bn_node_x.bias torch.Size([128]) cpu True\n",
      "regressor.target_model.gt_block.0.bn_edge_e.weight torch.Size([128]) cpu True\n",
      "regressor.target_model.gt_block.0.bn_edge_e.bias torch.Size([128]) cpu True\n",
      "regressor.target_model.gt_block.1.A.weight torch.Size([128, 128]) cpu True\n",
      "regressor.target_model.gt_block.1.A.bias torch.Size([128]) cpu True\n",
      "regressor.target_model.gt_block.1.B.weight torch.Size([128, 128]) cpu True\n",
      "regressor.target_model.gt_block.1.B.bias torch.Size([128]) cpu True\n",
      "regressor.target_model.gt_block.1.C.weight torch.Size([128, 128]) cpu True\n",
      "regressor.target_model.gt_block.1.C.bias torch.Size([128]) cpu True\n",
      "regressor.target_model.gt_block.1.D.weight torch.Size([128, 128]) cpu True\n",
      "regressor.target_model.gt_block.1.D.bias torch.Size([128]) cpu True\n",
      "regressor.target_model.gt_block.1.E.weight torch.Size([128, 128]) cpu True\n",
      "regressor.target_model.gt_block.1.E.bias torch.Size([128]) cpu True\n",
      "regressor.target_model.gt_block.1.bn_node_x.weight torch.Size([128]) cpu True\n",
      "regressor.target_model.gt_block.1.bn_node_x.bias torch.Size([128]) cpu True\n",
      "regressor.target_model.gt_block.1.bn_edge_e.weight torch.Size([128]) cpu True\n",
      "regressor.target_model.gt_block.1.bn_edge_e.bias torch.Size([128]) cpu True\n",
      "regressor.target_model.gt_block.2.A.weight torch.Size([128, 128]) cpu True\n",
      "regressor.target_model.gt_block.2.A.bias torch.Size([128]) cpu True\n",
      "regressor.target_model.gt_block.2.B.weight torch.Size([128, 128]) cpu True\n",
      "regressor.target_model.gt_block.2.B.bias torch.Size([128]) cpu True\n",
      "regressor.target_model.gt_block.2.C.weight torch.Size([128, 128]) cpu True\n",
      "regressor.target_model.gt_block.2.C.bias torch.Size([128]) cpu True\n",
      "regressor.target_model.gt_block.2.D.weight torch.Size([128, 128]) cpu True\n",
      "regressor.target_model.gt_block.2.D.bias torch.Size([128]) cpu True\n",
      "regressor.target_model.gt_block.2.E.weight torch.Size([128, 128]) cpu True\n",
      "regressor.target_model.gt_block.2.E.bias torch.Size([128]) cpu True\n",
      "regressor.target_model.gt_block.2.bn_node_x.weight torch.Size([128]) cpu True\n",
      "regressor.target_model.gt_block.2.bn_node_x.bias torch.Size([128]) cpu True\n",
      "regressor.target_model.gt_block.2.bn_edge_e.weight torch.Size([128]) cpu True\n",
      "regressor.target_model.gt_block.2.bn_edge_e.bias torch.Size([128]) cpu True\n",
      "regressor.target_model.gt_block.3.A.weight torch.Size([128, 128]) cpu True\n",
      "regressor.target_model.gt_block.3.A.bias torch.Size([128]) cpu True\n",
      "regressor.target_model.gt_block.3.B.weight torch.Size([128, 128]) cpu True\n",
      "regressor.target_model.gt_block.3.B.bias torch.Size([128]) cpu True\n",
      "regressor.target_model.gt_block.3.C.weight torch.Size([128, 128]) cpu True\n",
      "regressor.target_model.gt_block.3.C.bias torch.Size([128]) cpu True\n",
      "regressor.target_model.gt_block.3.D.weight torch.Size([128, 128]) cpu True\n",
      "regressor.target_model.gt_block.3.D.bias torch.Size([128]) cpu True\n",
      "regressor.target_model.gt_block.3.E.weight torch.Size([128, 128]) cpu True\n",
      "regressor.target_model.gt_block.3.E.bias torch.Size([128]) cpu True\n",
      "regressor.target_model.gt_block.3.bn_node_x.weight torch.Size([128]) cpu True\n",
      "regressor.target_model.gt_block.3.bn_node_x.bias torch.Size([128]) cpu True\n",
      "regressor.target_model.gt_block.3.bn_edge_e.weight torch.Size([128]) cpu True\n",
      "regressor.target_model.gt_block.3.bn_edge_e.bias torch.Size([128]) cpu True\n",
      "regressor.target_model.gt_block.4.A.weight torch.Size([128, 128]) cpu True\n",
      "regressor.target_model.gt_block.4.A.bias torch.Size([128]) cpu True\n",
      "regressor.target_model.gt_block.4.B.weight torch.Size([128, 128]) cpu True\n",
      "regressor.target_model.gt_block.4.B.bias torch.Size([128]) cpu True\n",
      "regressor.target_model.gt_block.4.C.weight torch.Size([128, 128]) cpu True\n",
      "regressor.target_model.gt_block.4.C.bias torch.Size([128]) cpu True\n",
      "regressor.target_model.gt_block.4.D.weight torch.Size([128, 128]) cpu True\n",
      "regressor.target_model.gt_block.4.D.bias torch.Size([128]) cpu True\n",
      "regressor.target_model.gt_block.4.E.weight torch.Size([128, 128]) cpu True\n",
      "regressor.target_model.gt_block.4.E.bias torch.Size([128]) cpu True\n",
      "regressor.target_model.gt_block.4.bn_node_x.weight torch.Size([128]) cpu True\n",
      "regressor.target_model.gt_block.4.bn_node_x.bias torch.Size([128]) cpu True\n",
      "regressor.target_model.gt_block.4.bn_edge_e.weight torch.Size([128]) cpu True\n",
      "regressor.target_model.gt_block.4.bn_edge_e.bias torch.Size([128]) cpu True\n",
      "regressor.target_model.gt_block.5.A.weight torch.Size([128, 128]) cpu True\n",
      "regressor.target_model.gt_block.5.A.bias torch.Size([128]) cpu True\n",
      "regressor.target_model.gt_block.5.B.weight torch.Size([128, 128]) cpu True\n",
      "regressor.target_model.gt_block.5.B.bias torch.Size([128]) cpu True\n",
      "regressor.target_model.gt_block.5.C.weight torch.Size([128, 128]) cpu True\n",
      "regressor.target_model.gt_block.5.C.bias torch.Size([128]) cpu True\n",
      "regressor.target_model.gt_block.5.D.weight torch.Size([128, 128]) cpu True\n",
      "regressor.target_model.gt_block.5.D.bias torch.Size([128]) cpu True\n",
      "regressor.target_model.gt_block.5.E.weight torch.Size([128, 128]) cpu True\n",
      "regressor.target_model.gt_block.5.E.bias torch.Size([128]) cpu True\n",
      "regressor.target_model.gt_block.5.bn_node_x.weight torch.Size([128]) cpu True\n",
      "regressor.target_model.gt_block.5.bn_node_x.bias torch.Size([128]) cpu True\n",
      "regressor.target_model.gt_block.5.bn_edge_e.weight torch.Size([128]) cpu True\n",
      "regressor.target_model.gt_block.5.bn_edge_e.bias torch.Size([128]) cpu True\n",
      "regressor.MLP.0.weight torch.Size([128, 256]) cpu True\n",
      "regressor.MLP.0.bias torch.Size([128]) cpu True\n",
      "regressor.MLP.1.weight torch.Size([128]) cpu True\n",
      "regressor.MLP.1.bias torch.Size([128]) cpu True\n",
      "regressor.z_pi.weight torch.Size([10, 128]) cpu True\n",
      "regressor.z_pi.bias torch.Size([10]) cpu True\n",
      "regressor.z_sigma.weight torch.Size([10, 128]) cpu True\n",
      "regressor.z_sigma.bias torch.Size([10]) cpu True\n",
      "regressor.z_mu.weight torch.Size([10, 128]) cpu True\n",
      "regressor.z_mu.bias torch.Size([10]) cpu True\n",
      "regressor.atom_types.weight torch.Size([17, 128]) cpu True\n",
      "regressor.atom_types.bias torch.Size([17]) cpu True\n",
      "regressor.bond_types.weight torch.Size([4, 256]) cpu True\n",
      "regressor.bond_types.bias torch.Size([4]) cpu True\n",
      "lrs.0 torch.Size([]) cpu True\n",
      "lrs.1 torch.Size([]) cpu True\n",
      "lrs.2 torch.Size([]) cpu True\n",
      "lrs.3 torch.Size([]) cpu True\n",
      "lrs.4 torch.Size([]) cpu True\n",
      "lrs.5 torch.Size([]) cpu True\n",
      "lrs.6 torch.Size([]) cpu True\n",
      "lrs.7 torch.Size([]) cpu True\n",
      "lrs.8 torch.Size([]) cpu True\n",
      "lrs.9 torch.Size([]) cpu True\n",
      "lrs.10 torch.Size([]) cpu True\n",
      "lrs.11 torch.Size([]) cpu True\n",
      "lrs.12 torch.Size([]) cpu True\n",
      "lrs.13 torch.Size([]) cpu True\n",
      "lrs.14 torch.Size([]) cpu True\n",
      "lrs.15 torch.Size([]) cpu True\n",
      "lrs.16 torch.Size([]) cpu True\n",
      "lrs.17 torch.Size([]) cpu True\n",
      "lrs.18 torch.Size([]) cpu True\n",
      "lrs.19 torch.Size([]) cpu True\n",
      "lrs.20 torch.Size([]) cpu True\n",
      "lrs.21 torch.Size([]) cpu True\n",
      "lrs.22 torch.Size([]) cpu True\n",
      "lrs.23 torch.Size([]) cpu True\n",
      "lrs.24 torch.Size([]) cpu True\n",
      "lrs.25 torch.Size([]) cpu True\n",
      "lrs.26 torch.Size([]) cpu True\n",
      "lrs.27 torch.Size([]) cpu True\n",
      "lrs.28 torch.Size([]) cpu True\n",
      "lrs.29 torch.Size([]) cpu True\n",
      "lrs.30 torch.Size([]) cpu True\n",
      "lrs.31 torch.Size([]) cpu True\n",
      "lrs.32 torch.Size([]) cpu True\n",
      "lrs.33 torch.Size([]) cpu True\n",
      "lrs.34 torch.Size([]) cpu True\n",
      "lrs.35 torch.Size([]) cpu True\n",
      "lrs.36 torch.Size([]) cpu True\n",
      "lrs.37 torch.Size([]) cpu True\n",
      "lrs.38 torch.Size([]) cpu True\n",
      "lrs.39 torch.Size([]) cpu True\n",
      "lrs.40 torch.Size([]) cpu True\n",
      "lrs.41 torch.Size([]) cpu True\n",
      "lrs.42 torch.Size([]) cpu True\n",
      "lrs.43 torch.Size([]) cpu True\n",
      "lrs.44 torch.Size([]) cpu True\n",
      "lrs.45 torch.Size([]) cpu True\n",
      "lrs.46 torch.Size([]) cpu True\n",
      "lrs.47 torch.Size([]) cpu True\n",
      "lrs.48 torch.Size([]) cpu True\n",
      "lrs.49 torch.Size([]) cpu True\n",
      "lrs.50 torch.Size([]) cpu True\n",
      "lrs.51 torch.Size([]) cpu True\n",
      "lrs.52 torch.Size([]) cpu True\n",
      "lrs.53 torch.Size([]) cpu True\n",
      "lrs.54 torch.Size([]) cpu True\n",
      "lrs.55 torch.Size([]) cpu True\n",
      "lrs.56 torch.Size([]) cpu True\n",
      "lrs.57 torch.Size([]) cpu True\n",
      "lrs.58 torch.Size([]) cpu True\n",
      "lrs.59 torch.Size([]) cpu True\n",
      "lrs.60 torch.Size([]) cpu True\n",
      "lrs.61 torch.Size([]) cpu True\n",
      "lrs.62 torch.Size([]) cpu True\n",
      "lrs.63 torch.Size([]) cpu True\n",
      "lrs.64 torch.Size([]) cpu True\n",
      "lrs.65 torch.Size([]) cpu True\n",
      "lrs.66 torch.Size([]) cpu True\n",
      "lrs.67 torch.Size([]) cpu True\n",
      "lrs.68 torch.Size([]) cpu True\n",
      "lrs.69 torch.Size([]) cpu True\n",
      "lrs.70 torch.Size([]) cpu True\n",
      "lrs.71 torch.Size([]) cpu True\n",
      "lrs.72 torch.Size([]) cpu True\n",
      "lrs.73 torch.Size([]) cpu True\n",
      "lrs.74 torch.Size([]) cpu True\n",
      "lrs.75 torch.Size([]) cpu True\n",
      "lrs.76 torch.Size([]) cpu True\n",
      "lrs.77 torch.Size([]) cpu True\n",
      "lrs.78 torch.Size([]) cpu True\n",
      "lrs.79 torch.Size([]) cpu True\n",
      "lrs.80 torch.Size([]) cpu True\n",
      "lrs.81 torch.Size([]) cpu True\n",
      "lrs.82 torch.Size([]) cpu True\n",
      "lrs.83 torch.Size([]) cpu True\n",
      "lrs.84 torch.Size([]) cpu True\n",
      "lrs.85 torch.Size([]) cpu True\n",
      "lrs.86 torch.Size([]) cpu True\n",
      "lrs.87 torch.Size([]) cpu True\n",
      "lrs.88 torch.Size([]) cpu True\n",
      "lrs.89 torch.Size([]) cpu True\n",
      "lrs.90 torch.Size([]) cpu True\n",
      "lrs.91 torch.Size([]) cpu True\n",
      "lrs.92 torch.Size([]) cpu True\n",
      "lrs.93 torch.Size([]) cpu True\n",
      "lrs.94 torch.Size([]) cpu True\n",
      "lrs.95 torch.Size([]) cpu True\n",
      "lrs.96 torch.Size([]) cpu True\n",
      "lrs.97 torch.Size([]) cpu True\n",
      "lrs.98 torch.Size([]) cpu True\n",
      "lrs.99 torch.Size([]) cpu True\n",
      "lrs.100 torch.Size([]) cpu True\n",
      "lrs.101 torch.Size([]) cpu True\n",
      "lrs.102 torch.Size([]) cpu True\n",
      "lrs.103 torch.Size([]) cpu True\n",
      "lrs.104 torch.Size([]) cpu True\n",
      "lrs.105 torch.Size([]) cpu True\n",
      "lrs.106 torch.Size([]) cpu True\n",
      "lrs.107 torch.Size([]) cpu True\n",
      "lrs.108 torch.Size([]) cpu True\n",
      "lrs.109 torch.Size([]) cpu True\n",
      "lrs.110 torch.Size([]) cpu True\n",
      "lrs.111 torch.Size([]) cpu True\n",
      "lrs.112 torch.Size([]) cpu True\n",
      "lrs.113 torch.Size([]) cpu True\n",
      "lrs.114 torch.Size([]) cpu True\n",
      "lrs.115 torch.Size([]) cpu True\n",
      "lrs.116 torch.Size([]) cpu True\n",
      "lrs.117 torch.Size([]) cpu True\n",
      "lrs.118 torch.Size([]) cpu True\n",
      "lrs.119 torch.Size([]) cpu True\n",
      "lrs.120 torch.Size([]) cpu True\n",
      "lrs.121 torch.Size([]) cpu True\n",
      "lrs.122 torch.Size([]) cpu True\n",
      "lrs.123 torch.Size([]) cpu True\n",
      "lrs.124 torch.Size([]) cpu True\n",
      "lrs.125 torch.Size([]) cpu True\n",
      "lrs.126 torch.Size([]) cpu True\n",
      "lrs.127 torch.Size([]) cpu True\n",
      "lrs.128 torch.Size([]) cpu True\n",
      "lrs.129 torch.Size([]) cpu True\n",
      "lrs.130 torch.Size([]) cpu True\n",
      "lrs.131 torch.Size([]) cpu True\n",
      "lrs.132 torch.Size([]) cpu True\n",
      "lrs.133 torch.Size([]) cpu True\n",
      "lrs.134 torch.Size([]) cpu True\n",
      "lrs.135 torch.Size([]) cpu True\n",
      "lrs.136 torch.Size([]) cpu True\n",
      "lrs.137 torch.Size([]) cpu True\n",
      "lrs.138 torch.Size([]) cpu True\n",
      "lrs.139 torch.Size([]) cpu True\n",
      "lrs.140 torch.Size([]) cpu True\n",
      "lrs.141 torch.Size([]) cpu True\n",
      "lrs.142 torch.Size([]) cpu True\n",
      "lrs.143 torch.Size([]) cpu True\n",
      "lrs.144 torch.Size([]) cpu True\n",
      "lrs.145 torch.Size([]) cpu True\n",
      "lrs.146 torch.Size([]) cpu True\n",
      "lrs.147 torch.Size([]) cpu True\n",
      "lrs.148 torch.Size([]) cpu True\n",
      "lrs.149 torch.Size([]) cpu True\n",
      "lrs.150 torch.Size([]) cpu True\n",
      "lrs.151 torch.Size([]) cpu True\n",
      "lrs.152 torch.Size([]) cpu True\n",
      "lrs.153 torch.Size([]) cpu True\n",
      "lrs.154 torch.Size([]) cpu True\n",
      "lrs.155 torch.Size([]) cpu True\n",
      "lrs.156 torch.Size([]) cpu True\n",
      "lrs.157 torch.Size([]) cpu True\n",
      "lrs.158 torch.Size([]) cpu True\n",
      "lrs.159 torch.Size([]) cpu True\n",
      "lrs.160 torch.Size([]) cpu True\n",
      "lrs.161 torch.Size([]) cpu True\n",
      "lrs.162 torch.Size([]) cpu True\n",
      "lrs.163 torch.Size([]) cpu True\n",
      "lrs.164 torch.Size([]) cpu True\n",
      "lrs.165 torch.Size([]) cpu True\n",
      "lrs.166 torch.Size([]) cpu True\n",
      "lrs.167 torch.Size([]) cpu True\n",
      "lrs.168 torch.Size([]) cpu True\n",
      "lrs.169 torch.Size([]) cpu True\n",
      "lrs.170 torch.Size([]) cpu True\n",
      "lrs.171 torch.Size([]) cpu True\n",
      "lrs.172 torch.Size([]) cpu True\n",
      "lrs.173 torch.Size([]) cpu True\n",
      "lrs.174 torch.Size([]) cpu True\n",
      "lrs.175 torch.Size([]) cpu True\n",
      "lrs.176 torch.Size([]) cpu True\n",
      "lrs.177 torch.Size([]) cpu True\n",
      "lrs.178 torch.Size([]) cpu True\n",
      "lrs.179 torch.Size([]) cpu True\n",
      "lrs.180 torch.Size([]) cpu True\n",
      "lrs.181 torch.Size([]) cpu True\n",
      "lrs.182 torch.Size([]) cpu True\n",
      "lrs.183 torch.Size([]) cpu True\n",
      "lrs.184 torch.Size([]) cpu True\n",
      "lrs.185 torch.Size([]) cpu True\n",
      "lrs.186 torch.Size([]) cpu True\n",
      "lrs.187 torch.Size([]) cpu True\n",
      "lrs.188 torch.Size([]) cpu True\n",
      "lrs.189 torch.Size([]) cpu True\n",
      "n_params: 1050225.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'best_loss': 0.0,\n",
       " 'best_val_iter': 0,\n",
       " 'current_iter': 43800,\n",
       " 'best_epoch': 0,\n",
       " 'train_total_loss_mean': np.float64(0.8793659770725962),\n",
       " 'train_total_loss_std': np.float64(0.0511439905202655),\n",
       " 'train_mdn_loss_mean': np.float64(0.8793552911281586),\n",
       " 'train_mdn_loss_std': np.float64(0.051144483024852305),\n",
       " 'train_affi_coeffs_mean': np.float64(0.30650401670485733),\n",
       " 'train_affi_coeffs_std': np.float64(0.15846373011194897),\n",
       " 'train_atom_loss_mean': np.float64(0.003970644685905427),\n",
       " 'train_atom_loss_std': np.float64(0.002066610637330492),\n",
       " 'train_bond_loss_mean': np.float64(0.10284830048680306),\n",
       " 'train_bond_loss_std': np.float64(0.02092453590579454),\n",
       " 'train_total_affi_coeff_mean': np.float64(0.29317947251102905),\n",
       " 'train_total_affi_coeff_std': np.float64(0.17060049695686635),\n",
       " 'train_learning_rate_mean': np.float64(0.00043141474416372774),\n",
       " 'train_learning_rate_std': np.float64(1.6263032587282567e-19),\n",
       " 'val_total_loss_mean': np.float64(1.013706842350887),\n",
       " 'val_total_loss_std': np.float64(0.019942059980928635),\n",
       " 'val_mdn_loss_mean': np.float64(1.0136973410844803),\n",
       " 'val_mdn_loss_std': np.float64(0.019942019866096405),\n",
       " 'val_affi_coeffs_mean': np.float64(0.4235087241977453),\n",
       " 'val_affi_coeffs_std': np.float64(0.06457784836080582),\n",
       " 'val_atom_loss_mean': np.float64(0.0031938927859300748),\n",
       " 'val_atom_loss_std': np.float64(0.0009136307377799848),\n",
       " 'val_bond_loss_mean': np.float64(0.09207224333658814),\n",
       " 'val_bond_loss_std': np.float64(0.008534174262040513),\n",
       " 'val_total_affi_coeff_mean': np.float64(0.41473574879964714),\n",
       " 'val_total_affi_coeff_std': np.float64(0.05554275134676084),\n",
       " 'network': OrderedDict([('regressor.ligand_model.node_encoder.weight',\n",
       "               tensor([[ 0.0019,  0.4025,  0.1972,  ...,  0.1877,  0.2419, -0.3293],\n",
       "                       [ 0.0794, -0.0611, -0.0304,  ..., -0.1914, -0.3233, -0.3581],\n",
       "                       [ 0.2848, -0.1230, -0.2462,  ..., -0.2273, -0.2758, -0.4187],\n",
       "                       ...,\n",
       "                       [-0.0477, -0.0922, -0.0399,  ...,  0.2363,  0.0705,  0.4025],\n",
       "                       [-0.0656, -0.5056, -0.0659,  ...,  0.0820,  0.6419,  0.0457],\n",
       "                       [-0.1837, -0.1127,  0.1176,  ..., -0.4213, -0.5353, -0.1231]])),\n",
       "              ('regressor.ligand_model.node_encoder.bias',\n",
       "               tensor([ 2.6293e-02, -1.2419e-01,  9.4001e-02,  1.7051e-01, -9.4765e-02,\n",
       "                       -1.5349e-01, -1.4115e-01, -1.3974e-01,  1.4003e-02,  1.9148e-01,\n",
       "                       -2.5143e-01,  1.5703e-01,  8.8314e-02, -2.1520e-01,  1.9417e-01,\n",
       "                        2.4293e-01, -1.1322e-01, -5.6543e-02,  2.1432e-01, -1.2771e-02,\n",
       "                        3.6405e-03,  1.0357e-01,  2.3096e-01,  2.2416e-01, -4.8220e-03,\n",
       "                       -1.6118e-01, -1.8023e-01,  4.0011e-01, -3.3243e-02,  1.2974e-01,\n",
       "                        1.5351e-01,  4.7044e-02,  1.1897e-01, -1.0581e-01, -8.3026e-02,\n",
       "                        3.2333e-01, -1.1439e-02,  2.7797e-01, -2.9328e-02, -8.6663e-02,\n",
       "                       -2.1829e-01, -2.5175e-01, -3.1475e-01, -1.4364e-01, -1.0206e-01,\n",
       "                        1.2986e-01, -1.1328e-01, -8.5664e-02,  1.4246e-01,  7.2347e-02,\n",
       "                        1.1508e-01, -1.1039e-01,  5.4607e-02,  1.5561e-01,  4.0261e-01,\n",
       "                       -1.3632e-01, -1.0738e-01, -5.9741e-02,  2.0122e-01,  1.4903e-01,\n",
       "                        2.5564e-01, -1.6513e-01, -1.8390e-01, -3.6026e-01, -1.5118e-01,\n",
       "                       -1.0990e-01, -1.8166e-02, -4.6018e-02,  3.3505e-01, -3.0126e-01,\n",
       "                        1.2322e-01,  1.8444e-02, -5.3278e-02,  3.3415e-01, -1.0165e-01,\n",
       "                       -2.3866e-02,  2.2779e-01, -7.7986e-02,  3.8301e-01, -1.8553e-01,\n",
       "                        4.9359e-02,  1.7166e-01,  1.1793e-01, -5.9675e-02, -1.8440e-01,\n",
       "                        2.1788e-02,  3.4946e-01,  1.0392e-01,  9.6017e-02,  2.9086e-01,\n",
       "                        1.8237e-01, -1.7944e-01, -4.1434e-01,  4.2485e-03, -7.4100e-02,\n",
       "                       -2.6276e-01, -1.4631e-01, -1.3361e-01,  5.1108e-02,  1.2607e-01,\n",
       "                        1.8154e-02, -5.1546e-02, -5.8264e-02,  3.9222e-01, -2.1587e-01,\n",
       "                        3.8166e-01,  3.5717e-01,  1.3225e-01, -1.5228e-02, -1.5784e-02,\n",
       "                        1.1367e-04,  7.4186e-02, -1.4359e-02, -7.2735e-04, -1.0183e-02,\n",
       "                        1.0502e-01,  1.1115e-01, -3.7727e-02,  5.2804e-02,  3.6487e-01,\n",
       "                       -2.0657e-01, -2.6717e-01, -5.3487e-01,  1.5088e-02,  9.5175e-02,\n",
       "                        1.1180e-01,  5.7296e-02,  2.7236e-01])),\n",
       "              ('regressor.ligand_model.edge_encoder.weight',\n",
       "               tensor([[ 0.2397, -0.0685,  0.3602,  ...,  1.2278,  0.1245,  0.7026],\n",
       "                       [-0.3085,  0.0252,  0.1470,  ...,  0.5658,  0.0274,  0.0218],\n",
       "                       [ 0.2442, -0.4285,  0.0877,  ..., -0.3109,  0.2331, -0.0489],\n",
       "                       ...,\n",
       "                       [ 0.2264, -0.1797,  0.1820,  ...,  0.2050,  0.2407, -0.3368],\n",
       "                       [ 0.2251,  0.4101,  0.0349,  ..., -0.3409, -0.2279,  0.5604],\n",
       "                       [-0.0534, -0.1873,  0.1348,  ...,  0.6501, -0.1115,  0.4617]])),\n",
       "              ('regressor.ligand_model.edge_encoder.bias',\n",
       "               tensor([ 0.0648,  0.3688, -0.1839,  0.5569,  0.1748, -0.2276, -0.0474,  0.6555,\n",
       "                        0.0555, -0.4058,  0.0167,  0.1978,  0.4123,  0.2170, -0.0159, -0.0504,\n",
       "                       -0.4221, -0.1428, -0.0151,  0.1712,  0.1391, -0.3173,  0.0122,  0.1732,\n",
       "                        0.0864,  0.2540, -0.3068, -0.1261, -0.1788, -0.3766, -0.1264, -0.2118,\n",
       "                       -0.0676,  0.3516,  0.2715, -0.0146,  0.2229,  0.0652, -0.0417,  0.1734,\n",
       "                        0.1876, -0.1055,  0.2478, -0.2064, -0.0095, -0.0479,  0.0274, -0.2302,\n",
       "                        0.1525, -0.6590,  0.1097, -0.1634,  0.0938, -0.4110,  0.3817, -0.0138,\n",
       "                       -0.0756,  0.0839,  0.0606, -0.0389,  0.0823,  0.3220, -0.3055, -0.3409,\n",
       "                        0.1826,  0.1557,  0.3741, -0.3737,  0.1215,  0.3299,  0.1807,  0.2165,\n",
       "                       -0.2647, -0.1809,  0.4294, -0.0654, -0.0728, -0.6169,  0.4175, -0.0928,\n",
       "                        0.0707, -0.1026,  0.1139, -0.0190, -0.3603,  0.0635, -0.1672,  0.1095,\n",
       "                       -0.0373, -0.3127,  0.1269, -0.1261,  0.3250,  0.1681,  0.2322,  0.0580,\n",
       "                        0.1976,  0.1220, -0.1626,  0.2895,  0.1946, -0.2531, -0.1237, -0.2865,\n",
       "                       -0.0381,  0.0960,  0.3727,  0.2181,  0.1199, -0.0413,  0.0307, -0.0501,\n",
       "                        0.1516, -0.2582,  0.0700, -0.0272,  0.2474, -0.3436, -0.1833, -0.3098,\n",
       "                        0.0974,  0.3069, -0.0143,  0.2895,  0.1134,  0.1908, -0.1580,  0.4729])),\n",
       "              ('regressor.ligand_model.gt_block.0.A.weight',\n",
       "               tensor([[-0.0322, -0.0527, -0.0272,  ..., -0.0139, -0.2265,  0.1451],\n",
       "                       [ 0.0309, -0.2813,  0.1438,  ...,  0.0878, -0.0325,  0.1577],\n",
       "                       [-0.0625,  0.0606,  0.2693,  ..., -0.1185,  0.0353, -0.0384],\n",
       "                       ...,\n",
       "                       [-0.0269, -0.0942,  0.1556,  ..., -0.2321, -0.1232,  0.0136],\n",
       "                       [ 0.0572, -0.2301,  0.0486,  ...,  0.0470,  0.1612, -0.2611],\n",
       "                       [-0.0424, -0.2015, -0.0859,  ...,  0.0348, -0.1018, -0.1096]])),\n",
       "              ('regressor.ligand_model.gt_block.0.A.bias',\n",
       "               tensor([ 0.0428,  0.0343,  0.0390, -0.0105,  0.0221,  0.0247, -0.0821,  0.0328,\n",
       "                        0.0018, -0.0571,  0.0520, -0.0471, -0.0345, -0.0475, -0.0219,  0.0423,\n",
       "                        0.0177, -0.0488,  0.0539, -0.0596,  0.0446, -0.0079, -0.0179, -0.0795,\n",
       "                       -0.0709,  0.0781,  0.0810, -0.0595, -0.0072,  0.0449, -0.0695, -0.0183,\n",
       "                        0.0220, -0.0424, -0.0662, -0.0299, -0.0310, -0.0615, -0.0645, -0.0232,\n",
       "                        0.0028,  0.0677,  0.0606, -0.0213, -0.0297,  0.0012, -0.0351,  0.0145,\n",
       "                       -0.0749, -0.0440,  0.0256,  0.0205,  0.0176, -0.0368, -0.0385, -0.0320,\n",
       "                       -0.0694, -0.0808, -0.0593, -0.0633, -0.0398, -0.0228,  0.0661, -0.0091,\n",
       "                        0.0230, -0.0784,  0.0526,  0.0090,  0.0186,  0.0171,  0.0139,  0.0063,\n",
       "                        0.0017, -0.0522,  0.0867, -0.0729,  0.0309, -0.0632, -0.0376,  0.0402,\n",
       "                       -0.0251,  0.0522,  0.0081, -0.0269, -0.0212, -0.0821, -0.0494,  0.0539,\n",
       "                       -0.0883,  0.0452, -0.0126, -0.0468, -0.0727,  0.0483, -0.0101,  0.0715,\n",
       "                       -0.0406, -0.0312, -0.0311,  0.0814, -0.0335,  0.0705,  0.0813, -0.0223,\n",
       "                       -0.0251, -0.0788, -0.0274, -0.0545,  0.0377,  0.0200, -0.0483,  0.0618,\n",
       "                        0.0588,  0.0675,  0.0018,  0.0492,  0.0011, -0.0093,  0.0232,  0.0812,\n",
       "                       -0.0151,  0.0435,  0.0590,  0.0575,  0.0398,  0.0875, -0.0860, -0.0579])),\n",
       "              ('regressor.ligand_model.gt_block.0.B.weight',\n",
       "               tensor([[-0.0658, -0.1974, -0.1799,  ..., -0.2986,  0.1929, -0.0871],\n",
       "                       [-0.0219, -0.3291, -0.1875,  ..., -0.1617,  0.0133, -0.0237],\n",
       "                       [-0.1872,  0.1679,  0.0235,  ...,  0.2289,  0.0310,  0.0084],\n",
       "                       ...,\n",
       "                       [-0.1509, -0.1409,  0.2669,  ...,  0.0039,  0.1466, -0.2161],\n",
       "                       [-0.0393, -0.3721, -0.0392,  ...,  0.3971, -0.3715,  0.1674],\n",
       "                       [ 0.2678, -0.2139, -0.0415,  ..., -0.3254,  0.0871, -0.2490]])),\n",
       "              ('regressor.ligand_model.gt_block.0.B.bias',\n",
       "               tensor([-0.5490, -0.4942, -0.3723, -0.2642, -0.0887, -0.0144, -0.4093, -0.0066,\n",
       "                       -0.2228,  0.6121,  0.9425,  0.2678,  0.1483, -0.2996, -0.3675,  1.1450,\n",
       "                       -1.1935,  1.0205, -1.1869, -0.0622, -0.0843, -0.0734, -0.1527, -0.4748,\n",
       "                        0.1065, -0.0761,  0.1001, -0.2669, -0.3433, -1.0767,  0.8228,  0.3696,\n",
       "                        0.5431, -0.2252,  0.0739, -0.3434, -0.4847,  0.9565, -0.4976,  0.6423,\n",
       "                        0.2296, -0.9575,  0.3021, -0.2397, -0.4498,  0.4731, -0.5455,  0.2420,\n",
       "                       -0.1372, -0.1019, -0.8745, -0.7622, -0.5963, -0.8055, -0.3261, -0.2613,\n",
       "                        0.0370, -0.3020, -0.1583,  0.6907, -0.3486, -0.6720, -0.0143,  0.3233,\n",
       "                        0.1865,  0.2596, -0.4192, -0.0921, -0.4036, -0.4065, -0.4843, -0.4544,\n",
       "                        0.6798,  0.0764,  0.3806, -0.2652,  0.2174,  0.3827, -0.0646,  0.2923,\n",
       "                       -0.2968,  0.0609, -1.5027, -0.0921,  0.6436,  0.0479,  0.3843, -0.2056,\n",
       "                       -0.4641, -0.3713, -0.0561, -0.2736, -0.5048, -0.4634,  0.3400, -0.3491,\n",
       "                        0.0681, -0.4184, -0.0961, -0.9276,  0.0291,  0.2470, -0.1208,  0.2348,\n",
       "                       -0.4800,  0.1038,  0.5640, -0.7839, -0.8654, -0.5433,  0.5558,  0.9079,\n",
       "                        0.2786, -0.6513, -0.5412, -0.1046,  0.6589, -0.0061, -1.1133,  0.0779,\n",
       "                       -0.1613, -0.0782,  0.4455, -0.6447, -0.1023, -0.9722,  0.6110,  0.3534])),\n",
       "              ('regressor.ligand_model.gt_block.0.C.weight',\n",
       "               tensor([[ 0.1741, -0.2469,  0.1061,  ...,  0.0284,  0.1914, -0.0971],\n",
       "                       [-0.1159,  0.1855,  0.0983,  ...,  0.1339, -0.2053,  0.1854],\n",
       "                       [-0.1099, -0.1218,  0.1621,  ...,  0.1411, -0.1812, -0.1839],\n",
       "                       ...,\n",
       "                       [ 0.0059,  0.0999, -0.0028,  ...,  0.1369, -0.0563, -0.1164],\n",
       "                       [ 0.0505,  0.1770, -0.1555,  ..., -0.0540, -0.0286,  0.1242],\n",
       "                       [-0.0520, -0.1973,  0.2655,  ..., -0.0003, -0.1439, -0.1152]])),\n",
       "              ('regressor.ligand_model.gt_block.0.C.bias',\n",
       "               tensor([ 2.8772e-01, -6.1341e-01, -3.9992e-01,  1.8327e-01, -5.7660e-01,\n",
       "                       -9.9247e-02, -5.9628e-01, -4.0160e-01, -4.3312e-02, -3.4958e-01,\n",
       "                       -3.3446e-01, -2.6430e-01,  2.3256e-02, -3.1912e-01,  1.1101e-01,\n",
       "                        2.7631e-01,  5.6160e-02, -1.8231e-01, -3.2333e-01, -1.4542e-01,\n",
       "                       -3.0404e-01, -3.2019e-01, -5.9961e-01, -4.7296e-01, -3.7608e-01,\n",
       "                       -6.7474e-01, -2.8059e-01, -6.1203e-01,  3.6599e-02,  1.5990e-01,\n",
       "                       -7.9546e-02, -5.6350e-02,  8.9481e-02,  5.6456e-02, -2.4013e-01,\n",
       "                       -2.1783e-01, -1.9164e-01,  5.2424e-01, -5.5976e-01,  4.5838e-02,\n",
       "                       -5.1236e-01, -2.1480e-01, -3.4697e-01, -1.5720e-01, -2.0899e-01,\n",
       "                       -1.7970e-03, -9.5240e-04,  4.0683e-01, -6.3482e-01, -2.7519e-01,\n",
       "                       -2.8202e-01,  1.4158e-01, -4.2782e-02, -1.6997e-01, -7.9564e-01,\n",
       "                       -1.1105e-01, -2.7730e-01,  6.7623e-01, -2.5344e-01, -6.4944e-01,\n",
       "                        1.0414e-01, -2.5223e-02, -2.3884e-01, -7.8115e-01, -1.9613e-01,\n",
       "                        2.3727e-02,  1.9553e-02, -9.1712e-02, -4.3799e-01,  2.6911e-01,\n",
       "                       -1.1291e-01,  4.7048e-03,  1.0960e+00, -2.2091e-01, -7.3305e-01,\n",
       "                        1.1076e-01, -5.8502e-01, -1.2197e-01, -2.8373e-01, -1.8606e-01,\n",
       "                       -2.2110e-01, -2.0103e-01,  5.8482e-01, -2.2569e-01, -9.6014e-01,\n",
       "                       -3.8129e-01,  1.0227e-01, -1.6294e-01, -7.6880e-02, -1.0106e+00,\n",
       "                       -7.8681e-01, -3.1841e-01, -2.6781e-01, -3.1658e-01, -1.7133e-01,\n",
       "                       -5.0193e-01, -7.1054e-01, -4.4502e-01, -2.3161e-01, -2.3956e-01,\n",
       "                       -1.9688e-01, -8.8461e-02, -5.8772e-01,  5.9523e-01, -6.8113e-02,\n",
       "                       -3.5445e-01, -1.6223e-01, -1.5374e-01,  8.0466e-01, -1.6171e-01,\n",
       "                       -4.3870e-02,  7.8466e-01,  4.1757e-02,  1.6018e-01, -1.7176e-01,\n",
       "                       -1.6425e-01, -3.9318e-01, -2.3092e-01,  2.7785e-02,  1.7578e-01,\n",
       "                       -4.0423e-01, -7.0689e-02,  1.2971e-01, -6.5319e-01, -2.6296e-01,\n",
       "                        5.5718e-01, -1.5006e-01, -9.5847e-01])),\n",
       "              ('regressor.ligand_model.gt_block.0.D.weight',\n",
       "               tensor([[ 0.0014, -0.2285, -0.0578,  ...,  0.2885,  0.0910, -0.0085],\n",
       "                       [-0.0434,  0.2123,  0.1353,  ...,  0.0281, -0.0255,  0.1280],\n",
       "                       [-0.0763, -0.2673, -0.0689,  ..., -0.2922,  0.0153, -0.1637],\n",
       "                       ...,\n",
       "                       [-0.1630, -0.2139,  0.0294,  ...,  0.0676, -0.0461,  0.1707],\n",
       "                       [-0.1389,  0.0433,  0.1119,  ...,  0.1299,  0.2009,  0.0125],\n",
       "                       [-0.1086,  0.1532, -0.4170,  ...,  0.1751, -0.2794, -0.0999]])),\n",
       "              ('regressor.ligand_model.gt_block.0.D.bias',\n",
       "               tensor([ 3.1036e-01, -5.4188e-01, -5.0130e-01,  1.6305e-02, -4.8597e-01,\n",
       "                       -1.0567e-01, -6.6803e-01, -5.1797e-01,  1.9424e-02, -3.6854e-01,\n",
       "                       -2.8003e-01, -4.0655e-01, -1.2565e-01, -2.9774e-01,  3.8414e-02,\n",
       "                        2.5119e-01, -3.0384e-02, -1.9138e-01, -3.3059e-01, -1.4435e-01,\n",
       "                       -1.7426e-01, -2.9665e-01, -5.5815e-01, -5.7674e-01, -3.1190e-01,\n",
       "                       -7.0413e-01, -2.0369e-01, -5.2584e-01,  7.8345e-02,  4.9327e-02,\n",
       "                       -1.3095e-02,  5.2367e-05,  1.2492e-01,  9.1919e-02, -1.4500e-01,\n",
       "                       -7.1926e-02, -8.9676e-02,  5.3207e-01, -4.6513e-01,  8.4735e-02,\n",
       "                       -5.3649e-01, -3.0294e-01, -2.7596e-01, -1.9744e-01, -1.1684e-01,\n",
       "                        4.7872e-02,  6.3743e-02,  4.1430e-01, -5.2648e-01, -3.5414e-01,\n",
       "                       -2.6800e-01,  1.2028e-01, -4.7390e-02, -1.7309e-01, -8.7453e-01,\n",
       "                       -9.0973e-03, -3.5003e-01,  7.3514e-01, -2.3556e-01, -6.6908e-01,\n",
       "                        8.9292e-02,  5.3796e-03, -1.7769e-01, -7.7005e-01, -1.3611e-01,\n",
       "                        1.4377e-01,  7.6999e-02, -1.9284e-01, -4.5535e-01,  1.9709e-01,\n",
       "                       -2.1836e-01,  8.0570e-02,  1.0823e+00, -2.4182e-01, -7.2049e-01,\n",
       "                        8.7143e-02, -5.6326e-01, -1.6337e-01, -2.1664e-01, -1.5700e-01,\n",
       "                       -1.2933e-01, -1.9267e-01,  5.1924e-01, -1.8589e-01, -9.8917e-01,\n",
       "                       -2.6660e-01,  2.2720e-02, -6.7185e-02, -1.2002e-01, -1.1223e+00,\n",
       "                       -8.6361e-01, -2.8148e-01, -2.2417e-01, -2.9683e-01, -1.4659e-01,\n",
       "                       -4.2256e-01, -6.0779e-01, -3.1136e-01, -1.8037e-01, -3.9002e-01,\n",
       "                       -1.6455e-01, -1.8149e-01, -6.8979e-01,  4.6095e-01, -2.1525e-02,\n",
       "                       -3.6009e-01, -2.4612e-01, -2.9957e-01,  8.3410e-01, -1.0269e-01,\n",
       "                        2.2361e-02,  6.8838e-01, -6.3753e-02,  1.6205e-01, -1.3663e-01,\n",
       "                       -2.5213e-01, -4.6296e-01, -3.6866e-01,  1.8408e-01,  1.3278e-01,\n",
       "                       -5.0509e-01, -1.4020e-01,  1.1596e-01, -6.3505e-01, -3.6697e-01,\n",
       "                        6.5532e-01, -1.9129e-01, -9.0941e-01])),\n",
       "              ('regressor.ligand_model.gt_block.0.E.weight',\n",
       "               tensor([[-0.2623, -0.3145,  0.3538,  ...,  0.0874, -0.1160, -0.1448],\n",
       "                       [ 0.0116, -0.0836, -0.3483,  ...,  0.2444,  0.2444,  0.1599],\n",
       "                       [ 0.0708, -0.2996, -0.1954,  ...,  0.3253, -0.3456, -0.1140],\n",
       "                       ...,\n",
       "                       [ 0.0037, -0.2468,  0.0663,  ...,  0.1875,  0.1853, -0.2341],\n",
       "                       [-0.1082,  0.1084,  0.2728,  ...,  0.0405, -0.3502,  0.3821],\n",
       "                       [ 0.0057,  0.0417,  0.4945,  ..., -0.1122,  0.2633, -0.2185]])),\n",
       "              ('regressor.ligand_model.gt_block.0.E.bias',\n",
       "               tensor([ 0.2797, -0.5539, -0.4130,  0.1131, -0.5402, -0.0575, -0.7393, -0.4202,\n",
       "                       -0.0672, -0.3168, -0.2228, -0.2422, -0.1282, -0.3532,  0.0192,  0.2958,\n",
       "                       -0.0743, -0.2723, -0.2860, -0.0679, -0.2876, -0.2803, -0.5307, -0.5054,\n",
       "                       -0.2543, -0.6225, -0.3102, -0.6344,  0.0291,  0.2076,  0.0180, -0.0341,\n",
       "                        0.0984,  0.0437, -0.2342, -0.1137, -0.0963,  0.5919, -0.4596,  0.0835,\n",
       "                       -0.5201, -0.3057, -0.3593, -0.0879, -0.1769, -0.0702, -0.0365,  0.4445,\n",
       "                       -0.5696, -0.3928, -0.3609,  0.0395, -0.0603, -0.2657, -0.8285,  0.0345,\n",
       "                       -0.3062,  0.7960, -0.2767, -0.6652,  0.0145, -0.0092, -0.1909, -0.7327,\n",
       "                       -0.1192,  0.1315, -0.0155, -0.2228, -0.3214,  0.1766, -0.2570,  0.1087,\n",
       "                        1.1623, -0.2552, -0.7318,  0.0671, -0.5581, -0.0736, -0.2373, -0.2657,\n",
       "                       -0.2157, -0.2391,  0.5073, -0.1922, -0.9164, -0.3385,  0.0805, -0.0509,\n",
       "                       -0.1054, -1.0760, -0.7650, -0.4038, -0.1647, -0.3927, -0.2586, -0.4614,\n",
       "                       -0.6407, -0.4279, -0.0998, -0.3047, -0.1935, -0.1576, -0.6863,  0.4425,\n",
       "                        0.0073, -0.3742, -0.1321, -0.2544,  0.6778, -0.1857, -0.0028,  0.7720,\n",
       "                        0.0612,  0.1327, -0.1641, -0.1849, -0.4267, -0.3170,  0.1351,  0.1562,\n",
       "                       -0.4017, -0.1880,  0.1495, -0.6930, -0.3236,  0.5865, -0.1394, -1.0156])),\n",
       "              ('regressor.ligand_model.gt_block.0.bn_node_x.weight',\n",
       "               tensor([0.6926, 0.9104, 0.7596, 0.8301, 0.7087, 0.6243, 0.8064, 0.6623, 0.6569,\n",
       "                       0.6253, 0.6234, 0.6997, 0.9801, 0.9586, 0.6615, 0.6571, 0.9595, 0.6794,\n",
       "                       0.7033, 0.6745, 0.7384, 0.5867, 0.7016, 0.6880, 0.7868, 0.9077, 0.9598,\n",
       "                       0.6091, 0.6840, 0.6997, 0.6428, 0.5940, 0.7535, 0.8528, 0.7275, 0.7141,\n",
       "                       0.5613, 0.4839, 0.6990, 0.7251, 0.7421, 0.5414, 0.8133, 0.8452, 0.7785,\n",
       "                       0.7342, 0.7870, 0.9468, 0.9114, 0.8281, 0.8440, 0.6689, 0.7004, 0.6835,\n",
       "                       0.7427, 0.8997, 0.8715, 0.7422, 0.9814, 0.7223, 0.6551, 0.8112, 0.7468,\n",
       "                       0.6702, 0.7369, 0.7129, 0.6332, 0.6658, 0.7869, 0.6351, 0.6866, 0.7357,\n",
       "                       0.6234, 0.7805, 0.7334, 0.5213, 0.7405, 0.8849, 0.5801, 0.7831, 0.5640,\n",
       "                       0.6232, 0.8841, 0.5561, 0.9491, 0.5889, 0.8679, 0.5916, 0.8519, 0.5788,\n",
       "                       0.7320, 0.8646, 0.6831, 0.7485, 0.7394, 0.7813, 0.5677, 0.6993, 0.7443,\n",
       "                       0.6682, 0.7832, 0.5207, 0.8576, 0.8937, 0.4580, 0.7303, 0.7380, 0.6568,\n",
       "                       0.7386, 0.7904, 0.8766, 0.5989, 0.8124, 0.8708, 0.7964, 0.6555, 0.5914,\n",
       "                       0.6958, 0.6171, 0.6800, 0.8181, 0.7186, 0.5954, 0.7554, 0.6106, 0.6847,\n",
       "                       0.6144, 0.7088])),\n",
       "              ('regressor.ligand_model.gt_block.0.bn_node_x.bias',\n",
       "               tensor([-0.6845, -0.8339, -0.5631, -0.6438, -0.5479, -0.4034, -0.6899, -0.4762,\n",
       "                       -0.4185, -0.3045, -0.3120, -0.7077, -0.3957, -0.7888, -0.5253, -0.5612,\n",
       "                       -0.4082, -0.7007, -0.5423, -0.6230, -0.6710, -0.6614, -0.5721, -0.5745,\n",
       "                       -0.7516, -0.5956, -0.6400, -0.4818, -0.4519, -0.4149, -0.5146, -0.3805,\n",
       "                       -0.6618, -0.6813, -0.6017, -0.5646, -0.5496, -0.4243, -0.3773, -0.5303,\n",
       "                       -0.5308, -0.4810, -0.4322, -0.5749, -0.7343, -0.6112, -0.4795, -0.5697,\n",
       "                       -0.5562, -0.5616, -0.4909, -0.5537, -0.5778, -0.3932, -0.3801, -0.5900,\n",
       "                       -0.5470, -0.4610, -0.7258, -0.3953, -0.4267, -0.6253, -0.5040, -0.6894,\n",
       "                       -0.7463, -0.6707, -0.2965, -0.5029, -0.5859, -0.5817, -0.7332, -0.6538,\n",
       "                       -0.5220, -0.7345, -0.5171, -0.5423, -0.5246, -0.5776, -0.5099, -0.6074,\n",
       "                       -0.3742, -0.5295, -0.5489, -0.4456, -0.6791, -0.5498, -0.6850, -0.3935,\n",
       "                       -0.7357, -0.4222, -0.5349, -0.7845, -0.4243, -0.4638, -0.5071, -0.5003,\n",
       "                       -0.5571, -0.4615, -0.5942, -0.5886, -0.4719, -0.3873, -0.6589, -0.2449,\n",
       "                       -0.5172, -0.5900, -0.6677, -0.5460, -0.6270, -0.5135, -0.6041, -0.4017,\n",
       "                       -0.6985, -0.4853, -0.5094, -0.4070, -0.5774, -0.4942, -0.6292, -0.7329,\n",
       "                       -0.5536, -0.4556, -0.6327, -0.5419, -0.5904, -0.3607, -0.5580, -0.6673])),\n",
       "              ('regressor.ligand_model.gt_block.0.bn_node_x.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "              ('regressor.ligand_model.gt_block.0.bn_node_x.running_var',\n",
       "               tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1.])),\n",
       "              ('regressor.ligand_model.gt_block.0.bn_node_x.num_batches_tracked',\n",
       "               tensor(0)),\n",
       "              ('regressor.ligand_model.gt_block.0.bn_edge_e.weight',\n",
       "               tensor([0.7557, 0.8277, 0.9016, 0.9695, 0.8323, 0.6683, 0.7830, 0.6814, 0.7173,\n",
       "                       0.7653, 0.7457, 0.8995, 0.7531, 0.8982, 0.7121, 0.8441, 0.9570, 1.0235,\n",
       "                       0.7325, 0.9217, 0.5647, 0.8146, 1.0657, 0.8497, 0.9020, 0.5028, 0.9121,\n",
       "                       0.8420, 0.6535, 0.9107, 0.9298, 0.8986, 0.8273, 0.8079, 0.9275, 0.8246,\n",
       "                       0.6834, 0.8893, 0.9305, 0.7133, 0.6828, 1.0125, 0.9459, 0.8202, 0.8153,\n",
       "                       0.8675, 0.7956, 0.9292, 0.8707, 0.9677, 0.7708, 0.7437, 0.9308, 0.8593,\n",
       "                       0.8525, 0.9436, 0.8697, 0.8915, 0.6008, 0.8610, 0.8237, 0.9818, 0.8747,\n",
       "                       0.7128, 0.8646, 0.9439, 0.6941, 0.8800, 0.5253, 0.8601, 0.9537, 0.8092,\n",
       "                       0.9141, 0.9460, 0.9281, 1.0259, 0.7572, 0.8194, 0.9463, 0.9830, 0.8818,\n",
       "                       0.5351, 0.8726, 0.7712, 1.0357, 0.9136, 0.8650, 0.5444, 0.8302, 0.7126,\n",
       "                       0.7952, 0.5747, 0.8272, 0.7242, 0.8890, 0.8482, 0.8709, 0.6910, 0.8298,\n",
       "                       0.9696, 0.9561, 0.7620, 0.6247, 0.8895, 0.8033, 0.8217, 0.8165, 0.8527,\n",
       "                       0.9530, 0.6681, 0.7374, 0.9839, 0.8849, 0.8820, 0.7789, 0.9381, 0.7817,\n",
       "                       0.7473, 0.9563, 0.6429, 0.8405, 0.6961, 0.8484, 0.6220, 0.9010, 0.9717,\n",
       "                       0.8502, 0.8988])),\n",
       "              ('regressor.ligand_model.gt_block.0.bn_edge_e.bias',\n",
       "               tensor([-0.6261, -0.9769, -0.7688, -0.7725, -0.7445, -0.5633, -0.6084, -0.5113,\n",
       "                       -0.7428, -0.7595, -0.4279, -0.7755, -0.5941, -0.5967, -0.6232, -0.6798,\n",
       "                       -0.7722, -0.7088, -0.6727, -0.7811, -0.5253, -0.6301, -0.7744, -0.6301,\n",
       "                       -0.8229, -0.5746, -0.7879, -0.6597, -0.4557, -0.7583, -0.7930, -0.6967,\n",
       "                       -0.8007, -0.5318, -1.0117, -0.7905, -0.5285, -0.6434, -0.8154, -0.6018,\n",
       "                       -0.5410, -0.8779, -0.8110, -0.5365, -0.5942, -0.6832, -0.7220, -0.8443,\n",
       "                       -0.8915, -0.8352, -0.6306, -0.7380, -0.8623, -0.7084, -0.7151, -0.7547,\n",
       "                       -0.8735, -0.7297, -0.5049, -0.6418, -0.4951, -0.8677, -0.8646, -0.6278,\n",
       "                       -0.7904, -0.8539, -0.4985, -0.9416, -0.5617, -0.7022, -0.8027, -0.7589,\n",
       "                       -0.8184, -0.9614, -0.5809, -0.7984, -0.6335, -0.7643, -0.8021, -0.8894,\n",
       "                       -0.8334, -0.6487, -0.8680, -0.5489, -0.9032, -0.7856, -0.7593, -0.6423,\n",
       "                       -0.8410, -0.7983, -0.7245, -0.4500, -0.7152, -0.4798, -0.5849, -0.8885,\n",
       "                       -0.6561, -0.6737, -0.6772, -0.5756, -0.8232, -0.4645, -0.4970, -0.7539,\n",
       "                       -0.5245, -0.6132, -0.6199, -0.8181, -0.7260, -0.5087, -0.5893, -0.8361,\n",
       "                       -0.6185, -0.8491, -0.5228, -0.7395, -0.7162, -0.5716, -0.6673, -0.6065,\n",
       "                       -0.6552, -0.4984, -0.6451, -0.3911, -0.8025, -0.8800, -0.4695, -0.7692])),\n",
       "              ('regressor.ligand_model.gt_block.0.bn_edge_e.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "              ('regressor.ligand_model.gt_block.0.bn_edge_e.running_var',\n",
       "               tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1.])),\n",
       "              ('regressor.ligand_model.gt_block.0.bn_edge_e.num_batches_tracked',\n",
       "               tensor(0)),\n",
       "              ('regressor.ligand_model.gt_block.1.A.weight',\n",
       "               tensor([[-7.5987e-01, -8.4661e-02, -2.5301e-01,  ...,  1.8660e-01,\n",
       "                        -5.4101e-01,  2.9551e-02],\n",
       "                       [ 5.2840e-02, -1.0105e+00,  7.4106e-02,  ...,  2.2971e-02,\n",
       "                         1.1115e-01,  5.3375e-02],\n",
       "                       [ 8.9883e-02, -3.8250e-01, -1.0840e+00,  ..., -8.3501e-02,\n",
       "                        -4.0526e-02,  9.4594e-02],\n",
       "                       ...,\n",
       "                       [-1.0608e-01, -7.0513e-02, -4.2082e-02,  ..., -8.1501e-01,\n",
       "                        -2.2169e-01, -1.8257e-01],\n",
       "                       [-1.8081e-01,  1.2523e-01,  1.7310e-01,  ...,  1.7999e-01,\n",
       "                        -1.3071e-01, -1.9075e-01],\n",
       "                       [-2.9204e-01, -2.2797e-01, -1.2585e-01,  ..., -9.5922e-02,\n",
       "                         5.6669e-04, -8.0256e-01]])),\n",
       "              ('regressor.ligand_model.gt_block.1.A.bias',\n",
       "               tensor([ 0.0818, -0.0130,  0.0527, -0.0836, -0.0099,  0.0650, -0.0673,  0.0905,\n",
       "                       -0.0551,  0.0667,  0.0028,  0.0689, -0.0728,  0.0090, -0.0262, -0.0622,\n",
       "                        0.0307, -0.0436,  0.0740, -0.0005,  0.0261, -0.0361,  0.0581,  0.0604,\n",
       "                        0.0037, -0.0352, -0.0602,  0.0171, -0.0615,  0.0541, -0.0683,  0.0197,\n",
       "                        0.0202,  0.0434,  0.0555, -0.0229, -0.0667, -0.0476,  0.0380, -0.0852,\n",
       "                        0.0269,  0.0748, -0.0713,  0.0546, -0.0453, -0.0717,  0.0476, -0.0714,\n",
       "                       -0.0120,  0.0058, -0.0048, -0.0623,  0.0703, -0.0761, -0.0577, -0.0772,\n",
       "                        0.0056, -0.0375, -0.0428, -0.0768,  0.0069,  0.0792,  0.0179,  0.0445,\n",
       "                       -0.0448,  0.0376, -0.0845,  0.0451, -0.0843, -0.0107,  0.0612, -0.0589,\n",
       "                        0.0235, -0.0029, -0.0040, -0.0573,  0.0464, -0.0583, -0.0666, -0.0168,\n",
       "                       -0.0276,  0.0494,  0.0663,  0.0040, -0.0580,  0.0175,  0.0596, -0.0854,\n",
       "                       -0.0618, -0.0488, -0.0449, -0.0634, -0.0713,  0.0684, -0.0499, -0.0778,\n",
       "                       -0.0086,  0.0440, -0.0137, -0.0130,  0.0183,  0.0186, -0.0047, -0.0444,\n",
       "                        0.0282,  0.0590, -0.0673,  0.0211, -0.0670,  0.0091, -0.0107, -0.0843,\n",
       "                       -0.0711, -0.0523, -0.0231, -0.0062, -0.0400, -0.0001,  0.0863,  0.0286,\n",
       "                       -0.0348,  0.0545,  0.0158, -0.0197, -0.0157,  0.0433,  0.0578, -0.0760])),\n",
       "              ('regressor.ligand_model.gt_block.1.B.weight',\n",
       "               tensor([[-0.1678, -0.0689, -0.0157,  ...,  0.1517,  0.0916, -0.1169],\n",
       "                       [ 0.0846, -0.2173, -0.0963,  ...,  0.3674,  0.0503, -0.1302],\n",
       "                       [-0.2042, -0.2252, -0.2433,  ...,  0.3795, -0.1670,  0.1074],\n",
       "                       ...,\n",
       "                       [-0.3041, -0.1762,  0.1865,  ..., -0.0887, -0.1423, -0.3106],\n",
       "                       [ 0.2553,  0.3944, -0.4888,  ..., -0.2181,  0.2523,  0.0549],\n",
       "                       [ 0.2760,  0.1678, -0.1566,  ..., -0.2986,  0.0049, -0.3678]])),\n",
       "              ('regressor.ligand_model.gt_block.1.B.bias',\n",
       "               tensor([-1.3625, -0.7030,  0.4737,  0.5324,  0.4599, -0.1269,  0.6005, -0.2225,\n",
       "                       -0.2379,  0.8080, -0.2243,  0.3754,  0.4448, -0.5922,  0.3792,  0.3580,\n",
       "                       -0.8604,  0.3934, -0.5405,  1.2767, -0.2871,  0.0162, -0.7698, -0.3883,\n",
       "                       -0.5517, -0.3830,  0.1333,  1.0624,  0.2104, -0.3035, -1.6287,  1.4767,\n",
       "                       -0.4283, -0.0611,  0.1685,  0.3267, -0.0072,  0.4406, -0.8248,  0.1292,\n",
       "                       -0.2495, -1.4016,  0.2339,  0.5707, -0.5238, -0.0380, -0.6507, -0.2535,\n",
       "                       -0.1792,  0.1115, -0.1291,  0.0356, -0.4832, -0.1279, -0.4213,  0.3361,\n",
       "                        0.3797,  1.1358,  0.3804,  0.0730,  0.0837, -0.3360,  0.0207,  0.9152,\n",
       "                        0.3791,  1.0630, -0.1397, -0.2469,  0.2408,  0.1526,  0.5909,  0.1341,\n",
       "                        0.3788,  0.4293,  0.0368,  0.9241, -0.1412,  0.1267,  0.6324, -0.0373,\n",
       "                        0.3787,  0.5146,  0.3380,  0.0346,  0.3846, -0.3737,  0.2300,  0.3641,\n",
       "                        0.5787, -0.4764,  0.2398,  0.2308, -0.0489,  0.5003,  0.6592, -0.2508,\n",
       "                        0.3233, -0.9638,  0.0511, -0.3668, -0.0227,  0.1208, -0.3709, -0.0671,\n",
       "                       -0.1989,  0.4294, -0.7340,  0.6721, -1.1453, -0.1192,  1.1126,  1.0813,\n",
       "                       -0.1897, -0.9643,  0.5560,  0.6602,  0.3760,  0.6387,  0.5291,  0.5826,\n",
       "                       -0.1733, -0.2473,  0.4665, -0.3021, -0.9416, -0.6195, -0.4717,  0.4769])),\n",
       "              ('regressor.ligand_model.gt_block.1.C.weight',\n",
       "               tensor([[-0.5638, -0.1817,  0.3253,  ..., -0.0178,  0.1937, -0.0435],\n",
       "                       [-0.4893,  0.1185, -0.0132,  ...,  0.0634, -0.2788, -0.2315],\n",
       "                       [ 0.1527, -0.2179, -0.4092,  ..., -0.1397, -0.2501, -0.1102],\n",
       "                       ...,\n",
       "                       [-0.0817, -0.1015, -0.0656,  ..., -0.0446,  0.0213,  0.0517],\n",
       "                       [ 0.0876,  0.0150,  0.1710,  ..., -0.0188,  0.1636, -0.3003],\n",
       "                       [-0.1562,  0.0989,  0.2976,  ..., -0.0478,  0.2962,  0.1573]])),\n",
       "              ('regressor.ligand_model.gt_block.1.C.bias',\n",
       "               tensor([ 0.0108, -0.1717, -0.3482,  0.0160, -0.2967,  0.0445, -0.5380, -0.0175,\n",
       "                       -0.0706, -0.4331, -0.5963, -0.5405,  0.0564, -0.2260, -0.0705, -0.6735,\n",
       "                       -0.0579, -0.2292, -0.1991, -0.3179, -0.9876, -0.4613, -0.3816,  0.0850,\n",
       "                       -0.4393, -0.9055, -0.6410, -0.4466, -0.5296, -0.8867, -0.5091, -0.1305,\n",
       "                        0.1040, -0.4039, -0.4044, -0.1307, -0.6530, -0.2362, -0.2884, -0.9518,\n",
       "                       -0.0795, -0.0498, -0.4037, -0.0811, -0.0948, -0.1617, -0.0125, -0.0022,\n",
       "                       -0.4048, -0.4521,  0.0454, -0.4124, -0.0773, -0.0372, -0.0452, -0.1328,\n",
       "                       -0.2143, -0.1317, -0.4138, -0.9916, -0.1275, -0.4586, -0.1237, -0.3558,\n",
       "                        0.0034, -0.0958, -0.0834, -0.7643, -0.0814, -0.9713,  0.0627,  0.4114,\n",
       "                        0.0069, -0.4737, -0.3186, -0.1403, -0.1098, -0.0176,  0.0275, -0.1538,\n",
       "                       -0.0736, -0.3661, -0.0083, -0.5618, -0.3355, -0.1985, -0.7411, -0.4841,\n",
       "                       -0.2354, -0.6426, -0.3921, -0.0950, -0.0104, -0.0079, -1.0926,  0.0842,\n",
       "                       -0.1521, -1.0856, -0.4951, -0.0143, -0.0073,  0.0805, -0.2030,  0.0930,\n",
       "                       -0.2428, -0.6269, -0.6971, -0.4785, -0.6189, -0.1850, -0.2460,  0.0462,\n",
       "                       -0.3486, -0.2437, -0.8067, -0.4411, -0.0460, -0.2780, -0.0716, -0.1903,\n",
       "                       -0.2250, -0.0375, -0.1615, -0.6960, -0.7958, -0.2341, -0.2074, -0.0668])),\n",
       "              ('regressor.ligand_model.gt_block.1.D.weight',\n",
       "               tensor([[-0.3203,  0.0769,  0.4414,  ...,  0.4407,  0.2537,  0.0369],\n",
       "                       [-0.4073,  0.0834,  0.0069,  ...,  0.0149, -0.0800, -0.4669],\n",
       "                       [-0.0966, -0.3721,  0.0264,  ...,  0.1889, -0.0189, -0.1582],\n",
       "                       ...,\n",
       "                       [-0.1648, -0.1162,  0.1766,  ..., -0.0516, -0.1315,  0.6266],\n",
       "                       [ 0.0988,  0.3821, -0.0271,  ...,  0.2003, -0.0103,  0.2377],\n",
       "                       [-0.0190,  0.4631,  0.1287,  ...,  0.5229, -0.1452, -0.1294]])),\n",
       "              ('regressor.ligand_model.gt_block.1.D.bias',\n",
       "               tensor([ 0.0385, -0.3272, -0.3550,  0.0825, -0.3241, -0.0720, -0.4481,  0.0469,\n",
       "                       -0.0415, -0.5654, -0.6247, -0.4273,  0.0789, -0.2516,  0.0636, -0.6483,\n",
       "                       -0.1066, -0.2304, -0.2281, -0.2537, -0.8553, -0.4354, -0.2604,  0.1212,\n",
       "                       -0.4442, -0.9197, -0.6200, -0.4505, -0.5210, -0.9996, -0.4244, -0.0454,\n",
       "                        0.1238, -0.4499, -0.4569, -0.1567, -0.4889, -0.3664, -0.1911, -1.0633,\n",
       "                       -0.0953, -0.0096, -0.3688, -0.0840, -0.0927, -0.2013, -0.0614, -0.1366,\n",
       "                       -0.4031, -0.4633,  0.1324, -0.3928, -0.0158, -0.1523, -0.0637, -0.1870,\n",
       "                       -0.0812, -0.0377, -0.3704, -1.1278, -0.0886, -0.4259, -0.0512, -0.2963,\n",
       "                       -0.0865, -0.1384, -0.1560, -0.6553, -0.1197, -0.9643,  0.0487,  0.2979,\n",
       "                       -0.0614, -0.5059, -0.3814, -0.2422, -0.0273, -0.0240,  0.0542, -0.2445,\n",
       "                       -0.0332, -0.3818, -0.0052, -0.5338, -0.2340, -0.2275, -0.5962, -0.4653,\n",
       "                       -0.2479, -0.5571, -0.4010, -0.0740, -0.1310,  0.0873, -1.0920,  0.1187,\n",
       "                       -0.0161, -1.0429, -0.4888,  0.0166, -0.0309,  0.1474, -0.1304, -0.0510,\n",
       "                       -0.1777, -0.6959, -0.7405, -0.3596, -0.6569, -0.1353, -0.2467,  0.0532,\n",
       "                       -0.2059, -0.1917, -0.6604, -0.4372, -0.0208, -0.3768, -0.1461, -0.1687,\n",
       "                       -0.3595, -0.0419, -0.0110, -0.7692, -0.8587, -0.2488, -0.0521, -0.0884])),\n",
       "              ('regressor.ligand_model.gt_block.1.E.weight',\n",
       "               tensor([[-0.0581, -0.0856,  0.0911,  ..., -0.1573,  0.3164,  0.0632],\n",
       "                       [ 0.0252,  0.0463, -0.0270,  ..., -0.1925, -0.1577, -0.0862],\n",
       "                       [-0.0745, -0.1522, -0.0406,  ..., -0.2785, -0.4268,  0.2174],\n",
       "                       ...,\n",
       "                       [-0.0864, -0.4897,  0.0458,  ...,  0.5522, -0.2329, -0.5777],\n",
       "                       [ 0.0546, -0.0206, -0.0764,  ...,  0.0408, -0.3969,  0.1695],\n",
       "                       [-0.4706,  0.1469, -0.0624,  ..., -0.5720,  0.4553, -0.3179]])),\n",
       "              ('regressor.ligand_model.gt_block.1.E.bias',\n",
       "               tensor([-9.2823e-02, -2.7155e-01, -3.4343e-01,  2.4352e-03, -3.5150e-01,\n",
       "                       -4.5180e-05, -4.0304e-01,  1.8035e-02, -7.8580e-02, -5.5146e-01,\n",
       "                       -5.5339e-01, -5.0772e-01,  6.9039e-02, -2.5010e-01, -8.0102e-02,\n",
       "                       -6.6874e-01, -1.3109e-01, -1.3677e-01, -2.6247e-01, -3.9646e-01,\n",
       "                       -8.4001e-01, -4.3425e-01, -3.3154e-01, -1.8357e-02, -4.9392e-01,\n",
       "                       -8.2343e-01, -7.0888e-01, -5.9156e-01, -4.7652e-01, -1.0115e+00,\n",
       "                       -3.5957e-01, -5.4163e-02,  9.4255e-02, -3.3373e-01, -4.2226e-01,\n",
       "                       -1.8543e-01, -5.0444e-01, -2.9989e-01, -1.3748e-01, -1.0717e+00,\n",
       "                       -1.5418e-01, -3.6578e-02, -4.2289e-01, -8.2905e-02,  2.2879e-02,\n",
       "                       -1.5965e-01, -4.0725e-02,  9.2573e-04, -3.4952e-01, -5.4434e-01,\n",
       "                        7.6709e-02, -4.4814e-01, -1.5761e-01, -5.8095e-02, -7.1025e-02,\n",
       "                       -1.9513e-01, -1.4501e-01, -9.0757e-02, -3.8207e-01, -1.0980e+00,\n",
       "                       -1.6551e-01, -4.9274e-01, -1.3845e-01, -2.8881e-01, -9.1974e-02,\n",
       "                       -1.5008e-01, -1.5404e-01, -6.8515e-01, -1.1361e-01, -9.7025e-01,\n",
       "                        1.4118e-01,  3.9968e-01,  2.4308e-02, -4.9004e-01, -3.8123e-01,\n",
       "                       -2.2691e-01, -2.0910e-02, -8.4928e-02,  7.7539e-02, -1.0886e-01,\n",
       "                        2.0126e-02, -2.8558e-01, -1.5285e-02, -5.2179e-01, -3.0313e-01,\n",
       "                       -2.2268e-01, -6.6743e-01, -4.6294e-01, -1.8215e-01, -7.1192e-01,\n",
       "                       -3.7517e-01, -7.1965e-02, -6.2342e-02,  8.3400e-02, -1.2158e+00,\n",
       "                        1.2521e-01, -2.5416e-02, -1.1486e+00, -5.3923e-01,  1.2677e-02,\n",
       "                        3.5308e-04,  1.2503e-01, -2.1294e-01, -5.4561e-02, -1.3175e-01,\n",
       "                       -5.4158e-01, -7.6045e-01, -3.5606e-01, -6.8711e-01, -2.6618e-01,\n",
       "                       -3.5326e-01,  6.6770e-02, -2.5839e-01, -1.3322e-01, -7.7855e-01,\n",
       "                       -4.6108e-01, -2.3811e-02, -2.6091e-01, -1.1865e-01, -1.2467e-01,\n",
       "                       -3.6203e-01, -1.0152e-01, -1.0949e-01, -7.8758e-01, -8.9509e-01,\n",
       "                       -2.0269e-01, -2.0584e-01, -2.2922e-01])),\n",
       "              ('regressor.ligand_model.gt_block.1.bn_node_x.weight',\n",
       "               tensor([0.7385, 0.7383, 0.7725, 0.7792, 0.7984, 0.8128, 0.5908, 0.8220, 0.7194,\n",
       "                       0.8626, 0.9451, 0.6386, 0.8955, 0.8491, 0.7706, 0.8108, 0.7339, 0.6365,\n",
       "                       0.9163, 0.7255, 0.7425, 0.7889, 0.9594, 1.0518, 0.8374, 0.8146, 0.9255,\n",
       "                       0.8163, 0.8384, 0.8208, 0.7329, 0.6893, 0.8120, 0.7826, 0.8909, 0.8454,\n",
       "                       0.8701, 0.7217, 0.8425, 0.7702, 0.8651, 0.9182, 0.8573, 0.8272, 0.7451,\n",
       "                       0.7927, 0.9570, 0.9169, 0.8962, 0.7206, 0.7325, 0.7970, 0.7745, 0.8564,\n",
       "                       0.8712, 0.7088, 0.8158, 0.8232, 0.7387, 0.9829, 0.7784, 0.8456, 0.7413,\n",
       "                       0.9383, 0.7378, 0.8608, 0.6126, 0.6569, 0.6708, 0.7318, 0.7327, 0.7017,\n",
       "                       0.7794, 0.7681, 0.8305, 1.0392, 0.6653, 0.8554, 0.5691, 0.9022, 0.7549,\n",
       "                       0.8096, 0.7614, 0.8172, 0.8546, 0.7886, 0.9176, 0.6320, 0.7005, 1.0335,\n",
       "                       1.0568, 0.7379, 0.8837, 0.8874, 0.8078, 0.8476, 0.6997, 0.9706, 0.6787,\n",
       "                       0.8503, 0.8305, 0.7679, 0.8426, 0.8266, 0.9069, 0.7666, 0.7359, 0.8846,\n",
       "                       0.7231, 0.9215, 0.7574, 0.7552, 0.7343, 0.9492, 0.8430, 1.0221, 0.9394,\n",
       "                       0.7637, 0.7294, 0.6799, 0.7798, 0.7727, 0.9146, 0.9125, 0.6468, 1.0055,\n",
       "                       0.8457, 0.7038])),\n",
       "              ('regressor.ligand_model.gt_block.1.bn_node_x.bias',\n",
       "               tensor([-0.7244, -0.5632, -0.5296, -0.6799, -0.6150, -0.6283, -0.5014, -0.7413,\n",
       "                       -0.5389, -0.5471, -0.6367, -0.6082, -0.5285, -0.5933, -0.9160, -0.5957,\n",
       "                       -0.5104, -0.4990, -0.6551, -0.6191, -0.7094, -0.6528, -0.8378, -0.9511,\n",
       "                       -0.8941, -0.6536, -0.6668, -0.5343, -0.6749, -0.6710, -0.5866, -0.5876,\n",
       "                       -0.5937, -0.6557, -0.4637, -0.6893, -0.8378, -0.4652, -0.5320, -0.6277,\n",
       "                       -0.5410, -0.6931, -0.5775, -0.6840, -0.7050, -0.5564, -0.6226, -0.6623,\n",
       "                       -0.7111, -0.5998, -0.4861, -0.7112, -0.6378, -0.5726, -0.6164, -0.5390,\n",
       "                       -0.7830, -0.4907, -0.6777, -0.5602, -0.5455, -0.6893, -0.5381, -0.5863,\n",
       "                       -0.6465, -0.6048, -0.4255, -0.4449, -0.5097, -0.5616, -0.8721, -0.6445,\n",
       "                       -0.5741, -0.7579, -0.5432, -0.6326, -0.6634, -0.6850, -0.6037, -0.8250,\n",
       "                       -0.4775, -0.7158, -0.2651, -0.5082, -0.8188, -0.5590, -0.8055, -0.5603,\n",
       "                       -0.5692, -0.7692, -1.0166, -0.6391, -0.5297, -0.5310, -0.5394, -0.6933,\n",
       "                       -0.5353, -0.6063, -0.5123, -0.8094, -0.4938, -0.6222, -0.7987, -0.3810,\n",
       "                       -0.5705, -0.5112, -0.5570, -0.7334, -0.6219, -0.5911, -0.6262, -0.6341,\n",
       "                       -0.6108, -0.6195, -0.5555, -0.5333, -0.6644, -0.6279, -0.6079, -0.5438,\n",
       "                       -0.5738, -0.4235, -0.8737, -0.7118, -0.4892, -0.6569, -0.5985, -0.4988])),\n",
       "              ('regressor.ligand_model.gt_block.1.bn_node_x.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "              ('regressor.ligand_model.gt_block.1.bn_node_x.running_var',\n",
       "               tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1.])),\n",
       "              ('regressor.ligand_model.gt_block.1.bn_node_x.num_batches_tracked',\n",
       "               tensor(0)),\n",
       "              ('regressor.ligand_model.gt_block.1.bn_edge_e.weight',\n",
       "               tensor([0.9749, 0.6564, 0.7720, 0.9788, 0.6618, 0.9218, 0.9525, 0.9933, 0.3457,\n",
       "                       0.8151, 0.6641, 1.0311, 0.7436, 0.8844, 0.9393, 0.8104, 1.0230, 0.9521,\n",
       "                       1.0037, 0.8076, 0.9619, 0.8258, 1.1695, 1.0697, 0.8091, 1.0639, 1.0277,\n",
       "                       0.9522, 0.9282, 0.8590, 1.0467, 0.9723, 0.9669, 0.9024, 0.6295, 1.0061,\n",
       "                       0.9722, 0.9631, 0.8809, 0.8080, 1.0649, 0.6867, 0.5522, 1.1985, 0.8528,\n",
       "                       0.9824, 0.9558, 0.6378, 0.8505, 1.0643, 1.1174, 0.7227, 0.9318, 0.6821,\n",
       "                       0.6507, 0.9013, 0.6245, 0.9208, 0.7912, 0.9976, 0.8506, 0.7407, 0.8103,\n",
       "                       0.8845, 0.4730, 0.8374, 1.2930, 0.8961, 1.0245, 0.9797, 0.9377, 0.9742,\n",
       "                       0.9053, 0.5350, 0.6898, 0.5492, 0.7598, 0.9274, 1.0732, 0.7828, 0.5287,\n",
       "                       0.8170, 0.7225, 0.8816, 0.7687, 0.6431, 0.7539, 0.7734, 0.9216, 1.0695,\n",
       "                       0.8103, 0.8648, 0.8490, 0.8530, 1.0982, 0.9931, 0.6203, 0.8130, 0.9219,\n",
       "                       0.9974, 0.9689, 0.9517, 0.6333, 0.8164, 0.8492, 0.9988, 0.9801, 0.7276,\n",
       "                       0.8840, 0.9065, 0.8740, 0.7834, 0.6957, 0.8483, 0.9621, 0.6285, 0.6334,\n",
       "                       0.7365, 0.9725, 1.0484, 0.7807, 1.0283, 0.8633, 0.9291, 0.8866, 0.7892,\n",
       "                       0.6511, 0.6662])),\n",
       "              ('regressor.ligand_model.gt_block.1.bn_edge_e.bias',\n",
       "               tensor([-0.7609, -0.5736, -0.5139, -0.9271, -0.5541, -0.7215, -0.7507, -0.8175,\n",
       "                       -0.4163, -0.6329, -0.7586, -0.7917, -0.7835, -0.5306, -0.6939, -0.8921,\n",
       "                       -0.8159, -0.7485, -0.6611, -0.6800, -0.6892, -0.6298, -0.9035, -0.6814,\n",
       "                       -0.7571, -0.9210, -0.7064, -0.8945, -0.8485, -0.6431, -0.9786, -0.9217,\n",
       "                       -0.7842, -0.5930, -0.5373, -0.7890, -0.7899, -0.6270, -0.7053, -0.7495,\n",
       "                       -0.7996, -0.5522, -0.6483, -0.9600, -0.5743, -0.7845, -0.7125, -0.7094,\n",
       "                       -0.9338, -0.7111, -0.7272, -0.6621, -0.8629, -0.4926, -0.4202, -0.6990,\n",
       "                       -0.6350, -0.6918, -0.6505, -0.7049, -0.5493, -0.5597, -0.7348, -0.6266,\n",
       "                       -0.5769, -0.8787, -0.8873, -0.7908, -0.7155, -0.8004, -0.5782, -0.7153,\n",
       "                       -0.7135, -0.6922, -0.4177, -0.7161, -0.7681, -0.8496, -0.9037, -0.6091,\n",
       "                       -0.6076, -0.6561, -0.5456, -0.6315, -0.6109, -0.5989, -0.5264, -0.6407,\n",
       "                       -0.6961, -0.9079, -0.6958, -0.5413, -0.7750, -0.4305, -0.9085, -0.8372,\n",
       "                       -0.5585, -0.6599, -0.7321, -0.7072, -0.9112, -0.6116, -0.7296, -0.6471,\n",
       "                       -0.7421, -0.6406, -0.7407, -0.7672, -0.6018, -0.6670, -0.5185, -0.5752,\n",
       "                       -0.4940, -0.6085, -0.6607, -0.8257, -0.5212, -0.6279, -0.8191, -0.8415,\n",
       "                       -0.6012, -0.8467, -0.5957, -0.6723, -0.7251, -0.8401, -0.3649, -0.4682])),\n",
       "              ('regressor.ligand_model.gt_block.1.bn_edge_e.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "              ('regressor.ligand_model.gt_block.1.bn_edge_e.running_var',\n",
       "               tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1.])),\n",
       "              ('regressor.ligand_model.gt_block.1.bn_edge_e.num_batches_tracked',\n",
       "               tensor(0)),\n",
       "              ('regressor.ligand_model.gt_block.2.A.weight',\n",
       "               tensor([[-0.3150, -0.0960,  0.1289,  ..., -0.1363, -0.1441,  0.0194],\n",
       "                       [ 0.0851, -0.2898,  0.2270,  ...,  0.0508, -0.1503,  0.1940],\n",
       "                       [ 0.0048, -0.3009, -0.3947,  ...,  0.0403, -0.2490, -0.2189],\n",
       "                       ...,\n",
       "                       [ 0.0113,  0.1902, -0.0722,  ..., -0.5942, -0.2512, -0.2689],\n",
       "                       [-0.1740,  0.0847, -0.1585,  ..., -0.1100, -0.6718,  0.0309],\n",
       "                       [-0.2051,  0.1904,  0.0594,  ...,  0.0433,  0.0276, -0.6603]])),\n",
       "              ('regressor.ligand_model.gt_block.2.A.bias',\n",
       "               tensor([-0.0232,  0.0205, -0.0514,  0.0614, -0.0679, -0.0090, -0.0301,  0.0410,\n",
       "                       -0.0847,  0.0001,  0.0760,  0.0354, -0.0392, -0.0185,  0.0127,  0.0118,\n",
       "                       -0.0768, -0.0402,  0.0390,  0.0590, -0.0669,  0.0087, -0.0770,  0.0064,\n",
       "                       -0.0668,  0.0016,  0.0138, -0.0329,  0.0186, -0.0669,  0.0558,  0.0542,\n",
       "                       -0.0391,  0.0360, -0.0489, -0.0100,  0.0146, -0.0425, -0.0843,  0.0141,\n",
       "                        0.0117,  0.0309,  0.0869, -0.0722, -0.0310,  0.0853,  0.0196, -0.0409,\n",
       "                       -0.0671, -0.0472,  0.0161,  0.0666,  0.0290, -0.0781, -0.0477,  0.0051,\n",
       "                       -0.0309,  0.0130, -0.0539, -0.0021, -0.0097, -0.0702, -0.0591, -0.0492,\n",
       "                        0.0593, -0.0642, -0.0144, -0.0518, -0.0235,  0.0433,  0.0179, -0.0577,\n",
       "                       -0.0764,  0.0377,  0.0662,  0.0314, -0.0333,  0.0586,  0.0738,  0.0535,\n",
       "                       -0.0640,  0.0285, -0.0486, -0.0396, -0.0251,  0.0694, -0.0160,  0.0337,\n",
       "                        0.0502, -0.0469, -0.0494,  0.0482,  0.0854, -0.0140, -0.0461, -0.0520,\n",
       "                       -0.0318, -0.0218,  0.0038,  0.0427, -0.0850,  0.0415, -0.0234, -0.0505,\n",
       "                        0.0113, -0.0126, -0.0840,  0.0378,  0.0223, -0.0220,  0.0322, -0.0146,\n",
       "                       -0.0242,  0.0599,  0.0439, -0.0514, -0.0508, -0.0768, -0.0034,  0.0864,\n",
       "                        0.0718,  0.0527,  0.0309,  0.0773,  0.0709,  0.0432,  0.0709, -0.0385])),\n",
       "              ('regressor.ligand_model.gt_block.2.B.weight',\n",
       "               tensor([[-0.0558, -0.3276,  0.0503,  ...,  0.2368,  0.3455,  0.0239],\n",
       "                       [-0.2522,  0.0968,  0.0705,  ..., -0.0088,  0.0811, -0.1481],\n",
       "                       [-0.0447, -0.0951,  0.0108,  ...,  0.0712,  0.0681,  0.0596],\n",
       "                       ...,\n",
       "                       [-0.2630, -0.3903,  0.1054,  ...,  0.1884,  0.1797,  0.1162],\n",
       "                       [ 0.2998, -0.0038, -0.3525,  ..., -0.0365,  0.1862, -0.0021],\n",
       "                       [-0.0657, -0.2927,  0.0205,  ..., -0.0966,  0.2152,  0.0403]])),\n",
       "              ('regressor.ligand_model.gt_block.2.B.bias',\n",
       "               tensor([-1.4591e-01,  2.1478e-02, -5.8349e-01, -5.3403e-02, -1.6673e-01,\n",
       "                        1.4331e+00,  2.1984e-01, -9.4866e-02,  3.6587e-01, -8.0541e-01,\n",
       "                        2.5995e-01, -6.4900e-01,  6.3346e-02,  1.5800e-01,  4.4315e-01,\n",
       "                        5.9615e-01, -7.7332e-01,  1.9239e-01, -8.8922e-01,  2.8003e-01,\n",
       "                        7.2273e-01,  1.8387e-03, -1.7953e-01,  4.1738e-02,  7.8315e-01,\n",
       "                       -6.3411e-01, -1.2470e-01,  4.6432e-01, -2.9226e-02,  1.0024e-01,\n",
       "                        1.1037e-02,  1.3117e-03, -1.3713e-01, -7.5467e-02, -3.2701e-01,\n",
       "                        1.9829e-01,  2.6585e-01, -8.6543e-01, -2.4817e-01,  3.9822e-01,\n",
       "                        9.8826e-01, -3.9543e-01,  1.3085e+00,  2.5341e-01,  1.9058e-01,\n",
       "                       -8.4782e-01,  2.6660e-01,  1.0332e+00,  4.9108e-01, -1.5401e-01,\n",
       "                       -1.3370e-01,  6.2907e-02, -3.8702e-01,  5.6903e-01,  4.2240e-01,\n",
       "                        8.3561e-01,  1.1986e-01, -2.2422e-01,  6.0402e-02, -2.0995e-01,\n",
       "                        8.4885e-01,  7.6609e-01, -4.1617e-01,  8.5876e-01,  4.6363e-01,\n",
       "                        6.1600e-01, -3.5134e-02,  8.5779e-01,  1.8699e-01, -1.2599e-01,\n",
       "                       -3.8610e-01,  1.7167e-01,  1.8138e-01, -6.0383e-01,  1.6588e+00,\n",
       "                        8.0600e-01,  1.1645e+00,  6.1303e-02,  8.6401e-01, -3.0539e-01,\n",
       "                        7.4226e-01, -1.0072e-01,  9.7410e-01,  5.2393e-02,  1.7269e-01,\n",
       "                       -8.3740e-02,  1.1873e-01, -1.4603e-01,  1.7225e+00, -4.9034e-01,\n",
       "                       -2.3804e-02, -3.0794e-01, -6.5709e-02,  4.5463e-01,  1.6626e-01,\n",
       "                        5.0642e-01, -1.1010e+00,  2.4693e-01,  7.8893e-01,  3.5035e-02,\n",
       "                       -3.7393e-01, -1.0690e+00,  1.1814e-01, -9.9336e-01, -1.8049e-01,\n",
       "                        6.5296e-01, -4.8535e-01, -9.0177e-01, -1.2511e+00, -1.6703e-01,\n",
       "                        7.3122e-01, -1.5908e-02,  4.6296e-01,  1.4710e-01,  6.5109e-02,\n",
       "                        1.1967e-01,  1.4716e-01, -1.5587e-01, -6.9190e-01,  2.6556e-01,\n",
       "                        1.6135e-01, -3.0222e-01, -3.8913e-01, -2.9446e-01,  1.2571e-02,\n",
       "                       -4.7088e-01,  1.2322e-01,  1.9303e-01])),\n",
       "              ('regressor.ligand_model.gt_block.2.C.weight',\n",
       "               tensor([[-0.0760, -0.3421, -0.1475,  ...,  0.0109, -0.0857,  0.0644],\n",
       "                       [ 0.1780, -0.4433, -0.2826,  ..., -0.2760, -0.3155, -0.0884],\n",
       "                       [ 0.1618,  0.0025,  0.1906,  ..., -0.0177,  0.3405, -0.1267],\n",
       "                       ...,\n",
       "                       [-0.1792, -0.2013, -0.1995,  ..., -0.3236,  0.4266, -0.3952],\n",
       "                       [-0.1382,  0.0853,  0.0070,  ..., -0.1265, -0.3950, -0.0813],\n",
       "                       [-0.2509,  0.1053,  0.1108,  ..., -0.0091, -0.0741, -0.0795]])),\n",
       "              ('regressor.ligand_model.gt_block.2.C.bias',\n",
       "               tensor([-3.9354e-01,  1.4589e-02, -1.3012e-01, -4.5862e-01, -1.4915e-01,\n",
       "                       -5.6618e-03, -1.6354e-01, -6.6855e-01, -5.8754e-01, -3.0946e-01,\n",
       "                       -1.5275e-01, -8.2252e-02,  1.9452e-01,  3.4875e-01, -1.4105e-01,\n",
       "                        1.2413e-01, -7.4860e-02, -1.3538e-01, -5.1456e-02, -3.3186e-01,\n",
       "                       -2.3679e-03,  1.4867e-02,  3.1639e-02, -1.5029e-01, -2.9248e-01,\n",
       "                        1.6633e-01, -1.8770e-01, -5.9898e-01, -5.1126e-01, -3.6580e-01,\n",
       "                        3.1284e-04, -3.5695e-01, -6.5642e-02, -4.1957e-01, -3.5740e-01,\n",
       "                       -4.3188e-01, -2.5482e-01,  3.4282e-02, -7.6118e-01, -1.0426e-01,\n",
       "                        1.0608e-02, -3.1106e-01, -9.7559e-02, -6.0510e-01, -2.0298e-01,\n",
       "                       -1.8723e-01, -3.6749e-02, -5.2121e-01, -3.9987e-01, -8.5067e-02,\n",
       "                       -4.4968e-01, -2.0079e-01, -1.6154e-01, -2.5053e-01, -4.4109e-01,\n",
       "                       -1.7581e-01, -3.6080e-01, -2.3536e-01, -6.6097e-02, -3.4409e-01,\n",
       "                       -5.9675e-01, -1.7106e-02, -1.3045e-01, -3.0864e-02, -2.9774e-01,\n",
       "                        4.4192e-03, -4.3393e-01, -1.1324e-01,  8.9596e-02,  1.6521e-01,\n",
       "                        1.8357e-01, -1.9879e-01,  1.6895e-02, -3.1600e-01, -4.8980e-02,\n",
       "                       -7.4942e-01, -1.0151e-01, -6.6444e-01, -1.0186e-02, -2.3458e-01,\n",
       "                       -9.7336e-02, -3.7950e-01,  4.6815e-02, -2.8214e-01, -3.9588e-01,\n",
       "                       -3.8922e-01, -2.2257e-01, -4.8683e-01,  4.3862e-02, -1.9228e-01,\n",
       "                       -1.1591e-01, -6.1091e-01, -1.8595e-01, -2.7515e-01, -5.0105e-01,\n",
       "                       -2.0953e-01,  7.5104e-02, -1.3398e-01,  7.6284e-02, -1.2820e-01,\n",
       "                       -6.1131e-01, -2.5058e-01, -7.8344e-01,  1.5877e-02, -4.0369e-02,\n",
       "                       -5.7688e-02,  5.3942e-02,  2.4266e-02, -1.8540e-03, -5.5028e-01,\n",
       "                       -1.6040e-01, -5.5144e-01,  1.7311e-01, -1.9455e-01, -4.2132e-01,\n",
       "                       -4.5751e-01, -4.9616e-01,  6.4109e-04, -1.4363e-01, -4.1702e-01,\n",
       "                       -4.8554e-02, -1.2627e-01, -1.1349e-01, -1.7303e-01, -5.1908e-01,\n",
       "                        1.5465e-01, -3.3354e-01, -1.5622e-01])),\n",
       "              ('regressor.ligand_model.gt_block.2.D.weight',\n",
       "               tensor([[-0.0622,  0.0466, -0.1718,  ...,  0.2696,  0.0539,  0.5290],\n",
       "                       [-0.1382,  0.1556, -0.0821,  ...,  0.2355,  0.2796,  0.2159],\n",
       "                       [ 0.2281,  0.2269,  0.0367,  ..., -0.2034, -0.1308, -0.1317],\n",
       "                       ...,\n",
       "                       [-0.3170, -0.0878, -0.0282,  ...,  0.3891, -0.2859, -0.2408],\n",
       "                       [-0.4180, -0.0875, -0.3216,  ...,  0.0603, -0.0103,  0.4672],\n",
       "                       [ 0.0326,  0.0621,  0.4169,  ...,  0.0217,  0.0536, -0.2915]])),\n",
       "              ('regressor.ligand_model.gt_block.2.D.bias',\n",
       "               tensor([-0.3486, -0.0132, -0.0892, -0.5155, -0.1419,  0.0652, -0.1235, -0.7339,\n",
       "                       -0.5900, -0.3904, -0.0163, -0.0517,  0.1328,  0.2029, -0.1345,  0.0131,\n",
       "                       -0.0431, -0.0298, -0.1226, -0.4344,  0.0257, -0.0618,  0.0270, -0.1461,\n",
       "                       -0.2012,  0.1084, -0.1576, -0.7293, -0.3535, -0.4509, -0.1163, -0.4176,\n",
       "                       -0.0159, -0.5156, -0.2959, -0.3706, -0.2561, -0.0205, -0.7918,  0.0174,\n",
       "                        0.0280, -0.2949, -0.0679, -0.5209, -0.1451, -0.3117,  0.0746, -0.6180,\n",
       "                       -0.4345, -0.1810, -0.5546, -0.2068, -0.0089, -0.2283, -0.3487, -0.1204,\n",
       "                       -0.3328, -0.1148, -0.2237, -0.3806, -0.6085, -0.0600, -0.2030, -0.0867,\n",
       "                       -0.4244, -0.0060, -0.3919, -0.1463,  0.0673,  0.0219,  0.1673, -0.2749,\n",
       "                       -0.0998, -0.3434,  0.0568, -0.8278, -0.0395, -0.6376,  0.0101, -0.1292,\n",
       "                       -0.1991, -0.2639,  0.0856, -0.3614, -0.3641, -0.3161, -0.2775, -0.5309,\n",
       "                       -0.0197, -0.1687, -0.0762, -0.6640, -0.2136, -0.3296, -0.6716, -0.2531,\n",
       "                        0.1254, -0.1531,  0.1071, -0.1728, -0.6038, -0.2871, -0.7798,  0.0856,\n",
       "                        0.0940,  0.0902, -0.0242,  0.0272, -0.0227, -0.6157, -0.1229, -0.5376,\n",
       "                        0.1184, -0.1203, -0.4976, -0.4870, -0.5030, -0.1175, -0.1751, -0.3051,\n",
       "                       -0.0268, -0.2086, -0.2546, -0.1758, -0.5578,  0.1126, -0.4809, -0.2478])),\n",
       "              ('regressor.ligand_model.gt_block.2.E.weight',\n",
       "               tensor([[-0.2648, -0.3165, -0.3209,  ..., -0.3917, -0.3156,  0.1756],\n",
       "                       [-0.1348, -0.1023,  0.0802,  ...,  0.0776, -0.2518, -0.1602],\n",
       "                       [ 0.0499, -0.0141, -0.1743,  ...,  0.2629, -0.0930,  0.1774],\n",
       "                       ...,\n",
       "                       [ 0.0520, -0.0732,  0.0651,  ...,  0.1342, -0.0114, -0.1313],\n",
       "                       [ 0.0225,  0.2986, -0.2244,  ...,  0.1233, -0.1770,  0.0812],\n",
       "                       [-0.0469,  0.2279, -0.2223,  ..., -0.1941, -0.0151, -0.0309]])),\n",
       "              ('regressor.ligand_model.gt_block.2.E.bias',\n",
       "               tensor([-0.3274,  0.0054, -0.2408, -0.3614, -0.0751,  0.0078, -0.0776, -0.8020,\n",
       "                       -0.5896, -0.2706, -0.0497, -0.0513,  0.0759,  0.3526, -0.1837,  0.0576,\n",
       "                       -0.1008, -0.0677, -0.2249, -0.3599,  0.0384,  0.0265,  0.0381, -0.2471,\n",
       "                       -0.3467,  0.1631, -0.2405, -0.6399, -0.4501, -0.4463, -0.1420, -0.3450,\n",
       "                       -0.0118, -0.3878, -0.3397, -0.3664, -0.2384,  0.0180, -0.7110, -0.0110,\n",
       "                       -0.0477, -0.2124, -0.0707, -0.5606, -0.1240, -0.2054, -0.0160, -0.5912,\n",
       "                       -0.4549, -0.1043, -0.4657, -0.2833, -0.1549, -0.1728, -0.4190, -0.1062,\n",
       "                       -0.3149, -0.1491, -0.1475, -0.4110, -0.5376,  0.0143, -0.2011, -0.0233,\n",
       "                       -0.3974,  0.0441, -0.3477, -0.0652,  0.1479,  0.0439,  0.1696, -0.2319,\n",
       "                       -0.0158, -0.3200, -0.0767, -0.7980, -0.0360, -0.6513,  0.0897, -0.1508,\n",
       "                       -0.2043, -0.2108,  0.0107, -0.2617, -0.4081, -0.3560, -0.3010, -0.5375,\n",
       "                       -0.0183, -0.1741, -0.0298, -0.6049, -0.1184, -0.2618, -0.6737, -0.2277,\n",
       "                        0.1900, -0.1925,  0.0729, -0.1355, -0.6985, -0.3266, -0.7747,  0.1559,\n",
       "                        0.1125,  0.0730, -0.1014, -0.1002,  0.0066, -0.5381, -0.1792, -0.6018,\n",
       "                        0.0686, -0.1441, -0.4561, -0.3658, -0.4970, -0.0519, -0.2698, -0.3720,\n",
       "                       -0.0026, -0.2356, -0.1624, -0.1090, -0.4990,  0.0692, -0.3901, -0.2860])),\n",
       "              ('regressor.ligand_model.gt_block.2.bn_node_x.weight',\n",
       "               tensor([0.7666, 0.8238, 0.9852, 0.7502, 0.7191, 1.0132, 0.8505, 0.8155, 0.8877,\n",
       "                       0.9352, 0.8410, 0.9292, 0.8317, 0.9420, 0.9942, 0.8804, 0.7598, 0.7979,\n",
       "                       1.0417, 0.8658, 0.8298, 0.8318, 1.0982, 0.9777, 0.9491, 0.8015, 0.7114,\n",
       "                       0.9496, 0.8395, 0.7182, 0.8070, 0.7874, 0.7521, 0.6738, 0.8582, 0.8374,\n",
       "                       0.8464, 0.9480, 0.9547, 0.8035, 0.8066, 0.9162, 0.9593, 0.9041, 0.8720,\n",
       "                       0.7915, 0.9506, 0.8664, 0.6083, 0.7968, 0.7573, 0.9954, 0.8350, 0.7753,\n",
       "                       0.8457, 0.7748, 0.7835, 0.8137, 0.8881, 0.9470, 0.8442, 0.8000, 0.7998,\n",
       "                       1.0168, 0.9349, 0.7869, 0.7815, 0.8697, 0.7649, 0.8173, 0.8227, 0.8320,\n",
       "                       0.8899, 0.8706, 0.7913, 0.8225, 0.7259, 0.7964, 1.0221, 0.8933, 0.7647,\n",
       "                       0.7826, 0.8054, 0.8897, 0.7977, 0.8225, 0.8741, 1.0262, 0.9435, 0.7829,\n",
       "                       0.7987, 0.7804, 0.9790, 0.9754, 0.9074, 0.9680, 1.0076, 1.1024, 0.9441,\n",
       "                       0.7413, 0.8768, 0.9409, 0.6136, 0.8969, 0.9948, 0.9054, 0.9035, 0.7973,\n",
       "                       0.7439, 0.9711, 0.7932, 0.7881, 0.8210, 0.9679, 0.8374, 0.8902, 0.8641,\n",
       "                       0.8611, 0.7937, 0.8832, 0.7461, 0.9471, 0.9017, 1.0170, 1.0083, 0.9640,\n",
       "                       1.0123, 0.8701])),\n",
       "              ('regressor.ligand_model.gt_block.2.bn_node_x.bias',\n",
       "               tensor([-0.6689, -0.6111, -0.6793, -0.5394, -0.5140, -0.8022, -0.5707, -0.6437,\n",
       "                       -0.7067, -0.3756, -0.5595, -0.9547, -0.5315, -0.6937, -0.6787, -0.6612,\n",
       "                       -0.5533, -0.7325, -0.7640, -0.7267, -0.6188, -0.8528, -1.0104, -0.9575,\n",
       "                       -0.9566, -0.7373, -0.5194, -0.7203, -0.6122, -0.5041, -0.6830, -0.3989,\n",
       "                       -0.5284, -0.6485, -0.6461, -0.5918, -0.5834, -0.5065, -0.6071, -0.7716,\n",
       "                       -0.5545, -0.5966, -0.6725, -0.6362, -0.5714, -0.6274, -0.6270, -0.7062,\n",
       "                       -0.5256, -0.5676, -0.6146, -0.7752, -0.8103, -0.7748, -0.8085, -0.5859,\n",
       "                       -0.6893, -0.5944, -0.7564, -0.6370, -0.4624, -0.7326, -0.5290, -0.7902,\n",
       "                       -0.7858, -0.6348, -0.3972, -0.6998, -0.6534, -0.6024, -0.8546, -0.7978,\n",
       "                       -0.6531, -0.7241, -0.5519, -0.4742, -0.5198, -0.6609, -0.7712, -0.7800,\n",
       "                       -0.5017, -0.6751, -0.4298, -0.5914, -0.6300, -0.6308, -0.6098, -1.1070,\n",
       "                       -0.6591, -0.5757, -0.6394, -0.8175, -0.6451, -0.5271, -0.4891, -0.5664,\n",
       "                       -0.7217, -0.7155, -0.6647, -0.7054, -0.5705, -0.5634, -0.5765, -0.6374,\n",
       "                       -0.5183, -0.8222, -0.8050, -0.5949, -0.7062, -0.8024, -0.6039, -0.5481,\n",
       "                       -0.9459, -0.7095, -0.5678, -0.4689, -0.6322, -0.9610, -0.7896, -0.5778,\n",
       "                       -0.5774, -0.7154, -0.9025, -0.6921, -0.7793, -0.6171, -0.7767, -0.7283])),\n",
       "              ('regressor.ligand_model.gt_block.2.bn_node_x.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "              ('regressor.ligand_model.gt_block.2.bn_node_x.running_var',\n",
       "               tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1.])),\n",
       "              ('regressor.ligand_model.gt_block.2.bn_node_x.num_batches_tracked',\n",
       "               tensor(0)),\n",
       "              ('regressor.ligand_model.gt_block.2.bn_edge_e.weight',\n",
       "               tensor([0.8411, 0.8589, 0.4732, 0.8132, 1.0452, 0.6694, 0.8300, 0.8646, 0.9738,\n",
       "                       0.8551, 0.8236, 0.9987, 0.8880, 1.1247, 0.6591, 0.7033, 1.1018, 0.5716,\n",
       "                       0.8851, 0.5793, 0.8921, 0.9872, 0.6112, 1.1797, 0.9685, 0.9495, 0.6978,\n",
       "                       0.9135, 0.9544, 0.6238, 1.0150, 1.0359, 0.6756, 0.8645, 0.9663, 0.7622,\n",
       "                       0.9174, 0.8413, 0.7116, 1.0109, 1.0299, 0.9534, 0.5775, 0.7861, 0.8980,\n",
       "                       0.8655, 0.7231, 0.8107, 0.8229, 0.7184, 0.9481, 1.0396, 0.9124, 0.8194,\n",
       "                       0.8952, 0.6021, 0.7900, 0.8703, 0.7964, 0.9425, 1.1808, 0.5282, 0.6627,\n",
       "                       0.6825, 0.9239, 0.5939, 0.9020, 0.8091, 0.9984, 0.8009, 0.7152, 0.8428,\n",
       "                       0.7809, 0.6965, 1.2160, 0.9156, 0.7797, 1.0630, 0.4809, 0.9439, 0.7837,\n",
       "                       1.0401, 0.9260, 1.1116, 0.7891, 0.7612, 0.9156, 0.9647, 0.5972, 0.4846,\n",
       "                       1.0170, 1.1339, 0.6077, 0.6235, 1.0329, 0.7964, 0.6634, 0.4271, 0.9316,\n",
       "                       0.7385, 1.0313, 0.7234, 0.9541, 1.2349, 0.5466, 0.9202, 1.0661, 0.5933,\n",
       "                       0.8948, 0.9156, 0.8699, 1.0096, 0.8554, 0.7121, 0.9231, 0.7378, 0.7899,\n",
       "                       0.8365, 0.9591, 0.7834, 0.8935, 0.8558, 0.9703, 0.8840, 0.7395, 0.9270,\n",
       "                       0.9837, 1.1335])),\n",
       "              ('regressor.ligand_model.gt_block.2.bn_edge_e.bias',\n",
       "               tensor([-0.7759, -0.7564, -0.4979, -0.6042, -0.8048, -0.6422, -0.7433, -0.5478,\n",
       "                       -0.8084, -0.5966, -0.6850, -0.6157, -0.6463, -0.8749, -0.5278, -0.4810,\n",
       "                       -0.7096, -0.5222, -0.7471, -0.5075, -0.9048, -0.7882, -0.4861, -0.6408,\n",
       "                       -0.7568, -0.6178, -0.6799, -0.7097, -0.7221, -0.5265, -0.6277, -0.9886,\n",
       "                       -0.5519, -0.7172, -0.7824, -0.8331, -0.7146, -0.5766, -0.7588, -0.8212,\n",
       "                       -0.7240, -0.7539, -0.5500, -0.6652, -0.8900, -0.7774, -0.5099, -0.6378,\n",
       "                       -0.6253, -0.7016, -0.7831, -0.9087, -0.7051, -0.6344, -0.5890, -0.6332,\n",
       "                       -0.5699, -0.7909, -0.7312, -0.7005, -0.7567, -0.5763, -0.7048, -0.7133,\n",
       "                       -0.8492, -0.4203, -0.6496, -0.8335, -0.7121, -0.6168, -0.5297, -0.5609,\n",
       "                       -0.7239, -0.7068, -0.9116, -0.7666, -0.8178, -0.8544, -0.4691, -0.6500,\n",
       "                       -0.5562, -0.6784, -0.7547, -0.8335, -0.6976, -0.6636, -0.6821, -0.7612,\n",
       "                       -0.5769, -0.5309, -0.8010, -0.7570, -0.6102, -0.6752, -0.8201, -0.6338,\n",
       "                       -0.6952, -0.5475, -0.7312, -0.5892, -0.9050, -0.5117, -0.8353, -0.8319,\n",
       "                       -0.6355, -0.5349, -0.7360, -0.7485, -0.6456, -0.5568, -0.4292, -0.8309,\n",
       "                       -0.5962, -0.6335, -0.6077, -0.5540, -0.4523, -0.7530, -0.7058, -0.7998,\n",
       "                       -0.6502, -0.8703, -0.6039, -0.6519, -0.6232, -0.6731, -0.5494, -0.8745])),\n",
       "              ('regressor.ligand_model.gt_block.2.bn_edge_e.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "              ('regressor.ligand_model.gt_block.2.bn_edge_e.running_var',\n",
       "               tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1.])),\n",
       "              ('regressor.ligand_model.gt_block.2.bn_edge_e.num_batches_tracked',\n",
       "               tensor(0)),\n",
       "              ('regressor.ligand_model.gt_block.3.A.weight',\n",
       "               tensor([[-0.4422, -0.1459, -0.1293,  ..., -0.0557, -0.0095, -0.4290],\n",
       "                       [-0.1007, -0.1892,  0.1422,  ...,  0.1658,  0.0544, -0.2003],\n",
       "                       [-0.1183,  0.0899, -0.3791,  ..., -0.1125, -0.0932, -0.1020],\n",
       "                       ...,\n",
       "                       [-0.2195,  0.0946,  0.0752,  ..., -0.6299,  0.1330, -0.1349],\n",
       "                       [ 0.0257,  0.1478, -0.2708,  ..., -0.1094, -0.4698,  0.0438],\n",
       "                       [-0.0320,  0.0595,  0.0655,  ..., -0.0025,  0.1004, -0.3821]])),\n",
       "              ('regressor.ligand_model.gt_block.3.A.bias',\n",
       "               tensor([ 0.0710, -0.0063,  0.0491, -0.0002,  0.0411, -0.0247,  0.0211,  0.0445,\n",
       "                        0.0360, -0.0526,  0.0722,  0.0487,  0.0158, -0.0811, -0.0767, -0.0083,\n",
       "                        0.0224,  0.0437, -0.0062, -0.0401, -0.0836, -0.0364, -0.0652, -0.0740,\n",
       "                        0.0298,  0.0131, -0.0298, -0.0257, -0.0233,  0.0399,  0.0678,  0.0091,\n",
       "                       -0.0368, -0.0016, -0.0359, -0.0456, -0.0279, -0.0543, -0.0073,  0.0529,\n",
       "                        0.0895,  0.0493, -0.0073,  0.0402,  0.0489, -0.0392, -0.0575,  0.0487,\n",
       "                       -0.0598,  0.0279, -0.0534, -0.0833,  0.0011,  0.0366,  0.0586,  0.0221,\n",
       "                        0.0798, -0.0710, -0.0494, -0.0145, -0.0405, -0.0379, -0.0691, -0.0660,\n",
       "                        0.0269,  0.0511,  0.0860,  0.0650, -0.0651, -0.0514, -0.0566,  0.0076,\n",
       "                        0.0147,  0.0511, -0.0294,  0.0658, -0.0857, -0.0558, -0.0635, -0.0577,\n",
       "                       -0.0433,  0.0721, -0.0662, -0.0147, -0.0085,  0.0632,  0.0768, -0.0206,\n",
       "                       -0.0159, -0.0580, -0.0730,  0.0102, -0.0008, -0.0865, -0.0334,  0.0416,\n",
       "                       -0.0072, -0.0353,  0.0235, -0.0516, -0.0872, -0.0373, -0.0585, -0.0487,\n",
       "                       -0.0493, -0.0171,  0.0669,  0.0852, -0.0185, -0.0859,  0.0675,  0.0097,\n",
       "                        0.0764,  0.0714, -0.0826, -0.0218,  0.0868, -0.0613, -0.0512,  0.0310,\n",
       "                       -0.0150,  0.0371, -0.0448,  0.0322, -0.0494, -0.0628,  0.0413,  0.0269])),\n",
       "              ('regressor.ligand_model.gt_block.3.B.weight',\n",
       "               tensor([[-0.0814,  0.1226,  0.1140,  ..., -0.0732,  0.1068, -0.2890],\n",
       "                       [ 0.3016, -0.0371, -0.2228,  ..., -0.0613, -0.1065,  0.0357],\n",
       "                       [ 0.1674, -0.2377,  0.0899,  ...,  0.2276,  0.1270,  0.0223],\n",
       "                       ...,\n",
       "                       [-0.1723, -0.2690,  0.2047,  ..., -0.0635,  0.1840,  0.1621],\n",
       "                       [ 0.1447,  0.0174,  0.1581,  ...,  0.1495,  0.2316, -0.0096],\n",
       "                       [ 0.1074, -0.4583, -0.1848,  ..., -0.1875,  0.1311,  0.2886]])),\n",
       "              ('regressor.ligand_model.gt_block.3.B.bias',\n",
       "               tensor([-0.0671,  0.5846, -0.3225,  0.4542, -0.1098,  0.2146,  0.5159, -0.1327,\n",
       "                       -0.6254,  0.1587,  0.5310,  0.7251,  0.5818,  0.6838,  0.1436, -0.3652,\n",
       "                       -1.3130, -0.1968,  0.2587,  0.7071,  0.6725, -0.8925,  0.5709,  0.0312,\n",
       "                        0.4604, -0.5774,  0.6967,  0.4452, -0.3586,  0.1789,  0.2582, -0.7099,\n",
       "                        0.1612,  0.6597, -0.5284, -0.2356,  0.1318, -0.2238, -0.2244, -0.1773,\n",
       "                        1.4162, -0.9831,  0.4223, -0.1441,  0.6000,  0.6094, -2.4703,  0.7312,\n",
       "                        0.8453, -0.1269, -0.3206,  0.1601, -0.0254,  0.0367, -1.3578,  0.4076,\n",
       "                       -0.3420,  0.4369, -0.3059, -0.1392,  0.2902, -0.3823,  0.2724,  0.4112,\n",
       "                       -0.0607, -0.1958, -0.2931, -0.2694,  0.0692,  0.1699,  0.1169, -0.2123,\n",
       "                       -0.0663,  0.1445,  0.1637,  0.2671, -0.2268,  0.1512,  0.5075, -0.6901,\n",
       "                        0.6837, -0.3055, -0.4155,  0.5403,  0.0495, -0.7139,  0.3595,  0.1265,\n",
       "                       -0.1432,  0.0247,  0.8156,  0.1369, -0.5499,  0.1743, -0.1144,  0.8015,\n",
       "                        0.2066, -0.3823, -0.0865,  0.6106,  1.2789,  0.4407, -0.1827, -0.2015,\n",
       "                       -0.0557, -0.2068,  0.2390, -0.8890,  0.2929, -0.2775,  0.5915,  1.0802,\n",
       "                       -0.0519,  0.3239, -0.2675,  0.5413, -0.0214,  0.1097, -0.3358,  0.4040,\n",
       "                       -0.3856,  0.2250,  1.3988,  0.3528,  0.0855,  0.3785,  0.6822, -0.3163])),\n",
       "              ('regressor.ligand_model.gt_block.3.C.weight',\n",
       "               tensor([[-0.1466,  0.1422, -0.0031,  ..., -0.1690, -0.0323,  0.2389],\n",
       "                       [-0.0200, -0.0111, -0.3101,  ..., -0.0727,  0.0373,  0.2905],\n",
       "                       [-0.0165,  0.2395,  0.0175,  ...,  0.0817, -0.0589, -0.1440],\n",
       "                       ...,\n",
       "                       [-0.0595, -0.4610,  0.1435,  ..., -0.2222,  0.2434, -0.1901],\n",
       "                       [ 0.0434,  0.0727, -0.0466,  ..., -0.3433,  0.1131,  0.0327],\n",
       "                       [ 0.0269, -0.1963,  0.0914,  ...,  0.0346, -0.0586,  0.0534]])),\n",
       "              ('regressor.ligand_model.gt_block.3.C.bias',\n",
       "               tensor([-0.0086,  0.1126, -0.1537, -0.2470, -0.1659, -0.1392, -0.2011,  0.0280,\n",
       "                       -0.2239, -0.1043, -0.2019, -0.9094,  0.0585, -0.0873, -0.0295, -0.1389,\n",
       "                       -0.0765, -0.0177, -0.3680, -0.5807, -0.0909, -0.1386, -0.4936,  0.0865,\n",
       "                       -0.1141, -0.1124, -0.0072, -0.3825, -0.0499, -0.1739, -0.0784, -0.2696,\n",
       "                       -0.1778, -0.2016, -0.4169, -0.0673, -0.1182, -0.1242,  0.0181, -0.1032,\n",
       "                       -0.0260, -0.0389, -0.3218,  0.0583, -0.2912, -0.2404,  0.2312, -0.0566,\n",
       "                       -0.0983, -0.0903, -0.6071, -0.2103, -0.1879, -0.1485, -0.0736, -0.4163,\n",
       "                       -0.1492, -0.2423, -0.4425, -0.0955, -0.3814,  0.0095, -0.2287, -0.4493,\n",
       "                       -0.1277, -0.1955,  0.1343, -0.1618, -0.0680, -0.2341, -0.3288, -0.0776,\n",
       "                        0.0869, -0.3300,  0.0498, -0.1363, -0.2089, -0.4761, -0.1319,  0.0133,\n",
       "                        0.2600, -0.0654, -0.3109, -0.0408, -0.3531,  0.0684,  0.0465, -0.2268,\n",
       "                       -0.0542,  0.0627, -0.0585, -0.1078, -0.0728, -0.2178, -0.3730,  0.1190,\n",
       "                        0.0736, -0.2479, -0.0026, -0.1389, -0.0284, -0.1663,  0.4738, -0.0569,\n",
       "                        0.0771, -0.2137,  0.1839, -0.2766, -0.0842,  0.0874, -0.3592, -0.0548,\n",
       "                       -0.0523, -0.2523, -0.5043, -0.3229, -0.1797, -0.3431,  0.0456,  0.0507,\n",
       "                       -0.2736, -0.0561,  0.0288, -0.5656,  0.1373, -0.3865,  0.0168, -0.1874])),\n",
       "              ('regressor.ligand_model.gt_block.3.D.weight',\n",
       "               tensor([[ 0.4419,  0.0962, -0.0239,  ...,  0.5636, -0.1994,  0.4591],\n",
       "                       [-0.0632,  0.8552,  0.1158,  ..., -0.0820, -0.2835, -0.2207],\n",
       "                       [ 0.0567,  0.5175,  0.6268,  ...,  0.2219, -0.0829,  0.0796],\n",
       "                       ...,\n",
       "                       [ 0.1455,  0.0954, -0.1874,  ..., -0.1932,  0.1717, -0.3661],\n",
       "                       [ 0.5180,  0.9062,  0.1988,  ...,  0.1076, -0.0404,  0.0315],\n",
       "                       [ 0.2118,  0.0043, -0.0283,  ...,  0.0115, -0.0010, -0.0720]])),\n",
       "              ('regressor.ligand_model.gt_block.3.D.bias',\n",
       "               tensor([ 0.0042,  0.1159, -0.0765, -0.1272, -0.1739, -0.1194, -0.2676,  0.0710,\n",
       "                       -0.2598, -0.1661, -0.0602, -0.7780,  0.0937, -0.0343, -0.0251, -0.1140,\n",
       "                       -0.0047, -0.0835, -0.4349, -0.5171, -0.0068, -0.2068, -0.4192,  0.0769,\n",
       "                       -0.1533, -0.0176,  0.0103, -0.3491, -0.0460, -0.1205, -0.1861, -0.3815,\n",
       "                       -0.2084, -0.2244, -0.5425, -0.0123, -0.2270, -0.0713,  0.0038, -0.0325,\n",
       "                        0.0844, -0.0196, -0.2627, -0.0465, -0.2758, -0.3191,  0.1848, -0.0998,\n",
       "                       -0.1722, -0.0993, -0.6260, -0.2336, -0.2354, -0.2088, -0.0306, -0.2913,\n",
       "                       -0.1548, -0.1857, -0.5509, -0.0694, -0.3163,  0.0423, -0.3132, -0.4393,\n",
       "                       -0.1999, -0.1549,  0.0396, -0.0930, -0.0678, -0.1075, -0.2892, -0.0017,\n",
       "                        0.1537, -0.4564,  0.0149, -0.2708, -0.1892, -0.4458, -0.1244, -0.1116,\n",
       "                        0.2120, -0.0877, -0.3021,  0.0255, -0.3349,  0.0997, -0.0800, -0.3059,\n",
       "                       -0.0403,  0.0590, -0.0712, -0.1358, -0.0876, -0.0925, -0.2203,  0.1012,\n",
       "                        0.0351, -0.3026,  0.0321, -0.1275,  0.1226, -0.1589,  0.4871, -0.0417,\n",
       "                        0.0238, -0.2788,  0.1585, -0.2661, -0.0940,  0.0535, -0.3542, -0.0849,\n",
       "                       -0.0388, -0.3830, -0.4878, -0.3062, -0.2434, -0.3647, -0.0290,  0.0321,\n",
       "                       -0.3871, -0.0505,  0.1960, -0.5865,  0.0639, -0.4310,  0.0275, -0.1017])),\n",
       "              ('regressor.ligand_model.gt_block.3.E.weight',\n",
       "               tensor([[-0.0036,  0.0445,  0.0144,  ...,  0.1369,  0.4880,  0.0061],\n",
       "                       [-0.3161,  0.2725,  0.3211,  ...,  0.1848,  0.4104, -0.1131],\n",
       "                       [-0.1755, -0.1907, -0.0257,  ..., -0.0635, -0.3045,  0.0495],\n",
       "                       ...,\n",
       "                       [ 0.0877,  0.0663,  0.2448,  ...,  0.1617, -0.0279, -0.2337],\n",
       "                       [ 0.2199,  0.2929, -0.1103,  ..., -0.1119, -0.3009,  0.2563],\n",
       "                       [ 0.0265,  0.0285,  0.0993,  ..., -0.1564,  0.1880, -0.0178]])),\n",
       "              ('regressor.ligand_model.gt_block.3.E.bias',\n",
       "               tensor([-0.0153,  0.0685, -0.1671, -0.2523, -0.3281, -0.1094, -0.1623, -0.0509,\n",
       "                       -0.1947, -0.1744, -0.1290, -0.8524, -0.0049,  0.0057, -0.0910, -0.1979,\n",
       "                        0.0073, -0.0233, -0.4403, -0.5366,  0.0331, -0.1620, -0.4667,  0.1331,\n",
       "                       -0.2391, -0.1545, -0.0277, -0.3380,  0.0224, -0.1868, -0.1878, -0.2423,\n",
       "                       -0.0827, -0.2181, -0.5147,  0.0266, -0.1594, -0.1119, -0.0541, -0.1190,\n",
       "                        0.0036,  0.0322, -0.2951, -0.0316, -0.2730, -0.2625,  0.2686, -0.1686,\n",
       "                       -0.0724, -0.1419, -0.5813, -0.3357, -0.0903, -0.1808, -0.0941, -0.4042,\n",
       "                       -0.0998, -0.2796, -0.4831, -0.0233, -0.4020, -0.0559, -0.2931, -0.4751,\n",
       "                       -0.1224, -0.0764, -0.0185, -0.1948, -0.0797, -0.1165, -0.4119, -0.0342,\n",
       "                        0.1553, -0.4478,  0.1015, -0.2913, -0.1237, -0.4241, -0.0916, -0.1006,\n",
       "                        0.1250, -0.0421, -0.2944,  0.0191, -0.4250, -0.0455, -0.0246, -0.2273,\n",
       "                       -0.0528,  0.0325, -0.0650, -0.1381, -0.0374, -0.0762, -0.2377, -0.0528,\n",
       "                        0.0689, -0.2718,  0.0286, -0.0633, -0.0457, -0.1101,  0.3486, -0.0613,\n",
       "                        0.1571, -0.2037,  0.1017, -0.3318,  0.0228,  0.1039, -0.4584,  0.0052,\n",
       "                       -0.0542, -0.3930, -0.6149, -0.4276, -0.1727, -0.3424,  0.1016, -0.0705,\n",
       "                       -0.3820, -0.0094,  0.1156, -0.5326,  0.0292, -0.5278,  0.0052, -0.1545])),\n",
       "              ('regressor.ligand_model.gt_block.3.bn_node_x.weight',\n",
       "               tensor([0.8368, 0.8272, 1.3458, 0.7740, 0.9198, 1.1028, 0.9317, 1.0063, 1.0832,\n",
       "                       1.2592, 0.9675, 0.9891, 0.7896, 1.0146, 1.0180, 1.0875, 0.8324, 1.1945,\n",
       "                       1.0080, 1.1105, 0.8832, 0.9384, 1.0392, 0.9860, 0.7938, 0.9362, 0.9792,\n",
       "                       0.8946, 1.2176, 0.9644, 0.7953, 1.0075, 1.0150, 0.7547, 0.8812, 0.9224,\n",
       "                       0.9181, 1.1186, 1.0931, 1.1410, 1.1131, 1.0743, 0.9581, 0.8669, 0.9131,\n",
       "                       0.9065, 0.9114, 0.7051, 0.9202, 0.8774, 0.8278, 0.8387, 1.0243, 1.0274,\n",
       "                       0.7490, 0.9969, 0.9719, 1.0980, 0.9586, 0.8852, 1.0704, 1.0138, 0.8379,\n",
       "                       0.9664, 0.9622, 0.8918, 0.9840, 0.9318, 1.0218, 1.0417, 0.9812, 1.0677,\n",
       "                       0.9850, 1.1084, 0.9471, 1.0810, 0.9405, 0.7812, 1.0500, 0.7295, 0.8454,\n",
       "                       0.9045, 0.9772, 1.0516, 1.0067, 1.0155, 0.9519, 1.2468, 1.0093, 0.9962,\n",
       "                       0.9611, 1.0899, 1.0890, 0.9337, 1.0370, 1.2294, 1.1574, 0.9321, 0.9050,\n",
       "                       0.8532, 1.1468, 0.9906, 0.6753, 0.8693, 0.9758, 0.9073, 0.9051, 0.9310,\n",
       "                       0.9503, 0.9671, 0.9399, 1.0936, 0.7983, 0.7987, 1.0262, 1.1341, 0.9670,\n",
       "                       0.9897, 0.8712, 1.0622, 0.9119, 1.0538, 0.9066, 0.8052, 0.9650, 0.9412,\n",
       "                       1.0852, 0.9416])),\n",
       "              ('regressor.ligand_model.gt_block.3.bn_node_x.bias',\n",
       "               tensor([-0.6657, -0.7024, -0.7667, -0.4844, -0.5585, -0.9796, -0.6969, -0.9641,\n",
       "                       -0.8436, -0.6719, -0.6955, -0.9581, -0.6230, -0.8294, -0.5618, -0.6676,\n",
       "                       -0.7452, -1.0509, -0.7215, -0.9617, -0.6490, -0.8406, -0.9636, -0.7952,\n",
       "                       -0.6973, -0.7044, -0.6351, -0.6791, -0.9946, -0.7541, -0.5849, -0.6419,\n",
       "                       -0.9018, -0.6641, -0.7516, -0.6885, -0.6277, -0.5992, -0.8599, -1.0239,\n",
       "                       -0.8022, -0.8030, -0.8005, -0.7827, -0.6478, -0.6455, -0.8031, -0.6242,\n",
       "                       -1.0102, -0.7640, -0.7281, -0.7029, -0.8973, -0.8338, -0.4231, -0.7013,\n",
       "                       -0.9264, -0.8884, -0.7240, -0.6997, -0.7321, -0.8372, -0.6658, -0.6554,\n",
       "                       -0.7215, -0.5950, -0.8338, -0.7639, -0.8260, -0.6350, -0.8637, -0.7153,\n",
       "                       -0.6572, -0.9941, -0.6773, -0.7239, -0.6931, -0.5495, -0.7423, -0.4329,\n",
       "                       -0.5803, -0.7437, -0.7417, -0.8217, -0.7363, -0.7810, -0.6896, -1.0477,\n",
       "                       -0.7082, -0.8796, -0.7925, -1.1449, -0.8047, -0.4772, -0.7983, -0.8302,\n",
       "                       -0.8780, -0.6937, -0.4899, -0.6582, -0.7415, -0.5571, -0.5650, -0.5398,\n",
       "                       -0.6589, -0.8173, -0.7361, -0.7795, -0.7057, -0.6385, -0.6373, -0.7879,\n",
       "                       -0.6654, -0.6930, -0.5173, -0.6777, -0.6495, -0.8899, -0.6221, -0.9089,\n",
       "                       -0.7971, -0.7706, -0.6880, -0.5659, -0.6820, -0.6609, -0.5792, -0.7541])),\n",
       "              ('regressor.ligand_model.gt_block.3.bn_node_x.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "              ('regressor.ligand_model.gt_block.3.bn_node_x.running_var',\n",
       "               tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1.])),\n",
       "              ('regressor.ligand_model.gt_block.3.bn_node_x.num_batches_tracked',\n",
       "               tensor(0)),\n",
       "              ('regressor.ligand_model.gt_block.3.bn_edge_e.weight',\n",
       "               tensor([0.5743, 0.7055, 0.5395, 0.7625, 0.9485, 0.8519, 0.8127, 1.1656, 0.7057,\n",
       "                       1.0044, 0.9221, 0.9734, 0.4536, 0.7508, 0.7516, 0.9118, 0.6460, 0.6974,\n",
       "                       1.1465, 1.0262, 0.5399, 0.4643, 0.5720, 0.9205, 0.8064, 0.7592, 0.5290,\n",
       "                       0.3709, 1.0909, 0.8380, 0.7938, 0.4945, 0.7990, 0.8186, 0.6918, 0.4432,\n",
       "                       0.6018, 0.8235, 0.6804, 0.7496, 0.8943, 0.5876, 0.3857, 0.6475, 0.7902,\n",
       "                       0.8359, 0.5593, 1.3358, 1.0131, 0.6018, 0.9854, 0.7554, 1.0365, 0.7676,\n",
       "                       0.4444, 0.8812, 0.7179, 0.5597, 1.1613, 0.8559, 0.4864, 0.6722, 0.8295,\n",
       "                       0.9830, 0.7175, 0.4605, 0.6110, 0.5484, 0.7437, 0.6839, 0.6901, 0.5899,\n",
       "                       0.5902, 1.0012, 0.6241, 1.0041, 0.8846, 0.7436, 0.6640, 1.2521, 0.9917,\n",
       "                       1.0978, 0.8895, 0.9446, 0.5554, 0.5430, 0.8643, 0.6125, 0.6100, 1.2379,\n",
       "                       0.8359, 1.0113, 0.6649, 0.8037, 0.8667, 0.7548, 0.4044, 0.5214, 1.0897,\n",
       "                       0.7870, 0.3173, 0.9080, 1.1792, 0.8361, 0.9271, 0.9900, 1.1769, 0.9080,\n",
       "                       1.0038, 0.7765, 1.1405, 0.7691, 0.8477, 0.5626, 1.1516, 0.3751, 0.9662,\n",
       "                       0.5395, 0.5731, 0.5635, 1.0160, 0.7372, 0.6923, 0.7428, 0.7151, 1.0244,\n",
       "                       0.5364, 0.9505])),\n",
       "              ('regressor.ligand_model.gt_block.3.bn_edge_e.bias',\n",
       "               tensor([-0.7075, -0.5474, -0.8258, -0.5998, -0.6600, -0.7279, -0.7319, -0.7460,\n",
       "                       -0.6898, -0.6810, -0.6579, -0.7233, -0.4346, -0.7287, -0.6540, -0.7793,\n",
       "                       -0.8727, -0.4899, -0.8881, -0.7666, -0.6435, -0.4741, -0.5011, -0.5027,\n",
       "                       -0.5894, -0.6752, -0.7718, -0.5458, -0.8369, -0.5316, -0.5792, -0.6720,\n",
       "                       -0.5435, -0.5544, -0.6774, -0.4693, -0.7671, -0.9603, -0.5696, -0.4919,\n",
       "                       -0.9439, -0.5666, -0.3732, -0.6580, -0.5353, -0.7257, -0.3270, -1.0474,\n",
       "                       -0.7065, -0.5380, -0.6907, -0.6445, -0.6455, -0.6966, -0.4963, -0.5624,\n",
       "                       -0.7002, -0.5807, -0.7497, -0.5624, -0.4442, -0.4716, -0.7374, -0.7261,\n",
       "                       -0.7455, -0.5546, -0.6961, -0.5464, -0.6687, -0.7050, -0.7439, -0.5047,\n",
       "                       -0.6369, -0.6642, -0.6528, -0.9000, -0.7509, -0.7526, -0.4450, -0.8461,\n",
       "                       -0.8115, -0.9775, -0.7497, -0.7516, -0.6019, -0.6395, -0.6269, -0.5801,\n",
       "                       -0.7265, -0.9098, -0.6676, -0.9218, -0.6230, -0.5650, -0.6291, -0.5574,\n",
       "                       -0.4324, -0.5311, -0.9425, -0.7710, -0.4693, -0.5734, -1.0696, -0.8103,\n",
       "                       -0.8070, -0.7425, -0.8032, -0.6043, -0.7797, -0.6727, -0.9407, -0.6295,\n",
       "                       -0.7476, -0.5183, -0.7562, -0.3020, -0.7007, -0.3606, -0.5340, -0.5272,\n",
       "                       -0.8156, -0.6149, -0.5626, -0.5676, -0.5442, -0.5097, -0.7090, -0.7554])),\n",
       "              ('regressor.ligand_model.gt_block.3.bn_edge_e.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "              ('regressor.ligand_model.gt_block.3.bn_edge_e.running_var',\n",
       "               tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1.])),\n",
       "              ('regressor.ligand_model.gt_block.3.bn_edge_e.num_batches_tracked',\n",
       "               tensor(0)),\n",
       "              ('regressor.ligand_model.gt_block.4.A.weight',\n",
       "               tensor([[-0.1181, -0.1333, -0.0864,  ..., -0.2698, -0.0238, -0.3539],\n",
       "                       [-0.0565, -0.4220,  0.0492,  ..., -0.1390, -0.1350,  0.1230],\n",
       "                       [-0.1095, -0.1229, -0.4355,  ..., -0.0694, -0.0598, -0.2158],\n",
       "                       ...,\n",
       "                       [-0.0638, -0.0142,  0.2332,  ..., -0.6828, -0.0657, -0.0469],\n",
       "                       [-0.1007,  0.1428, -0.0270,  ...,  0.0035, -0.2893, -0.0614],\n",
       "                       [-0.1383, -0.0072, -0.0681,  ..., -0.1052, -0.1193, -0.3394]])),\n",
       "              ('regressor.ligand_model.gt_block.4.A.bias',\n",
       "               tensor([-0.0248,  0.0205, -0.0336, -0.0077,  0.0432, -0.0375, -0.0103, -0.0299,\n",
       "                        0.0074, -0.0262, -0.0350,  0.0453, -0.0612,  0.0473,  0.0307,  0.0777,\n",
       "                       -0.0494, -0.0755,  0.0477,  0.0578,  0.0392, -0.0064, -0.0711, -0.0808,\n",
       "                       -0.0040,  0.0268,  0.0460, -0.0134,  0.0209, -0.0809,  0.0539, -0.0059,\n",
       "                       -0.0819, -0.0831,  0.0355,  0.0296,  0.0264, -0.0333, -0.0445, -0.0355,\n",
       "                        0.0411, -0.0751,  0.0656,  0.0736, -0.0058,  0.0414, -0.0729, -0.0085,\n",
       "                       -0.0017,  0.0675,  0.0370, -0.0735, -0.0235,  0.0734, -0.0398,  0.0198,\n",
       "                        0.0258, -0.0840,  0.0500, -0.0314,  0.0173, -0.0208, -0.0146,  0.0851,\n",
       "                       -0.0518,  0.0379,  0.0529, -0.0433,  0.0283, -0.0261, -0.0517, -0.0799,\n",
       "                        0.0821,  0.0607, -0.0276, -0.0323,  0.0798, -0.0745, -0.0065,  0.0782,\n",
       "                       -0.0418,  0.0806, -0.0304, -0.0729, -0.0754,  0.0827,  0.0383, -0.0176,\n",
       "                       -0.0050,  0.0644,  0.0424, -0.0298,  0.0683, -0.0630,  0.0432, -0.0811,\n",
       "                        0.0743,  0.0662,  0.0225, -0.0012, -0.0792, -0.0517, -0.0166, -0.0406,\n",
       "                        0.0126,  0.0825, -0.0376,  0.0224, -0.0876, -0.0189,  0.0711, -0.0615,\n",
       "                        0.0545,  0.0860,  0.0334,  0.0285,  0.0500,  0.0603,  0.0250,  0.0366,\n",
       "                       -0.0608, -0.0618, -0.0150, -0.0502, -0.0134, -0.0175, -0.0518, -0.0091])),\n",
       "              ('regressor.ligand_model.gt_block.4.B.weight',\n",
       "               tensor([[ 0.3385, -0.1264,  0.0157,  ...,  0.1385,  0.2425, -0.0623],\n",
       "                       [-0.0892, -0.1237,  0.2051,  ...,  0.1367,  0.2718,  0.0135],\n",
       "                       [-0.2104, -0.1381,  0.5135,  ..., -0.0442,  0.2149, -0.4682],\n",
       "                       ...,\n",
       "                       [-0.7150, -0.1340,  0.1164,  ...,  0.2587, -0.2383,  0.4301],\n",
       "                       [-0.0625, -0.0315, -0.0059,  ..., -0.0219,  0.0521, -0.0971],\n",
       "                       [ 0.0248,  0.2925,  0.0601,  ..., -0.2729,  0.1975, -0.2314]])),\n",
       "              ('regressor.ligand_model.gt_block.4.B.bias',\n",
       "               tensor([-1.7192e-01, -6.9106e-01, -8.5239e-02, -9.4361e-01,  8.4986e-01,\n",
       "                        3.6986e-01,  5.6644e-01, -3.3675e-01,  9.1918e-01, -2.5741e-01,\n",
       "                        2.2422e-01, -6.4619e-01,  8.0783e-01,  3.9400e-01,  2.6324e-01,\n",
       "                       -3.1426e-01, -1.1842e+00,  7.0060e-01,  4.8872e-01,  2.0355e+00,\n",
       "                        1.9437e-01,  6.8165e-03,  3.6094e-01,  4.8665e-01,  3.4151e-02,\n",
       "                        6.5939e-01,  2.8569e-02,  4.2411e-02, -1.4629e-01,  1.6715e-02,\n",
       "                       -1.9321e-02,  1.6532e-01,  5.9634e-01,  2.1572e-01,  6.1452e-01,\n",
       "                        6.6111e-01, -8.6805e-01,  4.8518e-01, -3.3649e-01,  1.9947e-01,\n",
       "                        4.0032e-01, -7.6926e-02, -7.1891e-01,  1.1401e+00,  4.5862e-01,\n",
       "                       -3.4546e-01, -1.6694e+00, -4.4174e-01,  8.9713e-01, -3.4384e-01,\n",
       "                       -5.2086e-02, -1.2638e-01, -2.6531e-01,  6.1159e-01,  3.1459e-01,\n",
       "                        2.7331e-01, -6.1443e-01,  2.7497e-01,  1.2235e+00,  9.0995e-01,\n",
       "                       -1.2670e-01,  7.9284e-02,  4.8655e-01,  4.0617e-01, -1.8383e-01,\n",
       "                        9.2177e-01, -4.1695e-01, -4.3562e-01, -2.6902e-01,  1.3164e-01,\n",
       "                       -2.7735e-02, -4.4981e-03,  7.1478e-02,  8.6438e-02,  1.7709e-01,\n",
       "                       -6.6001e-02,  6.1163e-01,  6.1609e-01,  6.8932e-01,  6.3661e-01,\n",
       "                        1.3959e-01,  1.0461e+00,  1.1882e-01, -2.7793e-01, -9.1740e-02,\n",
       "                       -8.5469e-01,  3.3632e-01,  5.7921e-01,  8.8434e-02, -9.9661e-01,\n",
       "                        6.2581e-01, -1.9761e-01,  3.8896e-01, -1.5379e-01,  1.4673e+00,\n",
       "                        2.8037e-01, -4.6209e-01, -1.2448e-01, -2.6327e-01,  2.8493e-01,\n",
       "                        9.6636e-02,  1.2418e+00,  4.3115e-01,  1.1294e-01,  2.3076e-01,\n",
       "                       -1.6505e-01, -4.9285e-01, -4.0523e-01,  5.4716e-01,  6.9957e-01,\n",
       "                        7.4059e-01,  2.4056e-01, -1.5985e+00,  1.7911e-01,  2.7043e-01,\n",
       "                        2.2618e-01,  4.3525e-01, -1.9894e-03, -2.1778e-01,  7.3582e-01,\n",
       "                       -1.0459e+00,  1.1583e+00,  1.8056e+00,  5.5758e-01, -6.2272e-01,\n",
       "                       -7.6176e-02,  6.4700e-01, -3.6290e-02])),\n",
       "              ('regressor.ligand_model.gt_block.4.C.weight',\n",
       "               tensor([[ 0.3060, -0.2146,  0.2305,  ...,  0.0849,  0.1930, -0.1314],\n",
       "                       [ 0.0810, -0.1346, -0.5062,  ..., -0.2034, -0.1116,  0.0854],\n",
       "                       [ 0.1632, -0.2770,  0.1130,  ..., -0.0084, -0.0683, -0.0260],\n",
       "                       ...,\n",
       "                       [-0.0432, -0.2272,  0.0852,  ..., -0.1411,  0.1893, -0.1004],\n",
       "                       [ 0.0133, -0.0804, -0.0687,  ..., -0.1273, -0.0961, -0.2479],\n",
       "                       [ 0.2450, -0.2382,  0.3517,  ..., -0.0317,  0.0844,  0.0705]])),\n",
       "              ('regressor.ligand_model.gt_block.4.C.bias',\n",
       "               tensor([-0.0388,  0.0880, -0.1368, -0.2990,  0.0574, -0.2336,  0.0406, -0.3658,\n",
       "                       -0.2187, -0.0025, -0.1516, -0.0886, -0.0208, -0.2876, -0.3806, -0.2815,\n",
       "                       -0.1736, -0.0577, -0.0890, -0.0005, -0.2574, -0.2246, -0.3327, -0.0118,\n",
       "                       -0.1632, -0.1168, -0.2280, -0.2059, -0.3752, -0.2237, -0.1285, -0.2690,\n",
       "                       -0.0748, -0.0110, -0.2250, -0.2273,  0.0322, -0.2154,  0.0221, -0.2474,\n",
       "                       -0.1381, -0.0157,  0.0957, -0.0367, -0.2162, -0.1302, -0.0087, -0.1647,\n",
       "                        0.0212,  0.0281,  0.0552, -0.1334, -0.0428, -0.1262, -0.0806,  0.0351,\n",
       "                        0.0458, -0.0496, -0.0895, -0.3657, -0.1006, -0.0665,  0.0421,  0.0267,\n",
       "                       -0.0862, -0.0282, -0.0035,  0.0145,  0.0355,  0.0331, -0.0685, -0.3244,\n",
       "                        0.0606, -0.2813, -0.1292, -0.0948, -0.1162, -0.2486, -0.0576, -0.3049,\n",
       "                       -0.2931,  0.0911,  0.0895,  0.0616, -0.4079, -0.0860, -0.0981, -0.2867,\n",
       "                       -0.0333,  0.0029, -0.0148, -0.1082, -0.0704, -0.3321,  0.0917, -0.2517,\n",
       "                        0.1058, -0.3463,  0.0082, -0.1283, -0.2586, -0.3246, -0.1067, -0.2686,\n",
       "                       -0.2146, -0.0995,  0.0086,  0.0931, -0.1949, -0.2989, -0.1093, -0.0291,\n",
       "                       -0.0897, -0.0184,  0.0148, -0.3330,  0.3933, -0.0141, -0.3811, -0.2430,\n",
       "                       -0.1992, -0.1276,  0.0795, -0.1750,  0.0863, -0.3869, -0.2896,  0.0220])),\n",
       "              ('regressor.ligand_model.gt_block.4.D.weight',\n",
       "               tensor([[-0.1024, -0.6907, -0.0636,  ..., -0.0062,  0.1569,  0.2517],\n",
       "                       [-0.0194, -0.1580, -0.3745,  ..., -0.4800,  0.0897,  0.3900],\n",
       "                       [-0.2050,  0.0389, -0.3949,  ..., -0.1499, -0.1321, -0.3616],\n",
       "                       ...,\n",
       "                       [-0.1586, -0.1202,  0.2254,  ..., -0.1677,  0.1557,  0.0300],\n",
       "                       [ 0.0457,  0.2146, -0.1973,  ..., -0.1154, -0.0381, -0.2602],\n",
       "                       [-0.3616,  0.5787,  0.1655,  ..., -0.4523, -0.4080, -0.2144]])),\n",
       "              ('regressor.ligand_model.gt_block.4.D.bias',\n",
       "               tensor([-0.0829,  0.0558, -0.1545, -0.2600, -0.0052, -0.3153,  0.0053, -0.3969,\n",
       "                       -0.3277,  0.1338, -0.1050, -0.0577,  0.0069, -0.2905, -0.2773, -0.1629,\n",
       "                       -0.1934,  0.0396, -0.1081,  0.0489, -0.3092, -0.2785, -0.2968,  0.1101,\n",
       "                       -0.1507, -0.0722, -0.2498, -0.2132, -0.4081, -0.3603, -0.1938, -0.3216,\n",
       "                       -0.0685,  0.0293, -0.1855, -0.2393,  0.0813, -0.1265, -0.0455, -0.3158,\n",
       "                       -0.1539, -0.0682,  0.0733, -0.1755, -0.1483, -0.0678, -0.0650, -0.2159,\n",
       "                        0.0866,  0.0633, -0.0754, -0.1041,  0.0419, -0.1741, -0.1982,  0.0222,\n",
       "                        0.1371,  0.0224, -0.1378, -0.2298, -0.1642, -0.0794,  0.0596,  0.0823,\n",
       "                       -0.0076,  0.0013, -0.0711, -0.0323, -0.0305,  0.1595, -0.0747, -0.3051,\n",
       "                       -0.0956, -0.3087, -0.1272, -0.0692, -0.2388, -0.1530, -0.1443, -0.3244,\n",
       "                       -0.3290,  0.0839,  0.0051,  0.0235, -0.4304,  0.0243, -0.0401, -0.2101,\n",
       "                        0.0590,  0.0323, -0.0217, -0.1289,  0.0237, -0.2208,  0.1843, -0.2206,\n",
       "                        0.0061, -0.3206, -0.0480, -0.0928, -0.1552, -0.2431, -0.0429, -0.2264,\n",
       "                       -0.2301, -0.1078, -0.0491,  0.1090, -0.3055, -0.3276, -0.0846, -0.0046,\n",
       "                       -0.1827,  0.0331, -0.0608, -0.3775,  0.5399, -0.0985, -0.2993, -0.2341,\n",
       "                       -0.0693, -0.0258,  0.1658, -0.0117,  0.0686, -0.3308, -0.2302,  0.0695])),\n",
       "              ('regressor.ligand_model.gt_block.4.E.weight',\n",
       "               tensor([[ 0.0228,  0.2567, -0.0444,  ..., -0.0783, -0.1880, -0.1301],\n",
       "                       [-0.0165, -0.1344, -0.1336,  ..., -0.0089, -0.3316,  0.4242],\n",
       "                       [-0.0396,  0.0536, -0.1613,  ..., -0.2375, -0.2065,  0.2374],\n",
       "                       ...,\n",
       "                       [ 0.1273,  0.2167,  0.3791,  ..., -0.1437,  0.1503, -0.1436],\n",
       "                       [-0.3148,  0.0565,  0.1615,  ...,  0.0078, -0.1527, -0.2257],\n",
       "                       [-0.0559,  0.0762, -0.0954,  ..., -0.1359,  0.2677, -0.1542]])),\n",
       "              ('regressor.ligand_model.gt_block.4.E.bias',\n",
       "               tensor([-0.0074,  0.1209, -0.1130, -0.3249,  0.0373, -0.2893,  0.0024, -0.3762,\n",
       "                       -0.3128,  0.1523, -0.1769, -0.0907,  0.0223, -0.2366, -0.2375, -0.1866,\n",
       "                       -0.2127, -0.0467, -0.0883,  0.0718, -0.2701, -0.2006, -0.2944,  0.1080,\n",
       "                       -0.1951, -0.0583, -0.2578, -0.2106, -0.4397, -0.2869, -0.1836, -0.4236,\n",
       "                        0.0583, -0.0373, -0.2759, -0.2088,  0.0208, -0.0886, -0.0225, -0.2376,\n",
       "                       -0.1442, -0.0753,  0.0564, -0.1576, -0.2410, -0.1204,  0.0067, -0.2757,\n",
       "                        0.0882,  0.0552,  0.0114, -0.2168,  0.0560, -0.1238, -0.0858,  0.0397,\n",
       "                        0.1729, -0.0288, -0.0929, -0.2048, -0.0588, -0.0446, -0.0023, -0.0072,\n",
       "                       -0.0607,  0.0246, -0.0652, -0.1565, -0.0936,  0.1818, -0.0439, -0.2299,\n",
       "                       -0.0430, -0.2749, -0.2552,  0.0135, -0.1669, -0.2738, -0.0695, -0.2707,\n",
       "                       -0.1556,  0.0982,  0.0790,  0.0143, -0.4573, -0.0583, -0.0808, -0.3180,\n",
       "                       -0.0418,  0.1082, -0.0629, -0.2469, -0.0865, -0.2622,  0.1011, -0.2496,\n",
       "                        0.0014, -0.4444, -0.1419, -0.1115, -0.2067, -0.2623, -0.0906, -0.1463,\n",
       "                       -0.3392, -0.0611,  0.1131,  0.1091, -0.3341, -0.2793, -0.0572,  0.0185,\n",
       "                       -0.1785,  0.0289, -0.0453, -0.2802,  0.4218, -0.0967, -0.2512, -0.2137,\n",
       "                       -0.0841, -0.1594,  0.0535, -0.1329,  0.0134, -0.2677, -0.1573,  0.0439])),\n",
       "              ('regressor.ligand_model.gt_block.4.bn_node_x.weight',\n",
       "               tensor([1.2212, 1.2555, 1.4322, 0.8932, 1.1108, 1.1974, 1.1491, 1.2895, 1.4392,\n",
       "                       1.4549, 1.1166, 1.2049, 1.0272, 1.0888, 1.3313, 1.3851, 0.8432, 1.1014,\n",
       "                       1.0627, 1.1437, 1.0757, 1.1602, 1.0701, 1.0725, 1.2739, 1.4004, 1.3385,\n",
       "                       1.0238, 1.0854, 1.2048, 1.0045, 1.1046, 1.0943, 1.1427, 1.1933, 0.9403,\n",
       "                       1.3473, 1.2446, 1.5136, 1.0426, 1.2017, 1.1875, 1.1983, 1.0877, 1.4793,\n",
       "                       1.3228, 1.3168, 1.2437, 1.1543, 1.2594, 1.0661, 1.1507, 0.9134, 1.2503,\n",
       "                       0.9393, 1.2886, 1.2932, 1.2059, 1.1665, 1.0041, 1.0425, 1.2155, 1.4230,\n",
       "                       1.1672, 1.2864, 1.2905, 1.2999, 1.3312, 1.2118, 1.1742, 1.2703, 1.1660,\n",
       "                       1.1931, 1.2332, 1.2533, 1.1788, 0.9403, 0.9042, 1.3267, 1.0571, 1.1309,\n",
       "                       1.0732, 1.2962, 1.2964, 1.3352, 1.3073, 1.1551, 1.2385, 1.0769, 1.0135,\n",
       "                       1.2527, 1.0963, 1.4020, 1.2946, 1.3184, 1.0976, 1.1111, 1.1775, 1.3824,\n",
       "                       0.9372, 1.0935, 1.3574, 0.8570, 0.8751, 1.2180, 1.1836, 1.4111, 1.1866,\n",
       "                       1.1744, 1.2011, 1.3808, 1.3998, 0.9395, 0.8146, 1.3648, 1.1215, 1.2071,\n",
       "                       1.2321, 1.2841, 1.1542, 1.1018, 1.0480, 1.2788, 1.1157, 0.9607, 1.1967,\n",
       "                       1.2597, 1.2042])),\n",
       "              ('regressor.ligand_model.gt_block.4.bn_node_x.bias',\n",
       "               tensor([-1.1133, -0.7953, -0.5487, -0.4769, -0.6516, -1.0635, -0.7682, -0.8953,\n",
       "                       -0.9083, -0.7955, -0.5525, -0.8087, -0.5608, -0.7366, -0.7183, -1.0294,\n",
       "                       -0.5278, -0.9069, -0.7309, -0.6866, -0.7852, -0.7751, -0.8346, -0.7822,\n",
       "                       -1.0759, -0.8470, -0.8669, -0.8448, -0.7863, -1.0030, -0.6547, -0.8036,\n",
       "                       -1.0629, -0.8198, -0.7669, -0.4461, -0.7512, -0.7886, -0.6130, -0.5947,\n",
       "                       -0.8364, -0.7314, -0.9486, -0.6671, -1.1914, -0.6015, -1.1578, -0.5493,\n",
       "                       -0.9730, -1.0681, -0.6161, -0.6508, -0.7609, -0.9901, -0.8279, -0.6939,\n",
       "                       -0.7082, -0.8656, -0.8124, -0.6061, -0.8288, -0.9276, -0.9357, -0.5036,\n",
       "                       -0.6593, -0.6277, -0.9714, -0.8522, -0.7808, -0.5431, -0.9436, -0.8190,\n",
       "                       -0.8192, -0.7762, -1.0520, -0.7002, -0.5538, -0.6719, -0.7911, -0.8413,\n",
       "                       -0.5835, -0.9553, -0.8789, -0.9985, -1.0363, -0.6084, -0.9145, -0.8278,\n",
       "                       -0.6483, -0.7428, -0.7497, -0.8404, -0.9507, -0.8145, -0.4437, -0.6190,\n",
       "                       -0.6859, -0.6569, -0.7794, -0.8552, -0.6171, -1.0477, -0.6910, -0.5437,\n",
       "                       -0.8559, -0.8590, -0.4674, -0.9532, -0.7126, -0.4839, -0.8693, -0.7793,\n",
       "                       -0.9752, -0.5615, -0.8801, -0.6050, -0.8057, -1.0802, -0.8284, -0.8976,\n",
       "                       -0.7677, -0.6147, -0.9405, -1.0349, -0.6437, -0.7788, -0.8980, -0.7965])),\n",
       "              ('regressor.ligand_model.gt_block.4.bn_node_x.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "              ('regressor.ligand_model.gt_block.4.bn_node_x.running_var',\n",
       "               tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1.])),\n",
       "              ('regressor.ligand_model.gt_block.4.bn_node_x.num_batches_tracked',\n",
       "               tensor(0)),\n",
       "              ('regressor.ligand_model.gt_block.4.bn_edge_e.weight',\n",
       "               tensor([0.8533, 0.9089, 0.4180, 0.9255, 0.8124, 0.4741, 0.8139, 0.5980, 0.4984,\n",
       "                       0.5286, 0.8153, 0.5936, 0.6301, 1.1289, 0.9284, 0.7859, 0.4063, 0.3468,\n",
       "                       0.8819, 0.5550, 0.6485, 0.5514, 0.3489, 1.4732, 0.7094, 0.6767, 0.4631,\n",
       "                       1.1572, 1.1385, 0.5435, 0.3804, 0.6478, 0.4605, 0.7040, 0.4260, 0.8371,\n",
       "                       1.1013, 0.6746, 0.6736, 0.5592, 0.6332, 0.4410, 0.5228, 1.1792, 0.8853,\n",
       "                       1.1626, 0.8106, 0.8635, 0.7256, 1.2700, 1.5537, 0.4144, 0.7235, 0.7344,\n",
       "                       0.9026, 0.5502, 0.3820, 0.6503, 0.6934, 0.4843, 0.6721, 0.3046, 0.6934,\n",
       "                       1.1921, 1.1767, 0.7712, 0.5058, 0.5944, 0.9250, 1.0677, 0.6680, 0.5729,\n",
       "                       0.4161, 0.7040, 0.9517, 1.2448, 0.4727, 1.3461, 0.7415, 0.4289, 0.5077,\n",
       "                       0.8242, 0.4259, 0.4742, 0.5791, 0.4595, 0.8185, 0.8893, 1.1613, 0.4845,\n",
       "                       0.7058, 0.5525, 0.7028, 0.6593, 0.5381, 0.2702, 0.8230, 0.6551, 0.9142,\n",
       "                       0.5576, 0.3807, 0.9534, 0.7354, 0.9013, 0.4576, 0.5923, 0.8420, 0.6174,\n",
       "                       0.5598, 0.8633, 0.6586, 0.7074, 0.6458, 1.0299, 0.6297, 0.3864, 1.4201,\n",
       "                       0.4743, 0.3144, 0.9773, 0.6244, 0.7787, 0.7035, 0.6595, 1.1455, 0.9186,\n",
       "                       0.6892, 0.8407])),\n",
       "              ('regressor.ligand_model.gt_block.4.bn_edge_e.bias',\n",
       "               tensor([-0.8069, -0.9358, -0.7811, -0.7323, -0.7737, -0.5274, -0.5299, -0.4964,\n",
       "                       -0.6172, -0.6256, -0.6749, -0.8406, -0.7550, -0.7365, -0.6428, -0.5658,\n",
       "                       -0.3704, -0.5501, -0.7866, -0.6567, -0.6481, -0.6113, -0.5227, -1.1871,\n",
       "                       -0.4312, -0.7376, -0.3326, -0.8366, -0.7272, -0.6349, -0.3179, -0.5436,\n",
       "                       -0.4250, -0.6221, -0.5737, -0.4892, -1.1397, -0.5821, -0.6046, -0.3203,\n",
       "                       -0.5037, -0.7071, -0.5949, -0.8215, -0.8194, -0.9489, -0.5490, -0.7234,\n",
       "                       -0.8683, -1.1934, -0.6856, -0.4579, -0.6377, -0.7681, -0.5618, -0.5734,\n",
       "                       -0.5870, -0.5374, -0.4951, -0.5109, -0.4471, -0.1716, -0.4026, -0.7961,\n",
       "                       -1.1301, -0.7172, -0.5559, -0.7645, -0.7713, -1.2004, -0.6713, -0.5733,\n",
       "                       -0.6522, -0.5797, -0.6184, -1.0020, -0.4752, -1.1181, -0.6840, -0.4030,\n",
       "                       -0.3870, -0.7048, -0.3541, -0.2983, -0.6578, -0.7086, -0.6922, -0.6576,\n",
       "                       -0.7214, -0.7169, -0.5949, -0.7625, -0.5515, -0.6237, -0.5981, -0.6644,\n",
       "                       -0.6712, -0.5607, -0.6999, -0.5094, -0.4450, -0.7704, -0.3762, -0.8210,\n",
       "                       -0.5975, -0.9314, -0.9877, -0.5930, -0.7137, -0.8676, -0.4900, -0.8803,\n",
       "                       -0.5549, -0.7673, -0.6097, -0.4759, -0.9151, -0.6160, -0.5499, -0.6224,\n",
       "                       -0.5363, -0.7744, -0.6169, -0.5643, -0.7227, -0.7704, -0.5188, -0.7555])),\n",
       "              ('regressor.ligand_model.gt_block.4.bn_edge_e.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "              ('regressor.ligand_model.gt_block.4.bn_edge_e.running_var',\n",
       "               tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1.])),\n",
       "              ('regressor.ligand_model.gt_block.4.bn_edge_e.num_batches_tracked',\n",
       "               tensor(0)),\n",
       "              ('regressor.ligand_model.gt_block.5.A.weight',\n",
       "               tensor([[-0.1042,  0.0257,  0.1005,  ..., -0.0527, -0.0971,  0.0582],\n",
       "                       [ 0.0987, -0.2007, -0.0461,  ...,  0.0078, -0.0078,  0.0365],\n",
       "                       [-0.0600, -0.0579, -0.5893,  ..., -0.0802, -0.1454, -0.1064],\n",
       "                       ...,\n",
       "                       [-0.1185, -0.2009,  0.0812,  ..., -0.5765,  0.0203, -0.0601],\n",
       "                       [ 0.1434, -0.0129,  0.0278,  ..., -0.1008, -0.1554, -0.1055],\n",
       "                       [-0.0400, -0.2174, -0.0087,  ..., -0.1056,  0.0653, -0.0851]])),\n",
       "              ('regressor.ligand_model.gt_block.5.A.bias',\n",
       "               tensor([-0.0307,  0.0189, -0.0078,  0.0337, -0.0699,  0.0204, -0.0091, -0.0373,\n",
       "                       -0.0380, -0.0021,  0.0641,  0.0717, -0.0100, -0.0491, -0.0163, -0.0740,\n",
       "                        0.0422,  0.0699, -0.0309,  0.0579, -0.0239,  0.0429,  0.0813, -0.0660,\n",
       "                       -0.0182,  0.0385, -0.0113, -0.0457, -0.0325,  0.0588,  0.0633, -0.0750,\n",
       "                        0.0506, -0.0167, -0.0657,  0.0393,  0.0681, -0.0270,  0.0554,  0.0621,\n",
       "                       -0.0056,  0.0610,  0.0752,  0.0491,  0.0535, -0.0455,  0.0453, -0.0177,\n",
       "                        0.0323, -0.0554,  0.0021, -0.0355,  0.0586, -0.0673,  0.0206, -0.0800,\n",
       "                       -0.0266, -0.0048,  0.0497,  0.0601, -0.0650,  0.0552, -0.0158,  0.0703,\n",
       "                       -0.0172, -0.0396, -0.0877,  0.0542, -0.0407,  0.0335, -0.0096, -0.0222,\n",
       "                        0.0591, -0.0372,  0.0417, -0.0797, -0.0631,  0.0515,  0.0436,  0.0564,\n",
       "                       -0.0830,  0.0813, -0.0133, -0.0337, -0.0327, -0.0461, -0.0211, -0.0265,\n",
       "                        0.0729,  0.0440, -0.0387, -0.0086, -0.0414, -0.0457, -0.0594, -0.0741,\n",
       "                        0.0223,  0.0603,  0.0732,  0.0415, -0.0651, -0.0129, -0.0876, -0.0772,\n",
       "                       -0.0553,  0.0734,  0.0254, -0.0056,  0.0011, -0.0364, -0.0768,  0.0586,\n",
       "                        0.0048,  0.0247, -0.0092,  0.0784,  0.0096,  0.0386,  0.0518, -0.0798,\n",
       "                       -0.0689,  0.0039, -0.0873,  0.0368, -0.0334, -0.0176, -0.0022, -0.0623])),\n",
       "              ('regressor.ligand_model.gt_block.5.B.weight',\n",
       "               tensor([[ 0.3722, -0.0170,  0.1021,  ..., -0.1989,  0.2094,  0.1589],\n",
       "                       [ 0.4581,  0.4284,  0.0199,  ..., -0.0153, -0.4749,  0.0130],\n",
       "                       [ 0.1372,  0.0304,  0.3549,  ...,  0.0545,  0.0317, -0.3599],\n",
       "                       ...,\n",
       "                       [-0.0320, -0.1835,  0.1905,  ...,  0.4408,  0.0288,  0.0774],\n",
       "                       [ 0.0764, -0.3936,  0.1496,  ...,  0.2676,  0.3910, -0.1751],\n",
       "                       [ 0.1237,  0.2220,  0.2011,  ..., -0.1298,  0.0797,  0.2700]])),\n",
       "              ('regressor.ligand_model.gt_block.5.B.bias',\n",
       "               tensor([ 0.1880,  0.2406,  0.2530,  0.3548, -0.6724,  0.3645,  0.2757,  0.0200,\n",
       "                        0.1775, -0.3516, -0.0367, -0.9545, -0.2404, -0.0842, -0.7519,  1.0727,\n",
       "                       -0.2011, -0.3571, -0.3354,  0.1814,  0.7331,  0.1362,  0.9549,  0.5166,\n",
       "                       -0.0821,  0.0815,  0.7348, -0.0097, -0.1329,  0.6916, -0.0643,  0.1468,\n",
       "                       -0.7503,  0.2196,  0.1084,  1.0021,  0.3205, -0.0774,  0.3065,  0.3753,\n",
       "                        0.6743, -0.2191, -0.8198,  0.8676, -0.2303,  0.3480,  0.7824,  0.3165,\n",
       "                        0.5908, -0.5437,  0.1731,  0.2211,  0.7630,  0.3177,  0.0427,  0.0312,\n",
       "                        0.0707, -0.2192,  0.1189,  0.4232,  0.2559,  0.5287,  0.3249,  1.5243,\n",
       "                       -0.3033,  0.4611,  0.6856,  0.2733, -0.0309,  0.2205, -0.1191,  1.8088,\n",
       "                        0.1564,  0.0038,  0.5915, -0.4136,  0.6362, -0.6590,  0.3197, -0.0548,\n",
       "                        0.1735, -0.2505, -0.1545,  0.6299,  0.8627, -0.1719,  0.3599, -0.1941,\n",
       "                        0.0043,  0.2593, -0.8374,  0.7504, -0.1045,  0.3274, -0.3453,  1.1776,\n",
       "                        0.6755,  0.4713, -0.2792,  1.5629, -0.4620, -1.6576, -0.2819,  0.2022,\n",
       "                       -0.6636, -0.3103,  0.6541,  0.1347,  0.5302,  0.7958,  0.7102,  0.4232,\n",
       "                        0.4575, -1.9259,  0.2810, -0.6783,  0.3845,  0.2767,  0.7098, -0.2774,\n",
       "                        0.7292,  0.7416,  0.5394,  0.3565, -0.7636, -0.3043,  0.5263,  0.3733])),\n",
       "              ('regressor.ligand_model.gt_block.5.C.weight',\n",
       "               tensor([[-0.2011,  0.2702,  0.0225,  ...,  0.0343, -0.2642,  0.0114],\n",
       "                       [ 0.1182,  0.0335, -0.2588,  ...,  0.0209,  0.0231, -0.0054],\n",
       "                       [-0.2582, -0.1383,  0.0685,  ..., -0.0406,  0.0430, -0.2529],\n",
       "                       ...,\n",
       "                       [ 0.1960, -0.0750, -0.1512,  ...,  0.1896,  0.2117, -0.0764],\n",
       "                       [-0.0784, -0.0917,  0.0931,  ..., -0.2200, -0.3675,  0.2004],\n",
       "                       [ 0.0681,  0.3003, -0.0463,  ..., -0.1908,  0.0783,  0.0407]])),\n",
       "              ('regressor.ligand_model.gt_block.5.C.bias',\n",
       "               tensor([-0.2976, -0.2405,  0.0059, -0.1544, -0.1845,  0.0493, -0.3677,  0.1737,\n",
       "                       -0.4434, -0.1075, -0.1889, -0.2882,  0.0372, -0.2036,  0.1184, -0.0721,\n",
       "                        0.0559, -0.2142, -0.0042, -0.2488, -0.2871, -0.1910, -0.2762, -0.1244,\n",
       "                       -0.1091, -0.0838, -0.1368, -0.2891, -0.2559, -0.1668, -0.1583, -0.2220,\n",
       "                       -0.1172, -0.2232, -0.1917,  0.1253,  0.1976,  0.0481, -0.2182, -0.3715,\n",
       "                       -0.2246, -0.1179, -0.0223, -0.1752, -0.3718, -0.0055, -0.1251,  0.0538,\n",
       "                       -0.1756, -0.2437, -0.1933, -0.1589, -0.3022, -0.2605, -0.1266, -0.0567,\n",
       "                       -0.1519, -0.0719, -0.1741, -0.2034, -0.0715, -0.2097, -0.0432,  0.0830,\n",
       "                        0.1297, -0.0443, -0.1033, -0.2830,  0.1084, -0.0258, -0.1734, -0.0099,\n",
       "                       -0.0885, -0.2269,  0.2327, -0.1141, -0.2119, -0.1217, -0.2056, -0.2030,\n",
       "                       -0.1960,  0.0710, -0.0446, -0.0862, -0.0392, -0.2343,  0.0380, -0.1824,\n",
       "                       -0.2161, -0.0668, -0.0848, -0.1016, -0.0727, -0.2964, -0.0399, -0.0896,\n",
       "                        0.0279, -0.1560, -0.2799, -0.0550,  0.0700, -0.0624, -0.3365,  0.0822,\n",
       "                       -0.1065, -0.2282, -0.4226, -0.0703,  0.0097,  0.0094, -0.0212, -0.2923,\n",
       "                       -0.2170,  0.0578, -0.0550, -0.1279, -0.1281,  0.1372, -0.2423, -0.0212,\n",
       "                        0.0480, -0.2412,  0.0063, -0.3634,  0.0059, -0.2377, -0.1474, -0.1044])),\n",
       "              ('regressor.ligand_model.gt_block.5.D.weight',\n",
       "               tensor([[-0.1274,  0.0884, -0.0106,  ..., -0.1602,  0.0873,  0.0184],\n",
       "                       [-0.4779, -0.4216,  0.0234,  ...,  0.0617, -0.2743, -0.0826],\n",
       "                       [-0.4688, -0.2425, -0.0800,  ...,  0.1053,  0.0673,  0.4630],\n",
       "                       ...,\n",
       "                       [-0.3220, -0.1705, -0.3701,  ..., -0.1915, -0.5877, -0.1359],\n",
       "                       [ 0.0353,  0.2149, -0.2677,  ..., -0.4028, -0.4518,  0.2285],\n",
       "                       [ 0.1527, -0.5406, -0.3375,  ..., -0.8387,  0.0435,  0.1025]])),\n",
       "              ('regressor.ligand_model.gt_block.5.D.bias',\n",
       "               tensor([-0.2447, -0.1847, -0.0314, -0.1159, -0.2874,  0.1238, -0.4144,  0.0301,\n",
       "                       -0.4362, -0.1404, -0.1915, -0.1257,  0.0426, -0.0594,  0.0125, -0.0079,\n",
       "                        0.0459, -0.1352,  0.0931, -0.1226, -0.3396, -0.1915, -0.1824, -0.0341,\n",
       "                       -0.0020, -0.1502, -0.0902, -0.2540, -0.2632, -0.2194, -0.1571, -0.0904,\n",
       "                       -0.0390, -0.3143, -0.1424,  0.1336,  0.2248, -0.0471, -0.1702, -0.3306,\n",
       "                       -0.0825, -0.1041, -0.0207, -0.1940, -0.2988, -0.0183, -0.0450,  0.1551,\n",
       "                       -0.0990, -0.0938, -0.2227, -0.1515, -0.3810, -0.2274, -0.0681,  0.0203,\n",
       "                       -0.0839, -0.0406, -0.1270, -0.1609, -0.0742, -0.1244, -0.0383,  0.0997,\n",
       "                        0.1232, -0.1116, -0.0514, -0.4107, -0.0462, -0.1446, -0.1393,  0.1173,\n",
       "                       -0.1518, -0.2382,  0.1907, -0.0356, -0.1933, -0.1365, -0.2281, -0.2251,\n",
       "                       -0.1554,  0.0120, -0.0437, -0.0150, -0.0667, -0.2913, -0.0502, -0.2392,\n",
       "                       -0.2372, -0.1306, -0.2169, -0.0915,  0.0040, -0.3707, -0.1396, -0.1353,\n",
       "                       -0.0526, -0.1472, -0.2816, -0.0706,  0.0234, -0.1909, -0.3279,  0.0865,\n",
       "                       -0.0818, -0.1311, -0.3073, -0.0345,  0.0774,  0.0981,  0.0981, -0.2189,\n",
       "                       -0.2249, -0.0297, -0.0751, -0.1119,  0.0010,  0.0400, -0.1847, -0.1154,\n",
       "                        0.0512, -0.2112, -0.1056, -0.2897, -0.0011, -0.2213, -0.1073, -0.1473])),\n",
       "              ('regressor.ligand_model.gt_block.5.E.weight',\n",
       "               tensor([[ 0.0431, -0.0146,  0.0622,  ..., -0.2198,  0.0667, -0.0911],\n",
       "                       [-0.0209,  0.0368, -0.3618,  ..., -0.2341,  0.0323, -0.0398],\n",
       "                       [ 0.1816, -0.0596, -0.0907,  ..., -0.1029, -0.0589, -0.1568],\n",
       "                       ...,\n",
       "                       [ 0.3238,  0.1531, -0.1398,  ..., -0.1342, -0.0336,  0.0616],\n",
       "                       [-0.0952,  0.2782, -0.0336,  ..., -0.3253, -0.1721, -0.1291],\n",
       "                       [-0.4040,  0.0728, -0.3950,  ...,  0.0115, -0.2322, -0.0621]])),\n",
       "              ('regressor.ligand_model.gt_block.5.E.bias',\n",
       "               tensor([-0.2671, -0.2666, -0.0672, -0.1195, -0.1703,  0.1191, -0.3276,  0.0278,\n",
       "                       -0.5167, -0.1816, -0.2133, -0.2271, -0.0619, -0.2101,  0.0524, -0.0430,\n",
       "                       -0.0334, -0.1958, -0.0212, -0.2103, -0.2519, -0.1965, -0.2381, -0.1508,\n",
       "                        0.0097, -0.1661, -0.1119, -0.3669, -0.2773, -0.2009, -0.1534, -0.0882,\n",
       "                       -0.0654, -0.2717, -0.1023,  0.1155,  0.2051, -0.0525, -0.1765, -0.2252,\n",
       "                       -0.1432, -0.1413, -0.0222, -0.1053, -0.3616, -0.1095, -0.1967,  0.0119,\n",
       "                       -0.0360, -0.1144, -0.1796, -0.2397, -0.4211, -0.2242,  0.0090,  0.0070,\n",
       "                       -0.0213, -0.0650, -0.0855, -0.0988, -0.1709, -0.1140, -0.2003,  0.0110,\n",
       "                        0.1446, -0.1180, -0.0318, -0.4100,  0.1092, -0.0089, -0.2768, -0.0053,\n",
       "                       -0.2016, -0.1303,  0.1835, -0.0394, -0.2665, -0.0730, -0.2188, -0.1627,\n",
       "                       -0.0789, -0.0569, -0.0905, -0.0833, -0.0575, -0.2836, -0.0190, -0.2415,\n",
       "                       -0.2473, -0.1674, -0.1391,  0.0050, -0.0797, -0.3494,  0.0273, -0.1379,\n",
       "                        0.0156, -0.1992, -0.2848, -0.1008,  0.0221, -0.1110, -0.3225,  0.0843,\n",
       "                       -0.0141, -0.2573, -0.3007, -0.0126, -0.0380,  0.0908,  0.0434, -0.2779,\n",
       "                       -0.1584,  0.0337,  0.0303, -0.1680, -0.0161,  0.0517, -0.2598, -0.1571,\n",
       "                        0.1111, -0.2484, -0.1329, -0.3296, -0.0820, -0.2536, -0.0868, -0.0942])),\n",
       "              ('regressor.ligand_model.gt_block.5.bn_node_x.weight',\n",
       "               tensor([1.2933, 1.1700, 0.9981, 0.8457, 0.9796, 1.0771, 1.0544, 1.4978, 1.1912,\n",
       "                       1.2194, 1.0873, 1.2141, 1.1532, 1.5506, 1.1297, 1.2932, 1.0324, 1.1599,\n",
       "                       0.9979, 1.1642, 1.1666, 1.3314, 1.1247, 1.4090, 1.2196, 1.2630, 1.4210,\n",
       "                       1.0343, 1.4155, 1.4307, 1.3908, 1.3253, 1.2307, 1.1499, 1.5488, 1.1938,\n",
       "                       1.2850, 1.4521, 1.2090, 1.3815, 1.3172, 1.1467, 1.0122, 1.3929, 1.3762,\n",
       "                       1.3681, 1.1358, 1.1182, 1.2599, 1.4836, 1.3263, 1.2060, 1.1311, 1.3508,\n",
       "                       1.2907, 1.2307, 1.4235, 1.2264, 1.4201, 1.0158, 1.1441, 1.1421, 1.1991,\n",
       "                       1.4328, 1.1244, 1.2575, 1.4284, 1.4795, 1.3359, 1.3633, 1.1038, 1.1648,\n",
       "                       1.3004, 1.0133, 1.4032, 1.3147, 0.9677, 1.1823, 0.9266, 1.3749, 1.3249,\n",
       "                       1.4128, 1.1816, 1.3910, 1.2062, 1.3471, 1.3021, 1.0228, 1.0398, 1.1761,\n",
       "                       1.1305, 1.0658, 1.4048, 1.1379, 1.1792, 1.3642, 0.9893, 1.0062, 1.2332,\n",
       "                       1.0833, 1.1803, 1.3253, 1.1387, 0.6249, 1.1739, 1.2480, 1.4639, 1.7834,\n",
       "                       1.4075, 1.3960, 1.4945, 1.1343, 0.9441, 0.8586, 1.2288, 1.2614, 1.2064,\n",
       "                       1.3451, 1.4457, 1.2798, 1.4846, 1.1532, 1.2343, 1.4352, 1.3403, 1.2887,\n",
       "                       1.6231, 1.7436])),\n",
       "              ('regressor.ligand_model.gt_block.5.bn_node_x.bias',\n",
       "               tensor([-0.8088, -0.5476, -0.1068, -0.4238, -0.4815, -0.7011, -0.7613, -1.1654,\n",
       "                       -0.7322, -0.6763, -0.5676, -0.7588, -0.9093, -0.8803, -0.4014, -0.6087,\n",
       "                       -0.9553, -0.8111, -0.5100, -0.7268, -0.4995, -1.2822, -0.8995, -0.9073,\n",
       "                       -0.9241, -0.3052, -0.7908, -0.8912, -0.9382, -0.5293, -0.7234, -1.0029,\n",
       "                       -0.7921, -0.7728, -0.9564, -0.7325, -0.4649, -0.8798, -0.3263, -0.8377,\n",
       "                       -0.7683, -0.5951, -0.7817, -0.5481, -0.6980, -0.6035, -0.6873, -0.5362,\n",
       "                       -0.8739, -0.6644, -0.8534, -0.6024, -0.7510, -0.7878, -1.1126, -0.5900,\n",
       "                       -0.5665, -0.5980, -0.9346, -0.6106, -0.7035, -0.6798, -0.6993, -0.5596,\n",
       "                       -0.5025, -0.7778, -0.7710, -1.0493, -0.7644, -0.7076, -0.7558, -0.3569,\n",
       "                       -0.8170, -0.7096, -0.7382, -0.7364, -0.6401, -0.9819, -0.4124, -1.0997,\n",
       "                       -0.6949, -1.0129, -0.8437, -0.8615, -0.6999, -0.3067, -0.5864, -0.6862,\n",
       "                       -0.3134, -0.8572, -0.6809, -0.8090, -0.6523, -0.5758, -0.3644, -0.8114,\n",
       "                       -0.4247, -0.4048, -0.4093, -0.6491, -0.7834, -0.7162, -0.9296, -0.4556,\n",
       "                       -0.4323, -0.7311, -0.1322, -0.3572, -0.7481, -0.6169, -0.9895, -0.4985,\n",
       "                       -0.9266, -0.6004, -0.9855, -0.7590, -0.7874, -0.7971, -0.5877, -0.6840,\n",
       "                       -0.9513, -0.4125, -0.8508, -0.8824, -0.8765, -0.6833, -0.7946, -0.6237])),\n",
       "              ('regressor.ligand_model.gt_block.5.bn_node_x.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "              ('regressor.ligand_model.gt_block.5.bn_node_x.running_var',\n",
       "               tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1.])),\n",
       "              ('regressor.ligand_model.gt_block.5.bn_node_x.num_batches_tracked',\n",
       "               tensor(0)),\n",
       "              ('regressor.ligand_model.gt_block.5.bn_edge_e.weight',\n",
       "               tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1.])),\n",
       "              ('regressor.ligand_model.gt_block.5.bn_edge_e.bias',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "              ('regressor.ligand_model.gt_block.5.bn_edge_e.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "              ('regressor.ligand_model.gt_block.5.bn_edge_e.running_var',\n",
       "               tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1.])),\n",
       "              ('regressor.ligand_model.gt_block.5.bn_edge_e.num_batches_tracked',\n",
       "               tensor(0)),\n",
       "              ('regressor.target_model.node_encoder.weight',\n",
       "               tensor([[ 0.0344,  0.1396, -0.1045,  ...,  0.0262,  0.1453,  0.0250],\n",
       "                       [-0.1768, -0.3055,  0.1229,  ...,  0.0066,  0.0617, -0.0695],\n",
       "                       [-0.1894, -0.2799, -0.2638,  ..., -0.0396, -0.0969, -0.0317],\n",
       "                       ...,\n",
       "                       [-0.0528,  0.0663,  0.0960,  ..., -0.1376, -0.1148,  0.0728],\n",
       "                       [ 0.3854, -0.0877,  0.3326,  ..., -0.0125,  0.0938, -0.0256],\n",
       "                       [-0.0304, -0.0574, -0.0219,  ..., -0.0828, -0.0924, -0.0054]])),\n",
       "              ('regressor.target_model.node_encoder.bias',\n",
       "               tensor([-1.6230e-01, -3.1078e-01,  7.6090e-02, -1.9703e-01, -1.1125e-01,\n",
       "                        6.4047e-02,  7.3396e-02,  8.9568e-02, -1.2557e-01, -6.7841e-02,\n",
       "                        1.3197e-01, -1.8974e-01,  2.9184e-02, -4.3743e-02, -3.9341e-01,\n",
       "                        1.1639e-01,  2.1065e-01, -6.3260e-02, -2.1440e-01, -2.3835e-01,\n",
       "                        2.5763e-01, -8.1362e-02, -1.9574e-01,  2.4822e-01,  3.6136e-02,\n",
       "                       -1.8451e-01,  3.4632e-02,  1.4838e-01, -1.1149e-01,  8.3799e-03,\n",
       "                       -4.5054e-02, -4.0075e-01, -1.2100e-02,  2.5994e-01, -2.6991e-01,\n",
       "                       -2.7201e-02, -1.2611e-01, -1.8456e-01, -1.2449e-01,  5.8876e-02,\n",
       "                        2.3636e-01, -6.5329e-02,  2.7874e-01, -1.0118e-01, -3.3470e-01,\n",
       "                       -2.8391e-01,  1.6413e-01, -6.2839e-02,  3.9915e-01, -1.5824e-01,\n",
       "                       -5.5752e-05, -7.3335e-02,  3.8792e-02, -1.9027e-01,  2.8480e-01,\n",
       "                       -6.2757e-02,  1.6771e-02, -2.5447e-01,  2.3195e-01,  9.9158e-02,\n",
       "                       -3.8469e-02,  5.3316e-02, -2.2858e-01,  2.4796e-01,  9.4486e-02,\n",
       "                       -4.4432e-02, -1.8711e-01,  3.9015e-02,  2.2213e-01, -1.2060e-01,\n",
       "                       -1.5924e-01,  1.9852e-01, -8.1700e-02, -7.9731e-02, -7.9818e-02,\n",
       "                        1.5031e-01,  4.9583e-01, -1.0913e-01, -3.3151e-01, -1.6109e-01,\n",
       "                        1.2139e-01, -2.8079e-01,  1.6440e-01, -4.8131e-02, -2.6087e-01,\n",
       "                        3.4922e-01,  2.9570e-01,  5.8006e-02,  1.7768e-01,  3.7409e-01,\n",
       "                        1.1505e-01,  7.7285e-02,  1.7307e-01, -1.4323e-01,  6.1711e-03,\n",
       "                        2.5703e-01, -4.3094e-02,  8.4940e-02, -1.7444e-01, -2.6674e-01,\n",
       "                       -2.7209e-01, -2.1846e-01, -1.9697e-02,  3.4655e-01, -2.0677e-01,\n",
       "                       -4.0107e-02, -2.3586e-01, -1.5606e-01,  8.2487e-02, -1.7746e-01,\n",
       "                       -1.0562e-01,  7.4388e-02, -9.2215e-02, -1.7904e-01, -6.8700e-02,\n",
       "                       -4.4899e-02,  2.0238e-02, -2.1665e-01,  2.9202e-03,  9.0225e-02,\n",
       "                       -1.6147e-01,  5.9202e-02,  2.1149e-01, -3.7993e-01, -2.2033e-01,\n",
       "                        6.1654e-02,  1.2647e-01, -1.6122e-01])),\n",
       "              ('regressor.target_model.edge_encoder.weight',\n",
       "               tensor([[ 6.6815e-02,  2.5524e-01, -4.4725e-01, -3.0214e-01,  2.5593e-01],\n",
       "                       [-5.9301e-02, -7.0864e-02, -6.8256e-01,  7.3498e-02,  7.9141e-02],\n",
       "                       [-2.8932e-01,  9.3501e-01, -7.3647e-01, -5.1838e-01,  5.2788e-02],\n",
       "                       [-4.2513e-01,  8.1810e-01, -6.1161e-01, -4.0150e-01, -3.5212e-01],\n",
       "                       [ 9.1131e-01, -2.9928e-01, -6.7146e-01,  7.2703e-01,  4.8683e-01],\n",
       "                       [ 1.7586e-02, -3.7837e-01,  3.6268e-01, -6.1625e-01,  3.7214e-01],\n",
       "                       [ 2.2903e-01, -3.8098e-01, -4.0750e-01,  5.2844e-01, -1.6814e-01],\n",
       "                       [-2.0755e-01,  8.9026e-01, -6.2650e-01, -6.5142e-01,  2.1095e-01],\n",
       "                       [ 1.0887e-01,  5.8055e-01, -8.3194e-01, -2.6245e-02,  1.8884e-01],\n",
       "                       [-3.1716e-01, -6.2095e-01,  6.2790e-01, -5.0609e-01,  2.1438e-01],\n",
       "                       [ 1.1619e-01, -8.9600e-01,  1.0056e+00, -8.1975e-02, -2.0132e-02],\n",
       "                       [ 2.8728e-01,  7.0856e-01, -5.7806e-02,  2.2403e-01, -6.2540e-01],\n",
       "                       [-3.7297e-01,  2.8483e-01,  1.0486e+00, -1.1163e+00, -3.9355e-01],\n",
       "                       [ 2.8239e-01,  2.1301e-01,  1.6641e-01,  2.2111e-02, -3.7801e-01],\n",
       "                       [-5.0344e-01,  2.9197e-01,  3.7233e-01,  2.1402e-01,  1.8577e-01],\n",
       "                       [ 1.3180e-01, -2.2979e-03,  9.0702e-01,  4.3424e-01, -8.2839e-01],\n",
       "                       [ 1.7169e-01,  4.4565e-01, -1.0061e+00, -3.4824e-01,  3.9880e-01],\n",
       "                       [ 4.3972e-01,  5.4215e-01, -6.6751e-01, -3.5936e-01,  3.5824e-02],\n",
       "                       [ 2.7228e-01, -5.7413e-02,  1.6417e-01,  6.1588e-01, -2.2865e-01],\n",
       "                       [ 2.9718e-01, -1.0871e-01,  1.0285e+00,  3.9524e-02, -4.2731e-01],\n",
       "                       [-3.3140e-02,  3.4705e-01, -1.1676e+00,  5.3302e-01,  1.2417e-01],\n",
       "                       [ 7.0784e-03, -6.9190e-01, -6.1365e-01, -8.1182e-01,  7.4926e-01],\n",
       "                       [ 7.4046e-02, -1.4388e-01,  8.0877e-01, -4.1992e-01, -2.1997e-01],\n",
       "                       [ 3.1514e-01,  5.2279e-01, -4.1043e-01,  2.6252e-01,  2.9217e-01],\n",
       "                       [-2.7257e-01, -1.0464e-01, -8.2500e-02, -6.8997e-01,  2.5717e-01],\n",
       "                       [-3.1093e-01, -6.5039e-01,  5.9864e-01,  3.8279e-01, -6.1061e-01],\n",
       "                       [-3.0238e-02, -7.3241e-02,  9.0548e-01,  5.9524e-01, -1.4818e-01],\n",
       "                       [-1.1215e-01, -3.6718e-01, -2.8094e-02, -5.5611e-01, -4.7643e-03],\n",
       "                       [-7.2127e-01,  2.3800e-01, -4.0097e-01, -8.8044e-01, -3.2304e-01],\n",
       "                       [ 7.0630e-01, -3.1212e-01,  4.0061e-01,  8.3323e-01, -8.1467e-02],\n",
       "                       [ 6.1998e-01,  1.3077e+00, -9.7944e-01,  8.3716e-01, -7.9389e-01],\n",
       "                       [ 4.0388e-01, -3.9378e-01, -9.9821e-02,  6.9690e-01,  2.2047e-01],\n",
       "                       [ 1.0497e-01, -1.0571e+00,  6.2758e-01, -3.0037e-01,  4.7507e-01],\n",
       "                       [ 4.5639e-01,  2.6236e-02, -2.1033e-01,  1.1113e+00,  8.5275e-02],\n",
       "                       [-2.7273e-01, -8.1058e-02, -5.7354e-01, -5.9668e-01,  1.0138e-01],\n",
       "                       [-4.8424e-01,  5.6511e-02,  3.3136e-01, -1.8009e+00, -3.5498e-01],\n",
       "                       [ 1.2496e-01,  9.3134e-01, -5.2189e-01,  2.6778e-01, -2.4424e-01],\n",
       "                       [-1.1165e-01,  2.1536e-01,  3.4389e-01,  1.6263e-01, -5.3165e-01],\n",
       "                       [ 3.6523e-01, -2.0648e-01, -2.8196e-01,  1.1735e+00,  4.0961e-02],\n",
       "                       [ 1.2043e+00, -4.3347e-01,  7.9466e-01, -4.9391e-01, -2.4987e-01],\n",
       "                       [-1.4296e-01, -7.1365e-01, -4.6037e-01,  8.3748e-01,  3.7708e-01],\n",
       "                       [ 2.7242e-01,  5.8807e-01, -6.4455e-01, -5.3348e-01,  2.6260e-01],\n",
       "                       [ 9.1704e-02,  2.6608e-01,  5.3426e-01, -2.1548e-01, -5.0217e-01],\n",
       "                       [-3.6299e-01, -1.0226e-01, -4.4004e-01, -9.6759e-01, -1.8248e-01],\n",
       "                       [-2.1082e-01, -8.6507e-01, -3.6071e-01, -6.2192e-01,  9.5300e-01],\n",
       "                       [-4.3165e-01,  7.0781e-02, -7.9962e-01, -9.0434e-01,  3.0050e-02],\n",
       "                       [-2.8553e-01, -6.7797e-02,  9.0479e-01,  5.8934e-01,  3.0635e-02],\n",
       "                       [-4.4083e-01, -6.3072e-01,  4.9720e-01,  4.6054e-01, -2.3237e-01],\n",
       "                       [-2.1165e-01, -7.7869e-01,  7.3541e-01,  2.8479e-01, -1.9833e-01],\n",
       "                       [-3.6689e-02, -2.4014e-01,  2.0930e-01,  1.6843e-01, -5.1638e-01],\n",
       "                       [ 1.7838e-01,  5.9161e-01, -7.6585e-01, -1.6443e-01,  3.5639e-01],\n",
       "                       [-3.0871e-02,  2.5816e-01, -8.2657e-01,  8.6009e-01, -1.7556e-01],\n",
       "                       [-1.6558e-01, -4.9830e-01,  7.5277e-01,  3.5745e-01, -5.7961e-01],\n",
       "                       [-3.2904e-02, -8.3564e-01,  1.0786e+00, -1.7136e-01, -4.9740e-02],\n",
       "                       [ 4.1358e-02, -2.0367e-02,  2.0524e-01, -4.9699e-01, -1.8553e-01],\n",
       "                       [-3.3674e-01,  1.5309e-01,  1.9350e-02,  4.2268e-01, -3.6537e-01],\n",
       "                       [ 2.8700e-02,  9.2867e-01, -5.6232e-01, -3.8829e-01, -1.7523e-01],\n",
       "                       [-1.9060e-01, -5.6758e-01,  6.5981e-01, -6.0980e-01,  3.9036e-01],\n",
       "                       [ 4.8269e-01, -7.8836e-01,  9.0122e-01,  6.5078e-02, -3.8362e-02],\n",
       "                       [-1.4659e-01,  9.1444e-02,  2.1372e-01,  3.7796e-01, -4.1597e-01],\n",
       "                       [-5.2631e-01, -4.0377e-02, -4.8708e-01, -3.1650e-01,  5.1673e-01],\n",
       "                       [ 2.1238e-01, -1.2415e-01,  2.5563e-01,  1.2973e-01,  1.5695e-01],\n",
       "                       [ 5.1349e-01,  5.0744e-01,  4.1263e-01, -1.1137e+00, -2.5051e-01],\n",
       "                       [ 1.9891e-02, -8.8261e-02,  7.0729e-01,  1.8389e-01,  3.7584e-02],\n",
       "                       [-3.9983e-01, -5.8468e-01,  8.0089e-01, -5.7478e-01, -1.3430e-01],\n",
       "                       [ 2.4737e-01, -3.1078e-01, -6.3505e-01, -1.5637e-01,  9.7383e-02],\n",
       "                       [-4.1311e-02,  2.4895e-01, -5.2045e-01,  1.8615e-01, -1.4519e-01],\n",
       "                       [ 4.0917e-01,  6.9480e-02, -3.8579e-01,  2.9601e-01, -1.7824e-01],\n",
       "                       [-9.5837e-02,  4.8879e-01,  4.8327e-02, -6.7452e-01,  1.9283e-01],\n",
       "                       [ 5.4569e-01, -4.6675e-01,  2.3786e-01,  7.1335e-01, -7.7957e-02],\n",
       "                       [ 1.6463e-01, -6.5976e-01,  2.2080e-01,  1.5904e-01,  3.8356e-01],\n",
       "                       [ 3.5104e-01,  7.7497e-01,  8.5724e-01,  2.3610e-01, -5.3206e-01],\n",
       "                       [ 2.1236e-01, -4.4151e-01,  1.5852e-01,  2.0549e-01,  2.5553e-01],\n",
       "                       [-1.8234e-02, -7.2881e-02,  1.0015e+00, -8.5895e-01, -2.1245e-01],\n",
       "                       [ 1.5157e-01, -6.9742e-01,  5.5829e-01, -9.1502e-01,  4.9924e-01],\n",
       "                       [-1.0974e-01, -4.9078e-01,  4.2095e-01, -9.7849e-02,  1.0229e-01],\n",
       "                       [-2.3831e-01, -3.8714e-01,  8.9187e-01, -3.6575e-01, -2.6119e-01],\n",
       "                       [-4.7572e-02, -5.7620e-01,  4.9016e-01, -3.9598e-01,  5.6946e-01],\n",
       "                       [-8.1541e-02,  6.1044e-02, -5.8080e-01, -4.5664e-01,  4.8312e-01],\n",
       "                       [-5.1855e-01,  5.6231e-01, -3.9676e-01,  3.3726e-01, -7.1023e-01],\n",
       "                       [ 5.2961e-02, -8.1526e-01,  8.1190e-01,  1.6624e-01, -5.4378e-01],\n",
       "                       [-2.3021e-02,  8.2926e-01, -1.0703e+00, -2.6057e-01,  4.5595e-01],\n",
       "                       [ 4.9681e-01, -8.1037e-01,  7.4803e-01,  1.0401e+00, -1.4188e-01],\n",
       "                       [ 1.2690e-01, -4.8033e-01, -2.0318e-01,  2.6356e-01, -3.5514e-01],\n",
       "                       [ 2.9558e-01, -3.3543e-01, -8.5908e-02,  7.0749e-01,  9.6493e-02],\n",
       "                       [ 1.7094e-01,  2.4131e-01, -7.2044e-01,  8.0964e-01, -3.0931e-01],\n",
       "                       [ 1.9039e-01,  6.0381e-01, -9.5948e-01, -1.0186e+00, -1.3948e-01],\n",
       "                       [-5.3748e-02, -2.7336e-01, -1.2167e+00, -1.1044e-01,  3.4028e-01],\n",
       "                       [ 3.2149e-01,  6.0390e-01,  2.5973e-01, -1.4996e-01, -5.9410e-01],\n",
       "                       [-8.1828e-02,  7.9050e-01, -1.2755e-01, -8.3496e-01, -6.7013e-01],\n",
       "                       [ 4.5706e-02,  6.3148e-01, -3.0997e-01, -7.6268e-01, -1.4384e-01],\n",
       "                       [-1.4978e-01, -4.9248e-01, -3.3418e-01,  1.0427e+00, -1.5876e-01],\n",
       "                       [ 5.3153e-01, -5.8995e-01,  5.9674e-01,  8.9925e-01, -2.6674e-03],\n",
       "                       [ 5.6649e-02, -1.7465e-01,  6.6269e-01, -3.4406e-01, -7.7521e-02],\n",
       "                       [-2.7243e-01,  2.7619e-01,  2.8240e-01, -8.8430e-01,  3.7939e-02],\n",
       "                       [-5.0920e-02, -3.1190e-01,  8.8704e-01, -3.4240e-01, -2.8485e-01],\n",
       "                       [-9.0437e-03,  1.9371e-02,  1.2620e-01,  8.5509e-01,  1.6202e-01],\n",
       "                       [-1.6709e-01, -1.9757e-01,  7.2372e-01, -3.3325e-01,  3.0270e-01],\n",
       "                       [ 2.2516e-01, -1.0048e+00,  4.9967e-01,  3.7245e-01,  5.2287e-01],\n",
       "                       [-2.6116e-01, -2.9320e-02,  5.1836e-01, -4.5798e-01,  2.2562e-02],\n",
       "                       [-6.4137e-01, -7.0853e-01,  2.4858e-01, -4.1183e-01, -4.6079e-01],\n",
       "                       [ 8.9520e-01, -4.0146e-02,  8.4001e-01, -2.5988e-01, -1.2403e-02],\n",
       "                       [-2.1456e-01,  5.5989e-01,  6.5272e-01, -8.1255e-01, -3.6193e-01],\n",
       "                       [-4.6170e-01, -8.5592e-01,  2.7035e-01, -1.2481e-01,  2.2147e-01],\n",
       "                       [-2.8143e-01,  2.5846e-01, -5.6580e-01, -7.1176e-01,  6.7082e-01],\n",
       "                       [ 4.2545e-01,  6.5838e-01, -4.0162e-01, -1.3214e-01,  1.8298e-01],\n",
       "                       [ 1.0056e-01,  1.4046e-01, -6.9526e-01,  2.4200e-01,  2.5213e-01],\n",
       "                       [-1.6651e-01, -8.1782e-01,  7.4307e-01,  1.1011e+00, -8.0562e-01],\n",
       "                       [-2.5669e-01,  5.9523e-01,  9.1655e-02, -1.1454e+00,  2.4374e-01],\n",
       "                       [ 2.1427e-01,  1.0776e+00, -7.1044e-01, -7.7861e-02, -3.5896e-01],\n",
       "                       [-2.1725e-01, -8.0148e-01,  1.4983e-01,  4.9201e-01,  3.5212e-01],\n",
       "                       [-5.8077e-01, -4.0935e-01, -3.0245e-01,  9.5433e-01,  1.4638e-02],\n",
       "                       [ 5.2978e-01,  2.2041e-01,  1.2314e+00, -4.9619e-01,  5.5896e-01],\n",
       "                       [-6.5983e-02, -3.3362e-01,  1.0003e+00, -3.4772e-01, -5.2137e-01],\n",
       "                       [-2.7223e-01,  1.5377e-01,  2.2899e-01, -7.0438e-01,  3.9190e-01],\n",
       "                       [ 1.5497e-01,  2.8668e-01,  6.4766e-01,  8.1173e-01,  7.7699e-02],\n",
       "                       [ 1.7242e-01,  7.4149e-02, -1.0761e+00,  6.4758e-01, -1.6967e-03],\n",
       "                       [-1.1035e-01, -5.4386e-01,  3.7455e-01,  1.3281e-01, -7.5808e-02],\n",
       "                       [ 2.2788e-01,  7.4141e-01, -6.1727e-02, -9.8527e-01, -1.5365e-01],\n",
       "                       [ 3.1889e-02,  1.7815e-01,  4.0404e-01, -1.6786e-01, -1.7550e-02],\n",
       "                       [-1.2958e-01, -5.8897e-01,  8.1893e-01, -1.2482e-01, -3.6011e-01],\n",
       "                       [ 5.4111e-02,  2.4825e-01, -1.0396e+00,  3.1795e-01,  1.7737e-01],\n",
       "                       [-1.3667e-01, -8.5915e-01,  5.8472e-02, -4.7817e-01,  6.3324e-01],\n",
       "                       [-2.0417e-01, -3.0914e-01,  8.5551e-01, -5.7099e-01,  1.7516e-01],\n",
       "                       [-1.9137e-01, -3.2812e-01, -6.6359e-02, -1.2232e+00,  5.5308e-01],\n",
       "                       [-3.8397e-01, -7.0957e-01,  4.2956e-01,  1.0809e+00, -4.2973e-01],\n",
       "                       [-3.7001e-01,  2.1564e-02,  3.7475e-01,  1.1759e-01, -2.8617e-01],\n",
       "                       [-8.3982e-02, -4.2427e-01, -6.9177e-01,  1.1356e+00, -2.4901e-01]])),\n",
       "              ('regressor.target_model.edge_encoder.bias',\n",
       "               tensor([ 1.8301e-02,  2.8836e-01,  6.7015e-02,  2.1204e-01,  1.1829e-01,\n",
       "                       -2.4800e-01, -2.0487e-01, -2.5303e-01,  1.1704e-01,  2.0342e-02,\n",
       "                       -8.1998e-02, -6.3677e-02, -2.4269e-01, -2.1567e-02, -1.9771e-01,\n",
       "                       -1.1777e-01, -3.2419e-01, -5.3050e-02,  8.7424e-02, -4.1121e-01,\n",
       "                        5.2117e-01,  3.7809e-03,  3.1013e-02, -8.5956e-01,  1.7139e-01,\n",
       "                        6.2991e-01, -6.4022e-01,  4.1186e-01,  8.4497e-01, -7.2846e-01,\n",
       "                        1.6630e-01,  3.7084e-01,  1.5557e-01, -3.9537e-01,  7.0791e-01,\n",
       "                        1.2903e+00, -2.9371e-01,  1.1767e-02, -1.5667e-01,  1.5305e-01,\n",
       "                       -2.2510e-01, -5.6403e-02, -3.3220e-01,  8.7347e-01,  1.5676e-01,\n",
       "                        7.7551e-01, -9.0236e-02, -1.3427e-01,  3.2614e-01,  8.0459e-01,\n",
       "                        1.6333e-01, -7.1941e-02,  1.4779e-01, -1.3756e-01, -1.2494e-02,\n",
       "                        3.7020e-01, -2.6138e-01,  2.4304e-01,  3.6081e-01, -3.2825e-01,\n",
       "                       -1.2215e-02,  1.4969e-01,  5.2787e-01,  4.2558e-01,  1.6753e-01,\n",
       "                       -2.4447e-01,  5.0672e-01,  5.3980e-01, -3.4330e-01, -3.5826e-01,\n",
       "                       -5.5274e-01, -8.9964e-01, -2.4520e-01, -3.0840e-02, -9.9578e-02,\n",
       "                       -5.4980e-01, -1.8832e-01, -7.3100e-01, -2.1821e-01,  3.8603e-01,\n",
       "                        2.1851e-01, -1.7965e-01, -3.0052e-01,  1.1661e-01, -2.1752e-01,\n",
       "                        2.7059e-01,  5.8703e-01,  9.3615e-01, -2.0357e-01,  3.1416e-01,\n",
       "                       -7.5137e-02, -1.0528e-01, -2.3825e-01, -1.4738e-01,  4.1996e-01,\n",
       "                       -5.8701e-02, -4.0782e-01, -6.7580e-01, -4.1604e-01, -2.4352e-01,\n",
       "                        1.1542e+00,  7.2698e-01,  1.4512e-04,  2.2943e-01, -2.7380e-01,\n",
       "                       -6.8688e-01, -9.2839e-02,  1.2373e-01,  8.5691e-03,  1.4782e-02,\n",
       "                       -2.1036e-02,  5.9138e-03, -1.8286e-01,  6.8291e-01, -7.6442e-01,\n",
       "                       -1.6554e-01,  3.6340e-01,  1.1996e-01,  2.2379e-01, -8.0019e-01,\n",
       "                        1.5429e-01,  1.1637e-01,  4.4033e-01, -3.4712e-01,  2.5979e-01,\n",
       "                       -1.8930e-01, -3.3530e-01,  3.6483e-01])),\n",
       "              ('regressor.target_model.gt_block.0.A.weight',\n",
       "               tensor([[-0.2170, -0.2709,  0.1402,  ...,  0.1097, -0.0657,  0.1153],\n",
       "                       [-0.2257, -0.2743, -0.0005,  ...,  0.1484, -0.0456,  0.2050],\n",
       "                       [ 0.1272,  0.2333, -0.0742,  ..., -0.2057,  0.2703, -0.0028],\n",
       "                       ...,\n",
       "                       [-0.2139, -0.2901,  0.0903,  ...,  0.1766, -0.1263,  0.3758],\n",
       "                       [ 0.3730,  0.1269, -0.0997,  ..., -0.0479,  0.0654,  0.0485],\n",
       "                       [-0.0326, -0.1016, -0.0967,  ...,  0.0253,  0.0881,  0.0659]])),\n",
       "              ('regressor.target_model.gt_block.0.A.bias',\n",
       "               tensor([ 0.0193, -0.0032, -0.0772,  0.0629,  0.0480,  0.0567, -0.0144, -0.0548,\n",
       "                        0.0444, -0.0348, -0.0018, -0.0512,  0.0006, -0.0141, -0.0445,  0.0327,\n",
       "                       -0.0005, -0.0396, -0.0276, -0.0499, -0.0640,  0.0718,  0.0838, -0.0815,\n",
       "                        0.0036, -0.0629, -0.0040, -0.0073,  0.0203, -0.0762,  0.0667, -0.0139,\n",
       "                        0.0496,  0.0540, -0.0478,  0.0452, -0.0490,  0.0732,  0.0589, -0.0359,\n",
       "                       -0.0805, -0.0554, -0.0595,  0.0536,  0.0200,  0.0671, -0.0413,  0.0095,\n",
       "                        0.0789,  0.0795, -0.0540, -0.0485,  0.0693,  0.0431,  0.0112, -0.0434,\n",
       "                        0.0154, -0.0554,  0.0317,  0.0825, -0.0708, -0.0853, -0.0667, -0.0648,\n",
       "                       -0.0316,  0.0673,  0.0534,  0.0396,  0.0807,  0.0140, -0.0085, -0.0745,\n",
       "                       -0.0774, -0.0050,  0.0139, -0.0684,  0.0611, -0.0208, -0.0236, -0.0445,\n",
       "                        0.0716, -0.0372,  0.0049,  0.0854,  0.0853, -0.0149,  0.0129,  0.0949,\n",
       "                        0.0224,  0.0814,  0.0444,  0.0351,  0.0669, -0.0241, -0.0462, -0.0559,\n",
       "                        0.0342,  0.0772, -0.0308,  0.0446, -0.0816,  0.0486,  0.0095, -0.0478,\n",
       "                       -0.0328, -0.0710,  0.0682,  0.0716, -0.0809,  0.0388,  0.0192, -0.0812,\n",
       "                        0.0233, -0.0561, -0.0348, -0.0872,  0.0384, -0.0397, -0.0088,  0.0647,\n",
       "                        0.0650,  0.0415, -0.0187,  0.0732,  0.0766,  0.0534, -0.0031,  0.0078])),\n",
       "              ('regressor.target_model.gt_block.0.B.weight',\n",
       "               tensor([[-0.3897,  0.2564, -0.1452,  ..., -0.0036, -0.0946,  0.3958],\n",
       "                       [ 0.2292,  0.1181, -0.2836,  ..., -0.3149,  0.1612, -0.1347],\n",
       "                       [-0.3040,  0.3663, -0.0619,  ..., -0.2242, -0.2363, -0.0483],\n",
       "                       ...,\n",
       "                       [ 0.0400,  0.0452,  0.2120,  ..., -0.1310,  0.2720, -0.1840],\n",
       "                       [ 0.0447, -0.4822, -0.1934,  ...,  0.1051, -0.0145,  0.3237],\n",
       "                       [ 0.4777, -0.0620, -0.1635,  ..., -0.0170, -0.0308,  0.4484]])),\n",
       "              ('regressor.target_model.gt_block.0.B.bias',\n",
       "               tensor([-0.0936,  0.1204,  0.0410,  0.1544, -0.2009, -0.1515,  0.0299, -0.1618,\n",
       "                        0.1254, -0.1971, -0.0153,  0.3977, -0.0113, -0.1852,  0.3627,  0.1104,\n",
       "                       -0.0462, -0.1424, -0.3145, -0.2974, -0.1752,  0.0463,  0.1992, -0.1875,\n",
       "                       -0.2369, -0.3364, -0.2945, -0.1581,  0.2538, -0.1006, -0.1215,  0.1762,\n",
       "                        0.0295,  0.5752,  0.0146, -0.0136,  0.0321,  0.0092,  0.4069, -0.8359,\n",
       "                       -0.0368,  0.2152,  0.0514, -0.6178, -0.0291,  0.0051,  0.1493, -0.0287,\n",
       "                        0.2132,  0.3006, -0.2131,  0.0280, -0.3380,  0.1738,  0.2772,  0.0936,\n",
       "                       -0.1769,  0.0495,  0.0960, -0.3736,  0.3698, -0.1029,  0.3514, -0.0589,\n",
       "                       -0.0960, -0.0391, -0.0263,  0.1881, -0.0665, -0.2176, -0.2297,  0.8409,\n",
       "                        0.1619, -0.3418, -0.0392,  0.0499,  0.0248,  0.3899,  0.0491,  0.6694,\n",
       "                       -0.1112,  0.0371, -0.0898,  0.1070,  0.1049, -0.2948, -0.0205, -0.2600,\n",
       "                       -0.0027, -0.5356,  0.0374, -0.4485,  0.5694, -0.1948, -0.2436,  0.0829,\n",
       "                        0.2444,  0.1031, -0.0138, -0.2195, -0.2764, -0.2058,  0.2358, -0.5962,\n",
       "                       -0.4711,  0.0916,  0.3683,  0.1107,  0.4854,  0.4926, -0.3872,  0.0897,\n",
       "                        0.1389,  1.1526, -0.3181, -0.0842, -0.3924,  0.2183, -0.2049, -0.0847,\n",
       "                        0.1351, -0.0928,  0.0881, -0.6575, -0.0544, -0.3879, -0.4233, -0.1650])),\n",
       "              ('regressor.target_model.gt_block.0.C.weight',\n",
       "               tensor([[-0.0770,  0.0108,  0.2192,  ...,  0.0455, -0.0139,  0.1698],\n",
       "                       [ 0.0909,  0.0428, -0.1245,  ...,  0.1944, -0.0258,  0.3813],\n",
       "                       [ 0.1241, -0.0978,  0.3963,  ..., -0.2715,  0.1674, -0.2442],\n",
       "                       ...,\n",
       "                       [-0.2094, -0.0573,  0.1052,  ...,  0.0637,  0.0465,  0.2455],\n",
       "                       [-0.0534,  0.0020,  0.1949,  ..., -0.0963,  0.0189, -0.0479],\n",
       "                       [-0.1752,  0.0224, -0.1795,  ...,  0.4121,  0.0123,  0.2402]])),\n",
       "              ('regressor.target_model.gt_block.0.C.bias',\n",
       "               tensor([ 0.1056, -0.4692, -0.2723,  0.8236, -0.0501, -0.5587, -0.0190, -0.1835,\n",
       "                       -0.8600, -0.4664,  1.5462, -0.3093, -0.0537,  0.9548, -0.1611, -0.4022,\n",
       "                        0.5514, -0.0193,  0.0580,  0.2614, -0.3150, -0.1983, -0.5463, -0.8014,\n",
       "                        0.4014, -0.3393,  0.4698,  0.0184, -0.2988, -0.1682, -0.6097,  0.5976,\n",
       "                       -0.4176, -0.1920,  0.1687, -0.0383,  0.4339, -0.4834, -0.1768, -0.4955,\n",
       "                       -0.1188, -0.9714,  0.1007,  0.0031, -0.2357, -0.1204, -0.1431, -0.0537,\n",
       "                       -0.7731, -0.0079, -0.7117,  1.7219, -0.7603, -0.0515, -0.4466, -0.0114,\n",
       "                       -0.4802, -0.3051, -0.1698, -0.1357, -0.0826, -0.3817,  0.1473,  0.2178,\n",
       "                       -0.5706,  0.0780, -0.1632, -0.3678, -0.1474, -0.1259,  0.2330, -0.0512,\n",
       "                        0.0119, -0.7086,  0.2122,  0.1463,  0.8033,  0.1392, -0.1221, -0.3801,\n",
       "                       -0.3669,  0.1232, -0.1949, -0.0474, -0.1688,  0.0880, -0.1402, -0.1159,\n",
       "                        0.2155,  0.0964, -0.2196,  1.4347, -0.0125,  0.2376,  0.0641, -0.0440,\n",
       "                       -0.6091,  0.1534, -0.0842,  0.2850,  0.3725, -0.4342, -0.5905,  0.1053,\n",
       "                        0.0776,  0.4142, -0.4676,  0.3700,  0.1029, -0.3360,  0.3066, -0.0348,\n",
       "                        0.2750, -1.2590,  0.0876, -0.1234, -0.3648,  0.1164, -0.5058, -0.1692,\n",
       "                       -0.2280, -0.2339, -0.4023,  0.1471, -0.1909,  0.5545, -0.4925,  0.0787])),\n",
       "              ('regressor.target_model.gt_block.0.D.weight',\n",
       "               tensor([[-1.1236e-01, -7.6034e-02, -3.0832e-03,  ...,  1.3066e-01,\n",
       "                         1.2870e-01,  3.3301e-03],\n",
       "                       [ 1.1964e-02, -2.0553e-01,  1.2239e-01,  ..., -3.0819e-02,\n",
       "                         1.2785e-04, -2.2917e-01],\n",
       "                       [ 6.8347e-03, -1.5608e-01,  7.6234e-02,  ...,  1.0427e-01,\n",
       "                         2.1880e-01, -2.5527e-01],\n",
       "                       ...,\n",
       "                       [-1.7365e-01,  1.7797e-01, -9.0467e-03,  ..., -1.2464e-03,\n",
       "                        -1.5837e-01,  2.1561e-01],\n",
       "                       [ 1.4740e-02,  4.5758e-02,  3.9871e-01,  ..., -4.7295e-02,\n",
       "                         1.5177e-01, -6.6140e-02],\n",
       "                       [ 2.5305e-02,  2.9701e-01, -5.7723e-02,  ..., -6.3573e-02,\n",
       "                        -3.6183e-03,  6.1654e-02]])),\n",
       "              ('regressor.target_model.gt_block.0.D.bias',\n",
       "               tensor([ 1.7121e-01, -4.2527e-01, -2.1801e-01,  7.9074e-01,  1.2058e-01,\n",
       "                       -5.0658e-01, -7.6246e-02, -2.2166e-01, -7.6067e-01, -5.6524e-01,\n",
       "                        1.4343e+00, -4.7383e-01, -9.4996e-02,  9.4111e-01, -1.7312e-01,\n",
       "                       -3.2356e-01,  5.5900e-01, -5.7288e-02, -4.3273e-02,  2.8843e-01,\n",
       "                       -3.1166e-01, -2.4229e-01, -5.5824e-01, -7.1651e-01,  5.4859e-01,\n",
       "                       -3.1298e-01,  4.3650e-01,  1.3056e-01, -2.9731e-01, -6.6820e-02,\n",
       "                       -5.9933e-01,  6.5062e-01, -4.0707e-01, -2.6028e-01,  8.4533e-02,\n",
       "                        1.5213e-02,  2.7570e-01, -3.4170e-01, -1.5248e-01, -4.3976e-01,\n",
       "                       -5.9749e-02, -8.5306e-01,  6.4088e-02, -1.7571e-02, -1.8944e-01,\n",
       "                       -8.0598e-02, -1.8397e-01, -2.5532e-02, -7.7636e-01,  5.3153e-02,\n",
       "                       -6.9719e-01,  1.6629e+00, -8.3117e-01, -3.7283e-02, -3.1751e-01,\n",
       "                        8.9750e-03, -4.8111e-01, -3.0822e-01, -1.6281e-01, -1.9033e-01,\n",
       "                       -8.7404e-02, -2.7650e-01,  1.0826e-01,  3.2230e-01, -5.4577e-01,\n",
       "                        9.1866e-02, -8.3022e-02, -3.8807e-01, -1.9940e-01, -1.5445e-01,\n",
       "                        2.4898e-01,  2.1430e-02,  6.5783e-02, -8.4326e-01,  1.3942e-01,\n",
       "                        2.0205e-01,  7.1272e-01,  1.8024e-01, -4.3478e-02, -3.3202e-01,\n",
       "                       -3.7009e-01,  1.0707e-01, -1.3670e-01,  1.0180e-01, -1.0481e-01,\n",
       "                       -2.3696e-02, -1.9983e-01, -2.0547e-01,  3.3269e-01,  1.7177e-02,\n",
       "                       -2.4191e-01,  1.4572e+00,  5.2065e-02,  2.2658e-01,  7.8580e-02,\n",
       "                        4.9483e-03, -6.3521e-01,  1.2269e-01, -1.6922e-01,  3.3083e-01,\n",
       "                        3.5407e-01, -4.9480e-01, -6.5914e-01,  1.7870e-01,  1.1673e-01,\n",
       "                        4.6557e-01, -5.5587e-01,  3.7061e-01,  5.5118e-03, -3.4529e-01,\n",
       "                        2.4681e-01, -6.2158e-02,  2.0763e-01, -1.3318e+00, -1.0698e-03,\n",
       "                       -1.6044e-01, -3.7733e-01,  4.2705e-02, -4.8191e-01, -1.4330e-01,\n",
       "                       -2.2413e-01, -2.6638e-01, -3.0142e-01,  9.0064e-02, -2.0451e-01,\n",
       "                        6.2405e-01, -4.5915e-01, -6.3005e-02])),\n",
       "              ('regressor.target_model.gt_block.0.E.weight',\n",
       "               tensor([[ 0.0090, -0.0805,  0.1060,  ...,  0.0166,  0.1074,  0.2691],\n",
       "                       [-0.0031,  0.0066,  0.2140,  ..., -0.0695, -0.0105, -0.0711],\n",
       "                       [-0.0498,  0.0315,  0.0778,  ...,  0.0828, -0.1241,  0.1187],\n",
       "                       ...,\n",
       "                       [ 0.2294,  0.0425, -0.1454,  ...,  0.1327,  0.2617,  0.0515],\n",
       "                       [ 0.1094,  0.1235, -0.0467,  ...,  0.0284,  0.2080, -0.2116],\n",
       "                       [ 0.0450,  0.2198,  0.3079,  ...,  0.0174,  0.2013, -0.0702]])),\n",
       "              ('regressor.target_model.gt_block.0.E.bias',\n",
       "               tensor([ 0.1645, -0.3895, -0.2770,  0.7855,  0.0079, -0.4811, -0.0889, -0.1693,\n",
       "                       -0.7217, -0.4869,  1.5412, -0.3662,  0.0251,  0.8886, -0.1814, -0.3840,\n",
       "                        0.5483, -0.1214, -0.0353,  0.2385, -0.3950, -0.2795, -0.6226, -0.7114,\n",
       "                        0.5235, -0.3148,  0.4455,  0.0264, -0.2718, -0.0666, -0.6017,  0.5420,\n",
       "                       -0.4917, -0.3158,  0.1938, -0.0051,  0.4221, -0.4739, -0.1636, -0.5405,\n",
       "                       -0.0565, -0.9722,  0.1255, -0.0389, -0.2391, -0.2007, -0.1482,  0.0416,\n",
       "                       -0.8941,  0.0122, -0.7134,  1.6978, -0.8023, -0.1066, -0.4569, -0.0320,\n",
       "                       -0.4428, -0.2827, -0.2058, -0.1883,  0.0379, -0.3190,  0.0517,  0.1969,\n",
       "                       -0.5463,  0.1479, -0.0915, -0.4580, -0.2234, -0.0984,  0.2613,  0.0144,\n",
       "                        0.1190, -0.8036,  0.1401,  0.1156,  0.7954,  0.2137, -0.0953, -0.3209,\n",
       "                       -0.3733,  0.0820, -0.1457,  0.0485, -0.1419,  0.0276, -0.2396, -0.1981,\n",
       "                        0.3182,  0.0211, -0.1712,  1.4690,  0.1067,  0.1614,  0.0088,  0.0049,\n",
       "                       -0.5659,  0.1588, -0.0729,  0.2849,  0.3333, -0.4732, -0.5163,  0.1367,\n",
       "                        0.2371,  0.4760, -0.5397,  0.3295,  0.0031, -0.2644,  0.3338, -0.0352,\n",
       "                        0.1483, -1.3854,  0.0370, -0.2235, -0.3125,  0.1904, -0.5085, -0.1444,\n",
       "                       -0.3363, -0.1918, -0.3133,  0.1793, -0.1218,  0.6127, -0.4359, -0.0954])),\n",
       "              ('regressor.target_model.gt_block.0.bn_node_x.weight',\n",
       "               tensor([0.6555, 0.4950, 0.8133, 0.5895, 0.5266, 0.6981, 0.5289, 0.6383, 0.5803,\n",
       "                       0.6453, 0.9100, 0.5877, 0.5999, 0.3745, 0.6989, 0.5080, 0.4257, 0.4920,\n",
       "                       0.4500, 0.4881, 0.5094, 0.4449, 0.4463, 0.7140, 0.4945, 0.7171, 0.7797,\n",
       "                       0.5188, 0.5619, 0.6168, 0.5839, 0.4378, 0.5319, 0.3308, 0.5658, 0.6526,\n",
       "                       0.4214, 0.5798, 0.5399, 0.7192, 0.8060, 0.3891, 0.4850, 0.5371, 0.4564,\n",
       "                       0.6648, 0.5930, 0.5215, 0.6522, 0.4527, 0.7362, 0.6074, 0.7794, 0.5627,\n",
       "                       0.4499, 0.4104, 0.7128, 0.6405, 0.5864, 0.5579, 0.3928, 0.6262, 0.5454,\n",
       "                       0.5090, 0.6351, 0.9263, 0.4800, 0.5771, 0.5233, 0.5644, 0.5912, 0.4841,\n",
       "                       0.6368, 0.4980, 0.6005, 0.7736, 0.5109, 0.5874, 0.6388, 0.7549, 0.7798,\n",
       "                       0.4485, 0.8830, 0.7178, 0.5319, 0.5521, 0.6472, 0.5298, 0.5472, 0.5651,\n",
       "                       0.5183, 0.6431, 0.5889, 0.5857, 0.5141, 0.5370, 0.5660, 0.6280, 0.3622,\n",
       "                       0.6913, 0.5454, 0.7171, 0.4600, 0.6841, 0.5580, 0.7449, 0.5092, 0.4309,\n",
       "                       0.5912, 0.5639, 0.5153, 0.5837, 0.5416, 0.5111, 0.6390, 0.5425, 0.5365,\n",
       "                       0.4341, 0.4795, 0.7576, 0.6652, 0.5818, 0.6424, 0.4813, 0.5323, 0.7165,\n",
       "                       0.4479, 0.4184])),\n",
       "              ('regressor.target_model.gt_block.0.bn_node_x.bias',\n",
       "               tensor([-0.5101, -0.5108, -0.5287, -0.5542, -0.4667, -0.3752, -0.3872, -0.4410,\n",
       "                       -0.4928, -0.6930, -0.9519, -0.6508, -0.6378, -0.4029, -0.6711, -0.3400,\n",
       "                       -0.4543, -0.4469, -0.4776, -0.3245, -0.2879, -0.2484, -0.2355, -0.4796,\n",
       "                       -0.4478, -0.6549, -0.7880, -0.4899, -0.3456, -0.4710, -0.4545, -0.2737,\n",
       "                       -0.4991, -0.3344, -0.4799, -0.4521, -0.3458, -0.4853, -0.1579, -0.3558,\n",
       "                       -0.6448, -0.2500, -0.4860, -0.0784, -0.4387, -0.5369, -0.4265, -0.4159,\n",
       "                       -0.5503,  0.1753, -0.5035, -0.5637, -0.5868, -0.4849, -0.4066, -0.3497,\n",
       "                       -0.5337, -0.5810, -0.5964, -0.5588, -0.4124, -0.6027, -0.4130, -0.4705,\n",
       "                       -0.4816, -0.4577, -0.2247, -0.4539, -0.5396, -0.5984, -0.5420, -0.4820,\n",
       "                       -0.6695, -0.5049, -0.5492, -0.6660, -0.5159, -0.5334, -0.4667, -0.5349,\n",
       "                       -0.4016, -0.5212, -0.6312, -0.5229, -0.4264, -0.5157, -0.6120, -0.4055,\n",
       "                       -0.5757, -0.3582, -0.2269, -0.6676, -0.5111, -0.5750, -0.5413, -0.5262,\n",
       "                       -0.4818, -0.5053, -0.2612, -0.5506, -0.2842, -0.6008, -0.4449, -0.4757,\n",
       "                       -0.3458, -0.7185, -0.3402, -0.3838, -0.4152, -0.4335, -0.5265, -0.5421,\n",
       "                       -0.5744, -0.3721, -0.7235, -0.4834, -0.5783, -0.3873, -0.4094, -0.3742,\n",
       "                       -0.4778, -0.6089, -0.4977, -0.4719, -0.4368, -0.4657, -0.3770, -0.3971])),\n",
       "              ('regressor.target_model.gt_block.0.bn_node_x.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "              ('regressor.target_model.gt_block.0.bn_node_x.running_var',\n",
       "               tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1.])),\n",
       "              ('regressor.target_model.gt_block.0.bn_node_x.num_batches_tracked',\n",
       "               tensor(0)),\n",
       "              ('regressor.target_model.gt_block.0.bn_edge_e.weight',\n",
       "               tensor([0.8943, 0.9943, 1.2000, 0.8656, 1.1807, 0.6935, 0.8864, 1.1594, 0.7241,\n",
       "                       0.9680, 1.0285, 1.0982, 0.8236, 1.0783, 0.6584, 1.0669, 0.8970, 0.7205,\n",
       "                       1.1191, 0.9996, 0.9217, 0.7495, 0.8814, 1.1797, 1.0517, 0.8620, 0.8192,\n",
       "                       0.8816, 0.7434, 0.7541, 0.8955, 0.9326, 1.0584, 0.8917, 0.8593, 0.8335,\n",
       "                       0.9417, 1.1036, 0.7278, 0.7230, 1.3549, 1.1921, 1.2544, 1.0137, 0.8017,\n",
       "                       0.5948, 0.9153, 0.7149, 0.7738, 0.5296, 1.0690, 1.0732, 0.9846, 1.0501,\n",
       "                       0.8735, 1.9333, 0.9366, 0.5955, 0.9143, 0.9598, 0.9901, 0.8895, 0.7220,\n",
       "                       0.8622, 0.9938, 0.9066, 0.8423, 0.9205, 1.0346, 0.8650, 0.9802, 1.0291,\n",
       "                       0.7951, 1.0728, 0.9866, 0.6913, 0.9061, 1.3866, 0.8219, 0.9073, 0.7820,\n",
       "                       0.7022, 1.0145, 1.1447, 0.7380, 1.0823, 0.6089, 0.7841, 0.9995, 1.2090,\n",
       "                       0.7624, 1.0017, 0.8726, 0.8981, 1.0681, 0.7626, 0.7552, 1.4153, 0.9715,\n",
       "                       1.0749, 1.1719, 0.5059, 0.9859, 0.9218, 0.7542, 0.9317, 0.9465, 0.8419,\n",
       "                       0.6432, 0.9889, 0.9921, 1.1615, 0.5384, 0.5502, 0.9455, 0.6904, 0.9409,\n",
       "                       1.1552, 0.9024, 1.2703, 1.3896, 0.9937, 0.6293, 0.9307, 0.8341, 0.9026,\n",
       "                       0.8601, 0.9490])),\n",
       "              ('regressor.target_model.gt_block.0.bn_edge_e.bias',\n",
       "               tensor([-0.9934, -0.7855, -1.0851, -0.6249, -0.8947, -0.7831, -0.6948, -1.0286,\n",
       "                       -0.8192, -0.6800, -0.4117, -0.9102, -0.7274, -0.7611, -0.3029, -0.8429,\n",
       "                       -0.7556, -0.7189, -1.2341, -0.7063, -1.1576, -0.9127, -0.3672, -0.1640,\n",
       "                       -0.1442, -1.1044, -0.4357, -0.8588, -0.9090, -0.6069, -0.8446, -0.5726,\n",
       "                       -0.6480, -0.8859, -0.9741, -0.9815, -0.8180, -0.9348, -0.6905, -1.0462,\n",
       "                       -0.7062, -0.8613, -0.8840, -1.1527, -0.6997, -0.7722, -0.2411, -0.2732,\n",
       "                       -0.2044, -0.6201, -0.6398, -0.9520, -0.8878, -0.7968, -0.7237, -1.0651,\n",
       "                       -0.7469, -0.4167, -0.6233, -0.8036, -1.0435, -0.4764, -0.8749, -0.4756,\n",
       "                       -0.7620, -0.7052, -0.8601, -1.0209, -0.7741, -0.8117, -0.8124, -0.5155,\n",
       "                       -0.5146, -1.1116, -0.5823, -0.4517, -0.1921, -0.5917, -0.6810, -0.3182,\n",
       "                       -0.5829, -0.6207, -0.9128, -0.7888, -0.4339, -0.8405, -0.7919, -0.9620,\n",
       "                       -0.3317, -1.0309, -0.6943, -0.8481, -0.4938, -0.4655, -0.7751, -0.3036,\n",
       "                       -0.5760, -0.4194, -0.8881, -0.6350, -1.0411, -0.6413, -0.7738, -0.7189,\n",
       "                       -0.7684, -0.5936, -1.0033, -0.6027, -0.6365, -0.9042, -0.7683, -1.0075,\n",
       "                       -0.3590, -0.5939, -0.3886, -0.2492, -1.1174, -0.8837, -0.9972, -0.3158,\n",
       "                       -0.8645, -0.9295, -0.7506, -0.3011, -0.5099, -0.7766, -0.6374, -0.8475])),\n",
       "              ('regressor.target_model.gt_block.0.bn_edge_e.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "              ('regressor.target_model.gt_block.0.bn_edge_e.running_var',\n",
       "               tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1.])),\n",
       "              ('regressor.target_model.gt_block.0.bn_edge_e.num_batches_tracked',\n",
       "               tensor(0)),\n",
       "              ('regressor.target_model.gt_block.1.A.weight',\n",
       "               tensor([[-8.6382e-01,  4.3436e-02,  2.4173e-01,  ...,  5.2297e-02,\n",
       "                        -9.5177e-03, -1.5097e-01],\n",
       "                       [ 1.3548e-01, -3.3765e-01, -1.4213e-01,  ..., -5.9827e-02,\n",
       "                         1.2481e-01,  4.8983e-02],\n",
       "                       [ 2.2154e-03,  1.3322e-01, -8.0630e-01,  ...,  1.2292e-02,\n",
       "                        -4.4752e-03,  4.5360e-02],\n",
       "                       ...,\n",
       "                       [ 1.7662e-03,  2.2693e-04,  1.9073e-01,  ..., -9.1906e-01,\n",
       "                        -1.9164e-01,  8.2530e-02],\n",
       "                       [-1.0458e-01,  3.1989e-02, -6.1467e-02,  ...,  5.6520e-02,\n",
       "                        -1.2212e-01, -2.6586e-01],\n",
       "                       [-7.3638e-02, -1.9101e-01,  2.5806e-02,  ..., -3.0109e-02,\n",
       "                        -1.4777e-02,  1.2510e-01]])),\n",
       "              ('regressor.target_model.gt_block.1.A.bias',\n",
       "               tensor([-0.0576, -0.0011,  0.0130, -0.0836, -0.0026, -0.0439, -0.0602, -0.0305,\n",
       "                       -0.0596, -0.0226,  0.0796, -0.0469,  0.0046,  0.0907, -0.0442,  0.0556,\n",
       "                        0.0865, -0.0790, -0.0342,  0.0531,  0.0322, -0.0265,  0.0450, -0.0522,\n",
       "                        0.0055, -0.0841, -0.0274,  0.0457,  0.0559, -0.0182,  0.0588, -0.0639,\n",
       "                       -0.0874,  0.0447, -0.0532,  0.0158, -0.0465, -0.0280,  0.0611, -0.0660,\n",
       "                       -0.0859,  0.0743, -0.0485, -0.0340, -0.0263, -0.0841,  0.0422,  0.0644,\n",
       "                        0.0682,  0.0034,  0.0114,  0.0454,  0.0149,  0.0592,  0.0256, -0.0448,\n",
       "                        0.0284, -0.0784, -0.0191,  0.0130,  0.0224,  0.0375,  0.0302, -0.0477,\n",
       "                        0.0884, -0.0397, -0.0856, -0.0139, -0.0438,  0.0097, -0.0019, -0.0530,\n",
       "                       -0.0332, -0.0746,  0.0536,  0.0356,  0.0136, -0.0726,  0.0469, -0.0283,\n",
       "                       -0.0054, -0.0065,  0.0264, -0.0347, -0.0285, -0.0033,  0.0236, -0.0159,\n",
       "                        0.0935, -0.0499,  0.0327, -0.0397,  0.0474,  0.0134,  0.0763,  0.0328,\n",
       "                        0.0131, -0.0748,  0.0276,  0.0443, -0.0133,  0.0325, -0.0093, -0.0260,\n",
       "                        0.0715, -0.0373, -0.0639,  0.0255,  0.0383,  0.0025,  0.0293,  0.0766,\n",
       "                       -0.0748, -0.0090,  0.0354,  0.0107, -0.0046, -0.0746, -0.0307, -0.0259,\n",
       "                       -0.0411, -0.0492, -0.0462, -0.0358, -0.0096, -0.0526, -0.0002, -0.0285])),\n",
       "              ('regressor.target_model.gt_block.1.B.weight',\n",
       "               tensor([[-0.6452, -0.0215, -0.2532,  ..., -0.2998, -0.3134,  0.7003],\n",
       "                       [ 0.2894,  0.0553, -0.1141,  ..., -0.0591, -0.0275, -0.2432],\n",
       "                       [-0.2851,  0.3538, -0.2218,  ..., -0.0668, -0.0809,  0.1575],\n",
       "                       ...,\n",
       "                       [-0.1237,  0.1035,  0.0266,  ..., -0.0659, -0.0846, -0.0979],\n",
       "                       [-0.0687, -0.1390,  0.0077,  ..., -0.1390, -0.3464, -0.0431],\n",
       "                       [-0.1601, -0.1008,  0.2032,  ..., -0.0831, -0.0433,  0.0891]])),\n",
       "              ('regressor.target_model.gt_block.1.B.bias',\n",
       "               tensor([ 0.2389,  0.3239,  0.3186,  0.5932, -0.3255,  0.1585, -0.3178,  0.1747,\n",
       "                        0.0725,  0.0730,  0.3114,  1.0208,  0.2262,  0.6123,  0.6226,  0.1301,\n",
       "                        0.7763,  0.3638,  0.1675, -0.4654, -0.3057,  0.5921,  0.0096,  0.0114,\n",
       "                       -0.2696,  0.0324, -0.1842,  0.5472, -0.1911, -0.0393,  0.1781, -0.2890,\n",
       "                        0.3568,  0.1077,  1.2472,  1.0235, -0.0066,  0.5232,  0.9127,  0.1522,\n",
       "                        0.0780, -0.1564,  0.0471,  0.1806,  0.6276,  0.0101, -0.3556,  0.1670,\n",
       "                        0.0710,  0.2583,  0.2404,  0.0474,  1.2233,  0.6121, -0.0900,  0.8281,\n",
       "                       -0.0328,  0.1291, -0.2088,  0.0785,  0.2749,  0.0208, -0.0772,  0.1381,\n",
       "                       -0.0435, -0.1635, -0.2085,  0.2209, -0.1490, -0.4085,  0.1843, -0.6305,\n",
       "                        0.7258,  0.0272,  0.0423,  0.5031, -0.3344,  0.0537,  0.1165,  0.4868,\n",
       "                       -0.0775,  0.2373, -0.5093, -0.0263, -0.0590, -0.3629, -0.0174, -0.3864,\n",
       "                        0.2103, -0.0751,  0.5873,  0.0580,  0.7217,  0.1778, -0.0017,  0.6410,\n",
       "                        0.0524,  0.1443,  0.2337,  0.2340,  0.5442,  1.0840,  0.3434,  0.0920,\n",
       "                       -0.0203,  0.0829,  1.0224,  0.3474, -0.1602,  0.2738, -0.2491, -0.0982,\n",
       "                        0.3419, -0.4030, -0.1295,  0.1490,  0.3463,  0.2452,  0.1300,  0.0732,\n",
       "                        0.0657, -0.2624, -0.1242,  0.6798, -0.5169,  0.0800, -0.4893,  0.4017])),\n",
       "              ('regressor.target_model.gt_block.1.C.weight',\n",
       "               tensor([[-0.0279, -0.0483, -0.1487,  ...,  0.0210,  0.0336,  0.1200],\n",
       "                       [-0.3319, -0.1766,  0.0782,  ..., -0.4063, -0.1524, -0.2296],\n",
       "                       [ 0.0097, -0.4284, -0.9259,  ..., -0.2513,  0.1235, -0.6995],\n",
       "                       ...,\n",
       "                       [-0.2456, -0.1186, -0.0075,  ..., -0.6728, -0.1155,  0.1175],\n",
       "                       [-0.4986,  0.0261, -0.0659,  ...,  0.0171, -0.5842,  0.0135],\n",
       "                       [-0.1890, -0.0283, -0.1732,  ...,  0.0491, -0.1168,  0.5651]])),\n",
       "              ('regressor.target_model.gt_block.1.C.bias',\n",
       "               tensor([-2.2919e-01, -2.6205e-01, -1.6961e-01, -7.7862e-01, -3.9837e-02,\n",
       "                        1.2568e+00, -3.5007e-01, -1.9585e-01, -5.6760e-01, -1.0247e-01,\n",
       "                        1.8716e-01,  2.1733e-01,  1.2155e-02,  5.4940e-02, -1.6077e-01,\n",
       "                       -5.3546e-01, -1.6349e-01, -5.6741e-01, -3.5921e-01, -8.5228e-02,\n",
       "                        4.9704e-01, -5.3248e-02, -1.9477e-01, -7.8295e-01, -2.0814e-01,\n",
       "                       -9.4739e-02, -3.1967e-01, -1.1884e+00, -2.9294e-01,  6.6712e-01,\n",
       "                       -1.1013e-01, -3.1254e-01,  4.8403e-01, -7.5399e-01, -2.4484e-01,\n",
       "                       -2.9068e-01, -7.4113e-02, -3.4193e-01,  6.5457e-02, -1.8146e-01,\n",
       "                       -4.1004e-01, -2.1248e-01, -6.0918e-01,  1.8277e-01, -3.3734e-01,\n",
       "                        8.6957e-02, -6.7414e-01, -2.4908e-01, -1.7675e-01, -4.5895e-01,\n",
       "                       -4.4366e-02, -5.5557e-01, -2.6063e-01, -3.0745e-01,  2.1094e-01,\n",
       "                        1.2911e-01, -2.9198e-01,  2.2405e-01,  7.4775e-02, -7.4660e-01,\n",
       "                       -5.9833e-01, -1.2178e-01,  8.3559e-02, -5.5003e-01, -1.5059e-01,\n",
       "                       -6.6820e-01,  2.1313e-01, -1.1135e-01, -1.7503e-01,  4.7311e-02,\n",
       "                       -2.9217e-01,  1.4797e-01, -3.0867e-01,  7.3025e-02, -2.5577e-01,\n",
       "                       -2.2699e-01,  8.8931e-02,  3.5358e-03, -5.8402e-01, -6.9547e-02,\n",
       "                       -4.8004e-01,  1.5448e-02, -2.4978e-01, -9.4515e-01, -5.1602e-01,\n",
       "                       -2.6134e-01, -5.1350e-01,  1.3784e-01, -1.9174e-01, -2.0880e-01,\n",
       "                        9.0434e-02, -1.8691e-01, -9.4441e-02, -1.8740e-01,  3.7301e-01,\n",
       "                       -1.0828e-01, -7.0422e-02, -1.7190e-01,  1.2170e-01, -1.7880e-01,\n",
       "                       -1.1587e-01, -9.2729e-01, -4.3362e-01, -3.2134e-01, -1.2653e-01,\n",
       "                       -2.6553e-01, -1.7837e-01, -6.5801e-02,  3.0530e-01, -1.8367e-01,\n",
       "                       -2.3897e-02, -9.4472e-02, -7.0999e-01,  1.7904e-01, -7.0020e-01,\n",
       "                       -2.3066e-01, -5.5292e-02, -3.5512e-01,  1.4099e-01, -3.6101e-01,\n",
       "                       -1.1706e-03, -2.0695e-01, -1.6426e-02, -3.0853e-01, -4.3312e-02,\n",
       "                       -5.2857e-02, -3.6214e-01, -5.0327e-01])),\n",
       "              ('regressor.target_model.gt_block.1.D.weight',\n",
       "               tensor([[-0.4365, -0.0818,  0.0178,  ...,  0.0710,  0.0871, -0.1508],\n",
       "                       [-0.3257, -0.4240, -0.2937,  ..., -0.0442,  0.0253,  0.0201],\n",
       "                       [ 0.1099,  0.2285, -0.1168,  ..., -0.0343,  0.1865, -0.2055],\n",
       "                       ...,\n",
       "                       [-0.0918,  0.0367, -0.1560,  ..., -0.1476,  0.1371, -0.2147],\n",
       "                       [ 0.0132,  0.1840,  0.0970,  ...,  0.1153,  0.0134, -0.0057],\n",
       "                       [ 0.1227,  0.2175,  0.0117,  ...,  0.0616,  0.0234, -0.2192]])),\n",
       "              ('regressor.target_model.gt_block.1.D.bias',\n",
       "               tensor([-2.0276e-01, -1.9730e-01,  2.2495e-03, -7.3906e-01,  1.9425e-02,\n",
       "                        1.2073e+00, -3.5350e-01, -2.8618e-01, -5.6742e-01, -2.4029e-01,\n",
       "                        2.2939e-01,  9.1232e-02,  1.0843e-02,  5.3726e-02, -2.4779e-01,\n",
       "                       -5.2686e-01, -1.5277e-01, -5.4573e-01, -2.4062e-01,  5.6226e-02,\n",
       "                        5.6821e-01, -5.7600e-02, -1.9901e-01, -6.3417e-01, -2.8320e-01,\n",
       "                        4.4311e-05, -4.5072e-01, -1.2905e+00, -4.1462e-01,  6.5246e-01,\n",
       "                       -2.4825e-01, -1.6761e-01,  5.5570e-01, -7.7258e-01, -2.4466e-01,\n",
       "                       -2.7612e-01, -1.5179e-01, -3.3793e-01,  5.2905e-02, -2.8866e-01,\n",
       "                       -4.0483e-01, -1.3481e-01, -7.4953e-01,  1.9713e-02, -3.1861e-01,\n",
       "                        8.1965e-03, -5.4471e-01, -2.2316e-01, -2.2123e-01, -4.6979e-01,\n",
       "                       -1.2704e-01, -5.1063e-01, -1.6434e-01, -2.9572e-01,  1.0531e-01,\n",
       "                        1.3655e-01, -1.8030e-01,  2.0703e-01,  1.1645e-01, -7.7566e-01,\n",
       "                       -6.0714e-01, -2.6745e-01, -1.5122e-02, -4.9258e-01, -2.2292e-01,\n",
       "                       -5.8179e-01,  9.6481e-02, -7.0983e-02, -1.3106e-01,  1.0107e-01,\n",
       "                       -2.7504e-01,  1.6565e-01, -3.6001e-01,  1.2446e-03, -1.8486e-01,\n",
       "                       -1.4599e-01,  2.5207e-02, -7.7439e-02, -6.4496e-01, -3.3477e-02,\n",
       "                       -4.7857e-01, -3.6527e-02, -3.8926e-01, -7.8227e-01, -4.9133e-01,\n",
       "                       -3.1724e-01, -4.2379e-01,  9.8984e-02, -1.1295e-01, -1.4916e-01,\n",
       "                        6.3304e-02, -1.4629e-01,  1.8714e-02, -2.5102e-01,  3.4965e-01,\n",
       "                       -7.9334e-02, -1.0811e-01, -2.7355e-01,  1.4179e-01, -2.2423e-01,\n",
       "                       -1.5214e-01, -9.5144e-01, -3.0265e-01, -2.6253e-01, -1.7580e-01,\n",
       "                       -2.3751e-01, -8.3240e-02, -8.7117e-02,  4.3560e-01, -2.0932e-01,\n",
       "                        4.6661e-02, -1.2779e-01, -7.2259e-01,  8.9514e-02, -6.1660e-01,\n",
       "                       -1.7802e-01, -1.1473e-01, -3.2079e-01,  1.7883e-01, -4.7678e-01,\n",
       "                       -1.0738e-01, -2.2301e-01,  1.4063e-01, -2.8789e-01,  9.8328e-02,\n",
       "                       -9.6358e-02, -3.8982e-01, -4.4256e-01])),\n",
       "              ('regressor.target_model.gt_block.1.E.weight',\n",
       "               tensor([[-0.0665,  0.0740,  0.0310,  ..., -0.0697, -0.0414, -0.0115],\n",
       "                       [ 0.0204,  0.3367,  0.2038,  ...,  0.0550, -0.1986,  0.0716],\n",
       "                       [ 0.0935, -0.0989,  0.0052,  ...,  0.1296,  0.2142,  0.1434],\n",
       "                       ...,\n",
       "                       [-0.0591,  0.2045, -0.0038,  ..., -0.1309,  0.0247,  0.0332],\n",
       "                       [-0.2273,  0.2736, -0.0661,  ..., -0.1750, -0.2641, -0.0063],\n",
       "                       [ 0.2683,  0.0340, -0.0132,  ...,  0.0486, -0.1187, -0.2204]])),\n",
       "              ('regressor.target_model.gt_block.1.E.bias',\n",
       "               tensor([-0.2165, -0.2018, -0.1387, -0.7220, -0.0354,  1.1645, -0.3554, -0.2296,\n",
       "                       -0.6902, -0.1424,  0.2406,  0.2052,  0.0484, -0.0510, -0.1403, -0.4586,\n",
       "                       -0.0134, -0.4262, -0.3947, -0.0946,  0.6308,  0.0873, -0.1297, -0.7464,\n",
       "                       -0.2770,  0.0017, -0.4581, -1.1819, -0.3563,  0.5582, -0.2353, -0.2865,\n",
       "                        0.4468, -0.7006, -0.2736, -0.3505, -0.0953, -0.2504, -0.0043, -0.2777,\n",
       "                       -0.3112, -0.1535, -0.7633,  0.0361, -0.3431,  0.0794, -0.5529, -0.1965,\n",
       "                       -0.1440, -0.3428, -0.1184, -0.5217, -0.2407, -0.4203,  0.1734,  0.2269,\n",
       "                       -0.2229,  0.1856, -0.0193, -0.7807, -0.6431, -0.2344,  0.0105, -0.5089,\n",
       "                       -0.0866, -0.5349,  0.1633, -0.0744, -0.1146,  0.0877, -0.2408,  0.0239,\n",
       "                       -0.2723, -0.0630, -0.2677, -0.1728,  0.0684, -0.1130, -0.5859, -0.0276,\n",
       "                       -0.5476,  0.0391, -0.3238, -0.8018, -0.4886, -0.2522, -0.4035,  0.0814,\n",
       "                       -0.2567, -0.2576,  0.0466, -0.2143,  0.0239, -0.1132,  0.3229, -0.0712,\n",
       "                       -0.0130, -0.1096,  0.1201, -0.0691, -0.1348, -1.0234, -0.3184, -0.2551,\n",
       "                       -0.0850, -0.2275, -0.0955,  0.0210,  0.3559, -0.2524,  0.0491, -0.2294,\n",
       "                       -0.6258,  0.1389, -0.6993, -0.2868, -0.0059, -0.2010,  0.1443, -0.4469,\n",
       "                        0.0213, -0.1404,  0.1522, -0.3297,  0.0314, -0.0149, -0.3889, -0.3987])),\n",
       "              ('regressor.target_model.gt_block.1.bn_node_x.weight',\n",
       "               tensor([0.5370, 0.7198, 0.7141, 0.8582, 0.6271, 0.7838, 0.9130, 0.8182, 0.5584,\n",
       "                       0.6606, 0.6392, 1.1097, 0.5418, 0.5004, 0.6631, 0.7093, 0.8237, 0.5432,\n",
       "                       0.6063, 0.6179, 0.5772, 1.3030, 0.6184, 0.7090, 0.6132, 0.5304, 0.6904,\n",
       "                       0.5760, 0.5653, 0.7234, 0.6501, 0.5780, 0.5475, 0.8424, 0.6485, 0.6058,\n",
       "                       0.6432, 0.6009, 0.8034, 0.5710, 0.6799, 0.6349, 0.6136, 0.4689, 0.6766,\n",
       "                       0.6548, 0.7191, 0.5958, 0.5075, 0.5141, 0.7387, 0.6927, 0.7783, 0.8903,\n",
       "                       0.6671, 0.7002, 0.7347, 0.9023, 0.6705, 0.6991, 0.7317, 0.7102, 1.0408,\n",
       "                       0.7057, 0.5881, 0.8599, 0.6079, 0.5345, 0.5858, 0.6085, 0.4859, 1.1706,\n",
       "                       0.5489, 0.4844, 0.7875, 0.7348, 0.6876, 0.7052, 0.5863, 0.6373, 0.7029,\n",
       "                       0.6720, 0.7676, 0.6548, 0.6568, 0.6155, 0.6434, 0.5519, 0.7827, 0.6469,\n",
       "                       0.8216, 0.6689, 0.7095, 0.5835, 0.5576, 0.3992, 0.5505, 0.5083, 0.5966,\n",
       "                       0.7192, 0.6139, 0.5198, 0.6327, 0.7120, 0.5399, 0.7931, 0.6581, 0.5127,\n",
       "                       0.5428, 0.6148, 0.5825, 1.0039, 0.5832, 0.6334, 0.5540, 0.5912, 0.4761,\n",
       "                       0.6214, 0.6376, 0.6737, 0.7448, 0.6806, 0.6751, 0.7721, 0.5019, 0.6574,\n",
       "                       0.8364, 0.5763])),\n",
       "              ('regressor.target_model.gt_block.1.bn_node_x.bias',\n",
       "               tensor([-0.3816, -0.5039, -0.6292, -0.3977, -0.3230, -0.6304, -0.7511, -0.5255,\n",
       "                       -0.3349, -0.4359, -0.7995,  0.0241, -0.4967, -0.2258, -0.6294, -0.5428,\n",
       "                       -0.4040, -0.5274, -0.5263, -0.6049, -0.5678, -0.7886,  0.0020, -0.6377,\n",
       "                       -0.6691, -0.4102, -0.6274, -0.2931, -0.4164, -0.6726, -0.3032, -0.5326,\n",
       "                       -0.4699, -0.9709, -0.3917, -0.1200, -0.1477, -0.3258, -0.3207, -0.2895,\n",
       "                       -0.6310, -0.5667, -0.4914, -0.0409, -0.3523, -0.6054, -0.5687, -0.6329,\n",
       "                       -0.3555, -0.0661, -0.4080, -0.7489, -0.4579, -0.3285, -0.5301, -0.5361,\n",
       "                       -0.6660, -0.7646, -0.8373, -0.2787, -0.6184, -0.3845, -0.6008, -0.5569,\n",
       "                       -0.3770, -0.7455, -0.4345, -0.4967, -0.6148, -0.6803, -0.4212, -0.7643,\n",
       "                       -0.3605, -0.2137, -0.6891, -0.6985, -0.9510, -0.7651, -0.4989, -0.5918,\n",
       "                       -0.6037, -0.1355, -0.6838, -0.5972, -0.5880, -0.6478, -0.5969, -0.3628,\n",
       "                       -0.5535, -0.4998, -0.4798, -0.5690, -0.1573, -0.5460, -0.5872, -0.0310,\n",
       "                       -0.4834, -0.3776, -0.3757, -0.1478, -0.4486, -0.1906, -0.4626, -0.6484,\n",
       "                       -0.3636, -0.8736, -0.0494, -0.5870, -0.5306, -0.5493, -0.6760, -0.5858,\n",
       "                       -0.5078, -0.2081, -0.6309, -0.5427, -0.1587,  0.0242, -0.1573, -0.5605,\n",
       "                       -0.5432, -0.6030, -0.5762, -0.6074, -0.1776, -0.5997, -1.2652, -0.6869])),\n",
       "              ('regressor.target_model.gt_block.1.bn_node_x.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "              ('regressor.target_model.gt_block.1.bn_node_x.running_var',\n",
       "               tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1.])),\n",
       "              ('regressor.target_model.gt_block.1.bn_node_x.num_batches_tracked',\n",
       "               tensor(0)),\n",
       "              ('regressor.target_model.gt_block.1.bn_edge_e.weight',\n",
       "               tensor([1.1458, 0.4905, 1.1131, 0.7361, 0.2250, 1.2226, 1.0971, 0.9238, 1.0564,\n",
       "                       0.9881, 1.1414, 0.7921, 0.7628, 0.5757, 0.6086, 0.8665, 0.5335, 0.9686,\n",
       "                       0.9907, 1.0273, 0.9292, 0.9958, 0.9861, 1.3143, 1.2334, 0.9972, 0.9342,\n",
       "                       0.7423, 0.8750, 1.0202, 0.8349, 1.1280, 1.1418, 1.0160, 0.7014, 0.8474,\n",
       "                       0.6579, 1.0488, 1.2053, 0.7122, 1.2553, 1.2479, 1.1019, 0.9439, 0.6091,\n",
       "                       0.9229, 1.2073, 0.8266, 0.7707, 1.0736, 0.8731, 1.1359, 0.7489, 0.4698,\n",
       "                       1.0062, 1.6677, 1.0819, 0.8194, 0.9281, 0.7462, 0.9983, 0.9714, 1.0684,\n",
       "                       0.9722, 0.5887, 1.0685, 0.6092, 1.0469, 0.6900, 0.8563, 1.0513, 0.9664,\n",
       "                       1.0122, 0.3544, 1.1193, 0.8911, 1.1794, 1.2568, 1.0115, 0.9986, 0.7435,\n",
       "                       0.4192, 0.7582, 1.0541, 1.0622, 0.9285, 0.8489, 1.0021, 1.2139, 1.1135,\n",
       "                       0.8039, 0.5667, 0.4653, 0.8170, 1.2059, 0.8615, 0.9053, 1.2007, 0.7515,\n",
       "                       0.4944, 1.0352, 0.9114, 1.0850, 0.7296, 0.9977, 0.9480, 0.7821, 0.6107,\n",
       "                       1.0608, 0.8299, 1.0872, 0.8727, 0.9890, 0.3982, 0.8849, 0.8521, 0.8368,\n",
       "                       1.0293, 0.4682, 1.1753, 1.1640, 1.2297, 1.0903, 1.1580, 0.4457, 1.0860,\n",
       "                       1.0254, 0.9507])),\n",
       "              ('regressor.target_model.gt_block.1.bn_edge_e.bias',\n",
       "               tensor([-1.2043, -0.6401, -0.9349, -0.8037, -0.2490, -0.8494, -0.8583, -0.8758,\n",
       "                       -0.9604, -0.6483, -0.6206, -0.9614, -0.7774, -0.6732, -0.5404, -0.6851,\n",
       "                       -0.6078, -0.9193, -0.8168, -0.8837, -0.9208, -0.8434, -0.6757, -0.8518,\n",
       "                       -0.7380, -0.9134, -0.9444, -0.6769, -0.8931, -0.9377, -0.7859, -1.0021,\n",
       "                       -0.9146, -0.8660, -0.7209, -0.6935, -0.5376, -0.9315, -1.0877, -0.6540,\n",
       "                       -0.8818, -1.1949, -0.7647, -0.8876, -0.6041, -0.9377, -0.7274, -0.9637,\n",
       "                       -0.6895, -0.8503, -0.4862, -0.9647, -0.7889, -0.5170, -0.9403, -1.3844,\n",
       "                       -0.9144, -0.8915, -0.7354, -0.6620, -1.0023, -0.8951, -1.0435, -0.5977,\n",
       "                       -0.6483, -0.8854, -0.6487, -0.8501, -0.7854, -0.8055, -0.8751, -0.6078,\n",
       "                       -0.8550, -0.3821, -0.8163, -0.8193, -0.9025, -0.9937, -0.8591, -0.6194,\n",
       "                       -0.9044, -0.7251, -0.6498, -0.8153, -1.0812, -0.9523, -0.7196, -0.9259,\n",
       "                       -0.6850, -1.1323, -0.6609, -0.4690, -0.6356, -0.6450, -0.9310, -0.7378,\n",
       "                       -0.7669, -0.7689, -0.6466, -0.6414, -1.0237, -0.7759, -0.7820, -0.8907,\n",
       "                       -1.2304, -0.5760, -0.8249, -0.7351, -1.1213, -0.9080, -0.8644, -0.8403,\n",
       "                       -0.5929, -0.3738, -0.6157, -0.6059, -0.7364, -0.7980, -0.4230, -0.7113,\n",
       "                       -0.9379, -1.0253, -0.9775, -0.9361, -0.4903, -0.9120, -0.7748, -0.9735])),\n",
       "              ('regressor.target_model.gt_block.1.bn_edge_e.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "              ('regressor.target_model.gt_block.1.bn_edge_e.running_var',\n",
       "               tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1.])),\n",
       "              ('regressor.target_model.gt_block.1.bn_edge_e.num_batches_tracked',\n",
       "               tensor(0)),\n",
       "              ('regressor.target_model.gt_block.2.A.weight',\n",
       "               tensor([[-0.6875, -0.1945, -0.1528,  ..., -0.1110, -0.0295, -0.1777],\n",
       "                       [ 0.0086, -0.3255,  0.0296,  ..., -0.0429, -0.2114, -0.2036],\n",
       "                       [-0.1663,  0.0210, -0.5266,  ..., -0.0775, -0.0446,  0.0301],\n",
       "                       ...,\n",
       "                       [-0.0792,  0.0643,  0.0819,  ..., -0.5200,  0.0470,  0.2897],\n",
       "                       [-0.0251, -0.0496, -0.0406,  ...,  0.0487, -0.3642,  0.3419],\n",
       "                       [-0.0215, -0.1109,  0.0442,  ..., -0.0102,  0.1523, -0.0556]])),\n",
       "              ('regressor.target_model.gt_block.2.A.bias',\n",
       "               tensor([-0.0014,  0.0097,  0.0289,  0.0630,  0.0350, -0.0572,  0.0557, -0.0788,\n",
       "                       -0.0089, -0.0796,  0.0819,  0.0435,  0.0519,  0.0676,  0.0875,  0.0659,\n",
       "                       -0.0011,  0.0060, -0.0668,  0.0814, -0.0280, -0.0764, -0.0427, -0.0251,\n",
       "                       -0.0076,  0.0390,  0.0430, -0.0325, -0.0407, -0.0782, -0.0457,  0.0362,\n",
       "                        0.0174,  0.0061,  0.0578,  0.0009,  0.0314,  0.0665,  0.0498,  0.0450,\n",
       "                        0.0830, -0.0390,  0.0448,  0.0246, -0.0550, -0.0001,  0.0425, -0.0493,\n",
       "                        0.0517, -0.0865, -0.0152, -0.0130, -0.0105, -0.0274, -0.0351, -0.0490,\n",
       "                        0.0727,  0.0208,  0.0094, -0.1196, -0.0176,  0.0669, -0.0052, -0.0912,\n",
       "                       -0.0345, -0.0345,  0.0643,  0.0487,  0.0166, -0.0736, -0.0840, -0.0478,\n",
       "                       -0.0821, -0.0554, -0.0184,  0.0126, -0.0015,  0.0609, -0.0229,  0.0133,\n",
       "                       -0.0005, -0.0457, -0.0237,  0.0605, -0.0385, -0.0067,  0.0773,  0.0515,\n",
       "                       -0.0062,  0.0014, -0.0715,  0.0277, -0.0676,  0.0486,  0.0437, -0.0727,\n",
       "                        0.0674,  0.0686,  0.0054, -0.0592,  0.0385,  0.0516,  0.0145,  0.0667,\n",
       "                        0.0029, -0.0519,  0.0447,  0.0139, -0.0566, -0.0547,  0.0550,  0.0528,\n",
       "                        0.0290, -0.0585, -0.0658,  0.0630,  0.0707, -0.0448, -0.0234,  0.0337,\n",
       "                        0.0652, -0.0456, -0.0694,  0.0554,  0.0346, -0.0209,  0.0765,  0.0398])),\n",
       "              ('regressor.target_model.gt_block.2.B.weight',\n",
       "               tensor([[ 0.2244, -0.0850, -0.2338,  ...,  0.3238, -0.3888, -0.1313],\n",
       "                       [ 0.1341, -0.1580, -0.3684,  ..., -0.1883,  0.2950,  0.5788],\n",
       "                       [ 0.0787,  0.0671,  0.0213,  ...,  0.0212, -0.0849,  0.1683],\n",
       "                       ...,\n",
       "                       [-0.3094, -0.1484,  0.2008,  ..., -0.5319, -0.0592, -0.0252],\n",
       "                       [ 0.1764, -0.0438,  0.0368,  ..., -0.0234,  0.0951, -0.0868],\n",
       "                       [ 0.3014, -0.5756,  0.0335,  ..., -0.1638, -0.1234, -0.3835]])),\n",
       "              ('regressor.target_model.gt_block.2.B.bias',\n",
       "               tensor([ 0.5285,  0.5100,  0.3649, -0.0927, -0.7074, -0.2508,  0.3281,  0.1369,\n",
       "                        0.3521,  0.2680, -0.4942,  0.7574, -0.2706,  0.0127,  0.0792, -0.0402,\n",
       "                        0.5181, -0.2341,  0.5692, -0.3966, -0.1303, -0.2386,  2.1806,  0.1795,\n",
       "                       -0.0511, -0.1133,  0.0068, -0.2397,  1.7902,  0.3278, -0.5053,  0.3779,\n",
       "                        0.1426,  0.6808,  1.2975,  0.9764, -0.5039,  0.2024,  0.2969, -0.0497,\n",
       "                       -0.1919,  0.3824,  0.0445,  1.2479,  0.6137, -0.1350, -0.1256,  0.2975,\n",
       "                       -0.5146,  0.3703, -0.0821,  0.4409,  1.3162,  1.1409, -0.3843,  0.4842,\n",
       "                        0.1498, -0.0214, -0.3126,  0.4110,  0.1106,  0.0464, -0.1489,  0.2255,\n",
       "                       -0.1114, -0.2499,  0.0856,  0.5651,  0.6644,  0.2358, -0.1963, -0.7007,\n",
       "                       -0.4533,  0.0854, -0.1742,  0.5938, -0.2030, -0.1554,  0.4223, -0.2765,\n",
       "                        0.0515,  0.2480,  0.1057, -0.0852,  0.4134,  0.0113,  0.2410,  0.8575,\n",
       "                        0.2943, -1.0101, -0.7068,  0.2928,  0.7915,  0.1406, -1.1772,  0.2318,\n",
       "                       -0.3908,  0.1653, -0.0375,  0.1896, -0.0985,  0.1867,  0.7297,  0.4075,\n",
       "                        0.4331, -0.1080,  0.4056,  0.0422, -0.0933, -0.6430,  0.4004,  0.3331,\n",
       "                        0.2943,  0.3560,  0.0401,  0.0955,  0.8009,  0.6592, -0.0445, -0.1496,\n",
       "                       -0.4664,  0.6811,  0.0081,  0.5602,  0.4615,  0.0066, -0.1205, -0.4303])),\n",
       "              ('regressor.target_model.gt_block.2.C.weight',\n",
       "               tensor([[ 0.2841,  0.0415, -0.0661,  ...,  0.2720, -0.0538,  0.2495],\n",
       "                       [ 0.3638, -0.2049,  0.4204,  ...,  0.2682, -0.1858,  0.0474],\n",
       "                       [ 0.2256,  0.1337, -0.0280,  ..., -0.0432,  0.1784, -0.2904],\n",
       "                       ...,\n",
       "                       [-0.1085, -0.1283, -0.2600,  ..., -0.0783, -0.0013, -0.0466],\n",
       "                       [ 0.0394, -0.0557, -0.1994,  ...,  0.0897, -0.3085,  0.2347],\n",
       "                       [ 0.3333,  0.0285,  0.2347,  ...,  0.4824,  0.0288,  0.2338]])),\n",
       "              ('regressor.target_model.gt_block.2.C.bias',\n",
       "               tensor([-0.0764, -0.0459, -0.2987, -0.0699,  0.0972,  0.0075, -0.0168,  0.0636,\n",
       "                       -0.1551, -0.1181, -0.2638,  0.0941, -0.4925, -0.1680, -0.1651, -0.1477,\n",
       "                       -0.4797, -0.3578, -0.1809,  0.0044, -0.1266, -0.0741, -0.0417, -0.1957,\n",
       "                       -0.1054, -0.1901, -0.1953,  0.2281, -0.3736, -0.2220, -0.0076, -0.3580,\n",
       "                        0.0388, -0.4014,  0.0330,  0.1102,  0.1292, -0.0528,  0.1645,  0.0338,\n",
       "                       -0.3929, -0.1782, -0.0917, -0.0373, -0.0694, -0.0420, -0.3823, -0.4029,\n",
       "                        0.1200, -0.0685,  0.0926, -0.1379, -0.0850, -0.1152,  0.1739,  0.0603,\n",
       "                       -0.0865, -0.1161, -0.5036, -2.4863, -0.2833,  0.1184,  0.0644, -0.1831,\n",
       "                       -0.1383, -0.1683,  0.1213, -0.0194, -0.3408, -0.0057, -0.3232,  0.0036,\n",
       "                        0.0743,  0.1831, -0.0801,  0.4126,  0.0335, -0.0645, -0.1186,  0.0219,\n",
       "                       -0.4471, -0.0244,  0.1108, -0.1643,  0.1031, -0.1559, -0.2305, -0.0426,\n",
       "                       -0.4774,  0.2050, -0.3635, -0.2004,  0.1176, -0.2841, -0.4485,  1.4403,\n",
       "                        0.2595, -0.0030, -0.1672, -0.1094,  0.1750, -0.0780,  0.0797,  0.0750,\n",
       "                       -0.0921, -0.4800, -0.0875, -0.2607, -0.3075,  0.2079,  0.0135,  0.0422,\n",
       "                       -0.2203, -0.0951, -0.3193, -0.1418, -0.0853,  0.0541,  0.1228, -0.0161,\n",
       "                       -0.0487, -0.0129, -0.6507, -0.5352, -0.1460,  0.1441, -0.4933,  0.0321])),\n",
       "              ('regressor.target_model.gt_block.2.D.weight',\n",
       "               tensor([[ 0.1795, -0.2053,  0.2107,  ...,  0.2235, -0.0569, -0.0241],\n",
       "                       [ 0.1653, -0.0581, -0.1467,  ..., -0.2211, -0.2796, -0.1707],\n",
       "                       [ 0.0213,  0.1053, -0.1511,  ..., -0.0325, -0.1754, -0.0984],\n",
       "                       ...,\n",
       "                       [-0.0663,  0.1181, -0.2586,  ..., -0.3575, -0.5916, -0.0539],\n",
       "                       [ 0.1118,  0.0838,  0.0736,  ..., -0.0913, -0.4417,  0.1872],\n",
       "                       [ 0.0517, -0.1427, -0.2918,  ..., -0.0590,  0.0597,  0.1844]])),\n",
       "              ('regressor.target_model.gt_block.2.D.bias',\n",
       "               tensor([-1.4162e-01, -8.4447e-02, -2.6238e-01,  8.0957e-02,  5.4170e-02,\n",
       "                        4.1568e-02,  4.6446e-02,  1.1451e-01, -1.4071e-01, -3.8926e-02,\n",
       "                       -2.6890e-01, -3.4468e-02, -4.1476e-01, -1.2161e-01, -7.3773e-02,\n",
       "                       -6.8486e-02, -5.1887e-01, -3.7523e-01, -1.3245e-01, -6.9604e-02,\n",
       "                       -2.5447e-02,  5.1987e-02, -1.6883e-02, -3.2266e-01, -5.0229e-02,\n",
       "                       -9.1866e-02, -3.0703e-01,  3.2782e-01, -2.1086e-01, -1.3871e-01,\n",
       "                        9.3847e-02, -3.0935e-01, -6.2349e-02, -4.9860e-01,  9.0578e-02,\n",
       "                       -3.6930e-02,  8.1963e-02,  7.6446e-02,  1.5110e-01, -9.2127e-03,\n",
       "                       -3.6479e-01, -1.2042e-01,  1.5328e-02, -5.6887e-02, -5.9104e-02,\n",
       "                        1.7517e-02, -3.6709e-01, -2.7733e-01,  1.9885e-02, -3.4573e-04,\n",
       "                        5.2199e-02, -2.5639e-01, -2.1598e-01, -5.7960e-02,  6.9236e-02,\n",
       "                        5.3130e-02, -1.6295e-01, -6.1903e-02, -5.5856e-01, -2.4694e+00,\n",
       "                       -3.0952e-01, -1.0587e-02,  1.2497e-01, -1.5517e-01, -2.4754e-01,\n",
       "                       -2.1977e-01,  1.4175e-01,  1.4216e-01, -2.9364e-01,  3.0661e-02,\n",
       "                       -3.2178e-01, -2.8531e-02, -8.5855e-02,  1.7177e-01, -6.8142e-02,\n",
       "                        3.0957e-01,  2.6617e-03,  4.6348e-03, -2.6314e-04,  9.1425e-02,\n",
       "                       -4.1857e-01, -1.3072e-02,  1.1880e-01, -1.3550e-01,  1.1575e-01,\n",
       "                       -1.7892e-01, -3.6323e-01, -4.8580e-02, -3.4210e-01,  6.4221e-02,\n",
       "                       -3.8834e-01, -1.7169e-01,  1.5548e-01, -3.3749e-01, -3.8788e-01,\n",
       "                        1.3864e+00,  2.7817e-01,  3.9792e-02, -1.0887e-01, -1.3570e-01,\n",
       "                        2.6801e-01, -1.4713e-01, -2.7329e-03,  1.3352e-01, -1.8410e-02,\n",
       "                       -5.0892e-01,  5.7499e-02, -2.2603e-01, -2.9961e-01,  5.4910e-02,\n",
       "                        1.2392e-02,  2.3445e-02, -2.3879e-01, -8.3598e-02, -4.8323e-01,\n",
       "                       -3.4903e-02, -1.7990e-01,  1.7745e-01,  9.4706e-02,  8.0500e-02,\n",
       "                       -5.8583e-02,  2.2820e-02, -5.2879e-01, -6.6678e-01, -1.5475e-01,\n",
       "                        1.5967e-01, -4.8663e-01, -1.0279e-01])),\n",
       "              ('regressor.target_model.gt_block.2.E.weight',\n",
       "               tensor([[-0.0245, -0.3190, -0.2008,  ..., -0.1606, -0.0097,  0.2752],\n",
       "                       [ 0.0542,  0.2465, -0.0962,  ..., -0.4943, -0.3088, -0.2828],\n",
       "                       [ 0.0107, -0.0171, -0.0987,  ...,  0.0533,  0.0924,  0.1774],\n",
       "                       ...,\n",
       "                       [ 0.2523, -0.1643,  0.2039,  ..., -0.0479, -0.1344,  0.3056],\n",
       "                       [ 0.2342,  0.0550, -0.2474,  ..., -0.0074, -0.1925,  0.6012],\n",
       "                       [ 0.1986, -0.1098,  0.0435,  ..., -0.1881, -0.0868,  0.1384]])),\n",
       "              ('regressor.target_model.gt_block.2.E.bias',\n",
       "               tensor([-0.1668, -0.1544, -0.3464, -0.0391,  0.0857, -0.0115,  0.0860,  0.1304,\n",
       "                       -0.0137,  0.0093, -0.2657,  0.0082, -0.4073, -0.1355, -0.1410, -0.1090,\n",
       "                       -0.4642, -0.3039, -0.0332, -0.0286, -0.1585,  0.0235, -0.0407, -0.2652,\n",
       "                       -0.1552, -0.1295, -0.1938,  0.2878, -0.2965, -0.2299,  0.0059, -0.4256,\n",
       "                       -0.0371, -0.4164,  0.0266,  0.0520,  0.0507, -0.0354,  0.0521,  0.1453,\n",
       "                       -0.3642, -0.0405, -0.1002,  0.0657, -0.1174,  0.0061, -0.3401, -0.3502,\n",
       "                        0.1394, -0.0110, -0.0330, -0.3057, -0.1980, -0.1082,  0.0846,  0.0258,\n",
       "                       -0.0411, -0.1713, -0.5723, -2.4758, -0.2670,  0.0340,  0.1138, -0.1369,\n",
       "                       -0.2114, -0.2653,  0.2292,  0.0199, -0.3383, -0.0258, -0.2962, -0.0032,\n",
       "                        0.0111,  0.0326, -0.0177,  0.3364,  0.0137, -0.0400, -0.0550,  0.0635,\n",
       "                       -0.3006, -0.0560,  0.1140, -0.0983,  0.0324, -0.1766, -0.3226, -0.1901,\n",
       "                       -0.3949,  0.1177, -0.2603, -0.0829,  0.0923, -0.1868, -0.4456,  1.4056,\n",
       "                        0.3672,  0.0612, -0.0770, -0.0605,  0.2650, -0.0603, -0.0521,  0.1189,\n",
       "                       -0.0639, -0.5175, -0.0799, -0.2065, -0.3116,  0.0959,  0.0372,  0.0528,\n",
       "                       -0.3362,  0.0355, -0.4075, -0.0402, -0.2274,  0.1676, -0.0280,  0.0097,\n",
       "                       -0.0660,  0.0904, -0.5943, -0.6609, -0.1612,  0.1214, -0.4741,  0.0278])),\n",
       "              ('regressor.target_model.gt_block.2.bn_node_x.weight',\n",
       "               tensor([0.6161, 0.6256, 0.6731, 0.4931, 0.7768, 0.7519, 0.6846, 0.6603, 0.7024,\n",
       "                       0.6291, 0.7276, 0.6724, 0.6030, 0.6875, 0.6361, 0.7952, 0.6259, 0.6143,\n",
       "                       0.7621, 0.6267, 0.6899, 1.5224, 0.5222, 0.7210, 0.9239, 0.6399, 0.7301,\n",
       "                       0.9278, 0.6857, 0.6169, 0.6181, 0.6152, 0.7357, 0.5320, 0.6752, 0.6877,\n",
       "                       0.8799, 0.5146, 0.7792, 0.5555, 0.5946, 0.5581, 0.6322, 0.5167, 0.6231,\n",
       "                       0.6469, 0.7841, 0.6499, 0.6915, 0.6230, 0.8639, 0.6446, 0.7865, 0.8290,\n",
       "                       0.7665, 0.6204, 0.5640, 0.8157, 0.5452, 1.5271, 0.8029, 0.8043, 1.0298,\n",
       "                       0.7112, 0.5883, 0.7370, 0.7098, 0.7600, 0.7549, 0.6900, 0.6254, 0.9199,\n",
       "                       0.5235, 0.6404, 0.7723, 0.5260, 0.6417, 0.7193, 0.6107, 0.6446, 0.6630,\n",
       "                       0.6964, 0.7674, 0.5721, 0.5778, 0.6602, 0.6367, 0.8756, 0.8033, 0.9441,\n",
       "                       0.7089, 0.7034, 1.0138, 0.6304, 0.5316, 0.5977, 0.6452, 0.4738, 0.6947,\n",
       "                       0.6554, 0.6592, 0.5506, 0.8079, 0.6708, 0.5988, 0.5842, 0.8144, 0.5569,\n",
       "                       0.5127, 0.6563, 0.6115, 1.0607, 0.5124, 0.7266, 0.5605, 0.5685, 0.9076,\n",
       "                       0.5279, 0.7517, 0.6228, 0.9521, 0.8400, 0.6115, 0.6113, 0.5717, 0.5948,\n",
       "                       0.7267, 0.7204])),\n",
       "              ('regressor.target_model.gt_block.2.bn_node_x.bias',\n",
       "               tensor([-0.3235, -0.4936, -0.5682, -0.2537, -0.4063, -0.5932, -0.3661, -0.4250,\n",
       "                       -0.3561, -0.1073, -0.6941, -0.0951, -0.4986, -0.1155, -0.7206, -0.5802,\n",
       "                       -0.4670, -0.4315, -0.3550, -0.6363, -0.1971, -0.9383,  0.0056, -0.5758,\n",
       "                       -0.8329, -0.5917, -0.8770, -0.8729, -0.3678, -0.6584, -0.2112, -0.3092,\n",
       "                       -0.5291, -0.2865, -0.0132,  0.0656, -0.6749,  0.0620, -0.3823, -0.1430,\n",
       "                       -0.6043,  0.0397, -0.5131, -0.1836, -0.2200, -0.6963, -0.7685, -0.6947,\n",
       "                       -0.5142, -0.2358, -0.4931, -0.6093, -0.4215, -0.0555, -0.5502, -0.4761,\n",
       "                       -0.3325, -0.6575, -0.6287, -0.7378, -0.1873, -0.3835, -0.6838, -0.7248,\n",
       "                       -0.5513, -0.8376, -0.3119, -0.8357, -0.4761, -0.3928, -0.6981, -0.6407,\n",
       "                       -0.1410, -0.5010, -0.5614, -0.5131, -0.4789, -0.7378, -0.5220, -0.7734,\n",
       "                       -0.6839, -0.4695, -0.7643, -0.4168, -0.0652, -0.7263, -0.6832, -0.5776,\n",
       "                       -0.6115, -0.8343, -0.4232, -0.7387, -0.0983, -0.3715, -0.3658, -0.6087,\n",
       "                       -0.5936, -0.3124, -0.4419, -0.1598, -0.1881, -0.2962, -0.3737, -0.6169,\n",
       "                       -0.4485, -0.7221, -0.3471, -0.4536, -0.4444, -0.0919, -0.5660, -0.7326,\n",
       "                       -0.3965, -0.3088, -0.5517, -0.5586, -0.3481, -0.0419, -0.3824, -0.5283,\n",
       "                       -0.8313, -0.4682, -0.4769, -0.1512, -0.3754, -0.6003, -0.6649, -0.6129])),\n",
       "              ('regressor.target_model.gt_block.2.bn_node_x.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "              ('regressor.target_model.gt_block.2.bn_node_x.running_var',\n",
       "               tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1.])),\n",
       "              ('regressor.target_model.gt_block.2.bn_node_x.num_batches_tracked',\n",
       "               tensor(0)),\n",
       "              ('regressor.target_model.gt_block.2.bn_edge_e.weight',\n",
       "               tensor([0.3997, 0.9712, 1.0703, 0.9421, 0.4757, 0.7610, 0.9071, 0.6084, 0.5551,\n",
       "                       0.6290, 0.8479, 0.5448, 1.0826, 0.7523, 1.2222, 0.8422, 1.4405, 0.8098,\n",
       "                       0.4376, 0.6314, 0.4704, 0.7415, 0.6545, 0.8157, 0.5546, 1.1468, 0.9352,\n",
       "                       0.7680, 0.9630, 0.8768, 0.2620, 0.8618, 0.4915, 1.1226, 0.6096, 0.7137,\n",
       "                       0.9008, 0.7854, 0.8525, 0.5984, 0.9157, 0.6885, 0.6763, 0.5282, 0.1838,\n",
       "                       1.2597, 1.0940, 1.2464, 1.1472, 0.7488, 0.9484, 0.9242, 0.7483, 0.5440,\n",
       "                       0.3777, 1.4655, 0.7940, 1.1156, 1.1268, 0.3321, 0.7674, 0.6617, 0.7155,\n",
       "                       1.0164, 1.2347, 0.9648, 0.7383, 0.6432, 1.0833, 0.5770, 0.8879, 0.5447,\n",
       "                       0.5692, 0.5474, 0.9625, 1.0148, 0.4684, 0.7253, 0.6757, 0.8418, 1.0619,\n",
       "                       0.6929, 0.7382, 0.8384, 0.4995, 0.9683, 0.8825, 0.7301, 0.9046, 0.6851,\n",
       "                       0.9842, 0.9122, 0.7562, 1.3386, 0.7255, 1.2430, 1.0874, 0.9336, 0.6335,\n",
       "                       0.4361, 0.4582, 0.7928, 0.2279, 1.0500, 0.5773, 1.0388, 0.5397, 0.8060,\n",
       "                       1.1664, 0.4008, 0.9648, 0.5746, 1.0446, 0.6064, 1.0233, 0.9083, 0.6078,\n",
       "                       0.4399, 0.5900, 0.6665, 0.5189, 0.5034, 0.9818, 0.7581, 0.7541, 0.8016,\n",
       "                       0.9377, 0.6035])),\n",
       "              ('regressor.target_model.gt_block.2.bn_edge_e.bias',\n",
       "               tensor([-0.3977, -0.8286, -0.9671, -0.8346, -0.6502, -0.7750, -0.6722, -0.6866,\n",
       "                       -0.4313, -0.2906, -0.8939, -0.5307, -0.7339, -0.5419, -1.1937, -0.7747,\n",
       "                       -1.1942, -0.8596, -0.4572, -0.5128, -0.6858, -0.9540, -0.5654, -0.7780,\n",
       "                       -0.8303, -0.8536, -1.0257, -0.5152, -0.8450, -0.9139, -0.5746, -0.9213,\n",
       "                       -0.6940, -0.8965, -0.7851, -0.6806, -0.7139, -0.8715, -0.5856, -0.5689,\n",
       "                       -0.6352, -0.7699, -0.3302, -0.3925, -0.3300, -0.9154, -0.7085, -1.0434,\n",
       "                       -0.6492, -0.5445, -0.7114, -0.7516, -0.7212, -0.6431, -0.4806, -1.4471,\n",
       "                       -0.7715, -0.9430, -0.9583, -0.4938, -0.7600, -0.5527, -0.6859, -0.5991,\n",
       "                       -0.8134, -0.6490, -0.7834, -0.6855, -0.9613, -0.6655, -0.9109, -0.5948,\n",
       "                       -0.6785, -0.5259, -0.8162, -0.9719, -0.6744, -0.6059, -0.5880, -1.0000,\n",
       "                       -1.0697, -0.6363, -0.8003, -0.6615, -0.5540, -0.8684, -0.8988, -0.7097,\n",
       "                       -0.4457, -0.6891, -0.8375, -0.5442, -0.8118, -0.8169, -0.4008, -1.2568,\n",
       "                       -0.9108, -0.7361, -0.8083, -0.6305, -0.5082, -0.8112, -0.4028, -0.9509,\n",
       "                       -0.7254, -0.7066, -0.5737, -0.6313, -0.8416, -0.6252, -0.9803, -0.7631,\n",
       "                       -0.5779, -0.7275, -0.7614, -0.7422, -0.6789, -0.5065, -0.4351, -0.5270,\n",
       "                       -0.7646, -0.3887, -1.0826, -0.6298, -0.6780, -0.5556, -0.9222, -0.6194])),\n",
       "              ('regressor.target_model.gt_block.2.bn_edge_e.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "              ('regressor.target_model.gt_block.2.bn_edge_e.running_var',\n",
       "               tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1.])),\n",
       "              ('regressor.target_model.gt_block.2.bn_edge_e.num_batches_tracked',\n",
       "               tensor(0)),\n",
       "              ('regressor.target_model.gt_block.3.A.weight',\n",
       "               tensor([[-0.3535, -0.0295, -0.0616,  ..., -0.3151,  0.0213,  0.0150],\n",
       "                       [-0.0245, -0.3119, -0.0340,  ..., -0.0992, -0.0861, -0.2125],\n",
       "                       [-0.0775, -0.0021, -0.2526,  ..., -0.3721,  0.0568,  0.0418],\n",
       "                       ...,\n",
       "                       [-0.0962, -0.0509,  0.0800,  ..., -0.1067, -0.1928, -0.0413],\n",
       "                       [ 0.0081, -0.0645,  0.1078,  ...,  0.1292, -0.4594,  0.0901],\n",
       "                       [-0.1751, -0.0944, -0.0510,  ...,  0.0517, -0.0081, -0.1847]])),\n",
       "              ('regressor.target_model.gt_block.3.A.bias',\n",
       "               tensor([-0.0513, -0.0164,  0.0650,  0.0767,  0.0826,  0.0713,  0.0391,  0.0454,\n",
       "                        0.0527, -0.0002,  0.0216,  0.0122,  0.0178,  0.0093, -0.0142, -0.0692,\n",
       "                        0.0714,  0.0534,  0.0066, -0.0260,  0.0338,  0.0475, -0.0694, -0.0655,\n",
       "                        0.0565, -0.0179,  0.0374, -0.0727, -0.0354,  0.0828,  0.0054, -0.0112,\n",
       "                        0.0217,  0.0420,  0.0613, -0.0661, -0.0137, -0.0305, -0.0575,  0.0405,\n",
       "                        0.0463,  0.0754, -0.0764,  0.0914, -0.0832, -0.0691, -0.0321,  0.0340,\n",
       "                        0.0802,  0.0557,  0.0476,  0.0842,  0.0113,  0.0152, -0.0801, -0.0451,\n",
       "                        0.0418,  0.0506, -0.0755,  0.0263,  0.0823,  0.0832, -0.0630,  0.0084,\n",
       "                       -0.0168, -0.0439, -0.0089,  0.0099,  0.0387,  0.0421, -0.0540,  0.0201,\n",
       "                       -0.0207, -0.0086,  0.0894, -0.0426, -0.0621, -0.0549,  0.0895, -0.0672,\n",
       "                        0.0308,  0.0229, -0.0139, -0.0086,  0.0272, -0.0629,  0.0240,  0.0637,\n",
       "                       -0.0722,  0.0629, -0.0570,  0.0795,  0.0436,  0.0415, -0.0301, -0.0504,\n",
       "                       -0.0027, -0.0191,  0.0611,  0.0012, -0.0499,  0.0106,  0.0268,  0.0890,\n",
       "                        0.0873, -0.0655, -0.0507,  0.0019,  0.0255,  0.0392,  0.0352,  0.0389,\n",
       "                        0.0424, -0.0510,  0.0121,  0.0581, -0.0648,  0.0261, -0.0535,  0.0365,\n",
       "                        0.0536,  0.0329,  0.0467,  0.0383, -0.0560, -0.0866, -0.0217,  0.0072])),\n",
       "              ('regressor.target_model.gt_block.3.B.weight',\n",
       "               tensor([[-0.4572, -0.2037, -0.0657,  ..., -0.0982, -0.1867, -0.2888],\n",
       "                       [ 0.4339,  0.2682,  0.0840,  ..., -0.2897, -0.3855, -0.6311],\n",
       "                       [ 0.4196,  0.1668, -0.5477,  ..., -0.0485, -0.6813, -0.0117],\n",
       "                       ...,\n",
       "                       [-0.2620,  0.4828, -0.2618,  ..., -0.0841, -0.3763, -0.2141],\n",
       "                       [-0.4026,  0.3804, -0.2534,  ..., -0.3889,  0.0674, -0.3882],\n",
       "                       [ 0.3432, -0.4765, -0.1785,  ..., -0.2486, -0.1337,  0.3443]])),\n",
       "              ('regressor.target_model.gt_block.3.B.bias',\n",
       "               tensor([ 1.3855e+00, -6.2265e-01,  9.3610e-01, -3.6902e-02, -3.5535e-01,\n",
       "                        3.2325e-01,  2.5264e-03,  9.9878e-02,  1.0881e+00,  5.2719e-01,\n",
       "                       -2.4206e-01, -1.9215e-01, -6.0110e-01,  2.1946e-01,  3.9030e-01,\n",
       "                       -1.6051e-01,  1.3169e+00,  1.1202e-01,  8.0567e-02,  1.9619e-01,\n",
       "                       -1.4152e-01,  4.4833e-01,  2.4406e-01, -1.6720e-01,  1.5021e-01,\n",
       "                        2.0144e-01, -8.0861e-02,  1.8085e+00, -8.4597e-01,  5.1906e-01,\n",
       "                       -6.0965e-01,  1.5829e-01,  1.1641e-01, -1.5066e+00, -2.1114e-01,\n",
       "                        1.1272e+00, -7.2168e-01,  1.4057e+00, -5.9085e-02, -7.3034e-02,\n",
       "                       -3.1412e-04, -3.2580e-02, -2.0787e-01, -2.0544e-01, -1.1121e-01,\n",
       "                        1.5128e-01, -5.1339e-01,  3.6301e-01, -2.2378e-01,  5.2377e-01,\n",
       "                        2.5670e-01,  1.2539e+00, -4.8550e-01,  2.4766e-01,  3.0046e-01,\n",
       "                       -2.7119e-01,  3.6148e-02, -1.5959e-01,  4.0745e-01, -1.5183e+00,\n",
       "                       -7.0750e-02, -5.5536e-01,  5.1717e-01,  1.2332e-01, -4.0010e-01,\n",
       "                       -8.4744e-02, -9.9409e-01,  5.9219e-02,  5.3193e-02,  1.2006e-01,\n",
       "                        1.0935e-01, -2.5010e-01,  8.3248e-01, -3.0798e-01,  2.1939e-02,\n",
       "                        4.6871e-02,  8.0635e-01,  3.3093e-01,  1.7633e-01,  1.5643e-01,\n",
       "                       -1.3969e-01,  2.1361e-01,  1.8419e-01,  9.4169e-01, -2.4549e-01,\n",
       "                        4.8220e-01,  2.3744e-01,  2.3413e-01,  3.2731e-01, -8.6984e-01,\n",
       "                       -1.2687e-01, -6.9796e-01,  8.7522e-02, -2.8523e-01, -6.0734e-01,\n",
       "                       -8.3424e-02, -2.9063e-01, -7.8940e-02, -2.2327e-01,  1.6105e-01,\n",
       "                       -8.7141e-03, -6.6186e-01,  4.4464e-01, -1.4240e-01, -9.6270e-01,\n",
       "                       -1.6528e-01, -9.5377e-02,  4.6072e-01,  1.8838e-01, -2.7881e-01,\n",
       "                        3.1511e-01, -8.2548e-03,  3.6539e-03,  1.7276e-01,  2.7699e-01,\n",
       "                       -1.2446e+00,  6.5001e-01,  2.9107e-01, -3.7750e-01, -4.3785e-01,\n",
       "                        2.5634e-01,  2.1059e-01,  4.5391e-03,  4.5775e-01,  2.9126e-01,\n",
       "                       -2.9115e-02,  2.6938e-01, -1.0438e+00])),\n",
       "              ('regressor.target_model.gt_block.3.C.weight',\n",
       "               tensor([[-0.0498,  0.0373, -0.3010,  ..., -0.0810, -0.2908, -0.0881],\n",
       "                       [-0.0233, -0.5518,  0.0453,  ..., -0.1809, -0.6021, -0.3961],\n",
       "                       [ 0.1986, -0.0619,  0.0318,  ...,  0.1670,  0.0456,  0.2022],\n",
       "                       ...,\n",
       "                       [ 0.2976, -0.1421,  0.1133,  ..., -0.0472, -0.0581,  0.1324],\n",
       "                       [ 0.0718,  0.0315, -0.0220,  ...,  0.3115,  0.0026,  0.4359],\n",
       "                       [ 0.3691,  0.2063, -0.1471,  ..., -0.1197, -0.6082,  0.3074]])),\n",
       "              ('regressor.target_model.gt_block.3.C.bias',\n",
       "               tensor([-2.7377e-02,  9.0403e-02, -2.2853e-01, -9.3227e-02, -5.7672e-02,\n",
       "                        2.9333e-02,  3.4499e-02, -8.5699e-02, -1.5917e-02, -1.5718e-04,\n",
       "                       -7.1533e-02,  2.2120e-01,  7.7155e-02,  1.3051e-01,  1.8740e-01,\n",
       "                       -2.0913e-01, -2.1189e-01,  1.0084e-01, -1.1622e-01, -6.2022e-02,\n",
       "                       -2.8648e-02, -4.6749e-02,  8.8380e-02,  8.1912e-02, -3.6435e-02,\n",
       "                       -1.3979e-01, -5.0511e-02, -1.1065e-02, -1.9174e-01,  1.9899e-02,\n",
       "                        1.6454e-01, -1.0049e-01, -2.4428e-01, -7.2588e-04, -9.2409e-04,\n",
       "                        1.6083e-01,  9.2436e-02,  3.3425e-02,  4.2552e-02,  1.8425e-02,\n",
       "                        3.6570e-02,  1.1294e-01,  1.7048e-01, -3.3640e-02,  7.0520e-02,\n",
       "                        1.8908e-01, -6.3300e-02, -1.8408e-01, -6.1833e-02, -4.2525e-02,\n",
       "                        8.6824e-02,  8.2103e-02,  1.1420e-01,  2.8715e-02,  4.2404e-02,\n",
       "                       -1.8960e-01,  1.0930e-01, -1.2925e-03, -5.1508e-01, -2.6766e-01,\n",
       "                       -2.9282e-02,  1.2134e-01,  1.3815e-01, -2.5465e-01,  8.9001e-02,\n",
       "                       -2.2302e-02,  2.7938e-02,  8.0022e-02, -1.0181e-01,  6.5307e-02,\n",
       "                       -6.4454e-01, -8.3456e-03,  3.7722e-01, -8.4398e-02,  2.0544e-01,\n",
       "                       -3.6208e-02, -8.5677e-01, -9.5076e-02, -3.9772e-02,  2.2133e-01,\n",
       "                       -3.2556e-01,  1.6404e-01,  6.6664e-01, -4.2158e-01,  1.1214e-01,\n",
       "                       -1.1822e-01,  1.7795e-01, -4.0143e-01, -2.3571e-01,  4.4615e-02,\n",
       "                        2.8365e-03, -2.6989e-01, -4.0763e-02,  3.6441e-02,  5.6363e-02,\n",
       "                        9.7631e-02, -3.6838e-02, -3.3762e-01, -8.4230e-02,  1.7142e-01,\n",
       "                        8.1308e-02, -2.9378e-01,  1.5424e-01, -1.3924e-01, -2.4128e-02,\n",
       "                        3.8996e-01, -8.4595e-02, -2.3181e-01, -9.9539e-02,  5.2033e-02,\n",
       "                        6.5101e-02,  7.8584e-02, -8.3339e-04,  1.0715e+00,  8.7608e-02,\n",
       "                        9.1112e-02, -4.1469e-02,  6.7255e-02,  1.4103e-01, -3.6876e-02,\n",
       "                       -2.2505e-02,  1.2525e-01, -2.7353e-01,  1.2243e-02,  7.5148e-02,\n",
       "                       -2.8431e-01, -6.6383e-03,  6.9187e-02])),\n",
       "              ('regressor.target_model.gt_block.3.D.weight',\n",
       "               tensor([[-0.1157, -0.7876,  0.0079,  ...,  0.6590, -0.6766, -0.0979],\n",
       "                       [ 0.1782, -0.2609, -0.3118,  ..., -0.1126, -0.1173,  0.1521],\n",
       "                       [-0.5085, -0.1623, -0.2664,  ...,  0.0125,  0.0911, -0.1098],\n",
       "                       ...,\n",
       "                       [-0.2092, -0.2687,  0.0390,  ...,  0.0864, -0.0116, -0.1760],\n",
       "                       [-0.2282,  0.2621,  0.1434,  ...,  0.0260, -0.0411, -0.4448],\n",
       "                       [ 0.3939, -0.2615, -0.2170,  ..., -0.2257,  0.1915,  0.0294]])),\n",
       "              ('regressor.target_model.gt_block.3.D.bias',\n",
       "               tensor([ 0.0231,  0.0311, -0.2195, -0.1032,  0.0901,  0.0600,  0.0625, -0.0161,\n",
       "                       -0.0600,  0.1062,  0.0255,  0.2461,  0.0035,  0.0793,  0.1368, -0.3456,\n",
       "                       -0.2457,  0.0796, -0.0792, -0.0328,  0.0851,  0.0294,  0.2081,  0.1050,\n",
       "                        0.0159,  0.0187, -0.1484, -0.0271, -0.0923,  0.0528,  0.1252, -0.0545,\n",
       "                       -0.3132,  0.0211,  0.0679,  0.2570,  0.1059, -0.0786,  0.0315, -0.0061,\n",
       "                       -0.0361, -0.0376,  0.1041, -0.1144,  0.0961,  0.2229, -0.0356, -0.2104,\n",
       "                       -0.0484, -0.0710, -0.0551,  0.1828, -0.0277,  0.0059,  0.0497, -0.1686,\n",
       "                        0.0438, -0.0158, -0.5059, -0.1159, -0.0211,  0.1079,  0.1945, -0.3041,\n",
       "                        0.1001, -0.0687,  0.1297,  0.1471, -0.0179,  0.0238, -0.6812,  0.0259,\n",
       "                        0.4341, -0.0793,  0.1020,  0.0572, -0.8377, -0.1085, -0.1256,  0.3038,\n",
       "                       -0.2645,  0.1878,  0.6346, -0.4430,  0.0522, -0.1213,  0.2830, -0.3665,\n",
       "                       -0.2505, -0.0270, -0.0378, -0.2001,  0.0240,  0.0575,  0.0595,  0.1131,\n",
       "                        0.0147, -0.3667,  0.0570,  0.1783,  0.0468, -0.1554,  0.1595, -0.2548,\n",
       "                        0.0401,  0.4776, -0.1780, -0.2402, -0.0808,  0.1360,  0.1911, -0.0511,\n",
       "                       -0.1157,  1.0736,  0.0555,  0.0416,  0.0018,  0.0014,  0.1566, -0.0238,\n",
       "                       -0.0141,  0.1788, -0.2791,  0.0762,  0.0321, -0.4073, -0.0378, -0.0135])),\n",
       "              ('regressor.target_model.gt_block.3.E.weight',\n",
       "               tensor([[-0.3047,  0.1111, -0.0029,  ..., -0.0141, -0.0324,  0.0205],\n",
       "                       [ 0.0363,  0.3073,  0.2841,  ..., -0.1197,  0.2798, -0.1309],\n",
       "                       [-0.1126, -0.0222,  0.0232,  ...,  0.0703, -0.2142, -0.0527],\n",
       "                       ...,\n",
       "                       [ 0.0437,  0.2263, -0.2625,  ...,  0.3171,  0.0453,  0.0350],\n",
       "                       [-0.1845,  0.1310,  0.2183,  ..., -0.0617, -0.0210, -0.0812],\n",
       "                       [-0.0152, -0.1743, -0.2633,  ...,  0.0557, -0.0182,  0.2882]])),\n",
       "              ('regressor.target_model.gt_block.3.E.bias',\n",
       "               tensor([-0.0158,  0.1144, -0.1572, -0.0990, -0.0587,  0.0240,  0.1182,  0.0229,\n",
       "                       -0.0556, -0.0253, -0.0709,  0.2048, -0.0348,  0.1012,  0.2344, -0.2890,\n",
       "                       -0.2822,  0.1136, -0.0902, -0.0401,  0.0702,  0.0807,  0.1238, -0.0262,\n",
       "                       -0.0651, -0.1095, -0.1638, -0.1045, -0.1781,  0.0659,  0.1112, -0.1299,\n",
       "                       -0.2087,  0.0012,  0.0447,  0.1341,  0.0358,  0.0219,  0.0299, -0.0485,\n",
       "                        0.0029,  0.0583,  0.1365, -0.0684,  0.0104,  0.2212,  0.0222, -0.1888,\n",
       "                       -0.0586, -0.0441,  0.0961,  0.1855,  0.0171,  0.1020, -0.0250, -0.1284,\n",
       "                        0.0861,  0.0644, -0.6138, -0.2248,  0.0718,  0.0516,  0.2195, -0.3596,\n",
       "                        0.1115, -0.0923,  0.0126,  0.0655,  0.0055,  0.0765, -0.7418,  0.0720,\n",
       "                        0.3876, -0.0421,  0.1467,  0.0598, -0.7592, -0.1617, -0.1916,  0.2047,\n",
       "                       -0.2775,  0.1213,  0.6422, -0.4404,  0.0821, -0.1878,  0.2738, -0.3253,\n",
       "                       -0.2194,  0.0593,  0.0283, -0.2525,  0.0988,  0.1351, -0.0029,  0.0599,\n",
       "                        0.0877, -0.3627, -0.0685,  0.1506,  0.0376, -0.2165,  0.0626, -0.2052,\n",
       "                       -0.0268,  0.4439, -0.1657, -0.2904, -0.0690,  0.0893,  0.0576,  0.0258,\n",
       "                        0.0133,  1.1891,  0.0432, -0.0359, -0.0422,  0.1189,  0.1973,  0.0357,\n",
       "                        0.0226,  0.1028, -0.1568,  0.0385,  0.1198, -0.2462,  0.0181,  0.0544])),\n",
       "              ('regressor.target_model.gt_block.3.bn_node_x.weight',\n",
       "               tensor([1.3726, 0.9027, 0.7168, 0.9795, 0.7281, 0.7208, 0.8387, 1.1182, 0.7569,\n",
       "                       1.0456, 0.6979, 0.7688, 0.6702, 0.9339, 0.8057, 0.8975, 0.8011, 0.9093,\n",
       "                       0.9115, 0.6262, 0.7417, 1.3163, 0.6260, 0.7984, 0.7586, 0.7409, 0.9319,\n",
       "                       0.8607, 0.7459, 0.7325, 0.7257, 0.9536, 0.5535, 0.8685, 0.8680, 0.8315,\n",
       "                       0.9449, 0.8349, 0.7768, 0.6351, 0.7071, 0.7158, 0.8416, 0.6987, 0.8428,\n",
       "                       0.8605, 0.9968, 0.7670, 0.8550, 0.6593, 1.0487, 0.7439, 0.6994, 0.8374,\n",
       "                       0.8769, 1.0811, 0.8220, 0.9884, 0.7067, 0.9002, 0.8184, 0.9591, 0.8245,\n",
       "                       0.7009, 0.8750, 0.8263, 1.1672, 0.7213, 0.8538, 0.7314, 0.8815, 1.0114,\n",
       "                       0.9790, 1.1339, 0.7776, 0.7016, 0.9396, 0.8123, 0.7154, 0.6794, 0.7208,\n",
       "                       0.9866, 0.7291, 0.8146, 0.7160, 0.8730, 0.7491, 0.9212, 0.8542, 0.9248,\n",
       "                       0.7300, 0.7479, 0.9333, 0.9204, 1.1808, 1.0079, 0.8212, 0.7576, 1.0987,\n",
       "                       0.8388, 0.8372, 0.6802, 0.8128, 1.1665, 0.7371, 0.6660, 1.0043, 0.7101,\n",
       "                       0.9443, 0.8624, 0.9026, 0.9494, 0.7010, 1.2370, 0.7308, 0.9222, 0.8257,\n",
       "                       0.8448, 0.7042, 0.8345, 0.8184, 0.9694, 0.8422, 0.7711, 0.7634, 0.7041,\n",
       "                       0.8370, 1.1125])),\n",
       "              ('regressor.target_model.gt_block.3.bn_node_x.bias',\n",
       "               tensor([-0.2162, -0.8003, -0.3805, -0.3570, -0.3136, -0.5752, -0.3620, -0.6147,\n",
       "                        0.1394, -0.6321, -0.6339, -0.6017, -0.4632, -0.1027, -0.7073, -0.4761,\n",
       "                       -0.0839, -0.3542, -0.3573, -0.4997, -0.3698, -0.5992,  0.0757, -0.5583,\n",
       "                       -0.9862, -0.5713, -0.9256, -0.4271, -0.5315, -0.7254, -0.0935, -0.4678,\n",
       "                       -0.3151, -0.9272, -0.1324,  0.1499, -0.9418,  0.1272, -0.5382, -0.2199,\n",
       "                       -0.6779, -0.1959, -0.6816, -0.2296, -0.3686, -1.0037, -0.9673, -0.5734,\n",
       "                       -0.6672, -0.1712, -0.4483, -0.9631, -0.1115,  0.2752, -0.5730, -0.4421,\n",
       "                       -0.8355, -0.6305, -0.7489, -0.8538, -0.2106, -0.4636, -0.4931, -0.5975,\n",
       "                       -0.6541, -0.9254, -0.6988, -0.7255, -0.4094, -0.4296, -0.9775, -0.5578,\n",
       "                        0.0129, -0.9618, -0.4930, -0.5729, -0.7983, -0.7434, -0.2005, -0.6916,\n",
       "                       -0.6353,  0.0748, -0.7611, -0.3583, -0.2415, -0.4415, -0.7717, -0.6579,\n",
       "                       -0.6762, -0.8840, -0.4721, -0.7622, -0.2526, -0.4873, -1.0664, -0.7830,\n",
       "                       -0.5811, -0.4747, -0.3913, -0.0871,  0.0515, -0.3590, -0.0155, -0.8197,\n",
       "                       -0.7634, -0.8702, -0.4979, -0.4512, -1.0324, -0.0172, -0.4230, -0.5609,\n",
       "                       -0.5525, -0.1310, -0.7354, -0.7755, -0.1887, -0.3405, -0.4574, -0.7300,\n",
       "                       -0.8816, -0.3582, -0.6121, -0.4192, -0.6341, -0.8435, -0.6479, -0.9406])),\n",
       "              ('regressor.target_model.gt_block.3.bn_node_x.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "              ('regressor.target_model.gt_block.3.bn_node_x.running_var',\n",
       "               tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1.])),\n",
       "              ('regressor.target_model.gt_block.3.bn_node_x.num_batches_tracked',\n",
       "               tensor(0)),\n",
       "              ('regressor.target_model.gt_block.3.bn_edge_e.weight',\n",
       "               tensor([0.6836, 0.4571, 0.5122, 0.5320, 0.7430, 1.2502, 0.3563, 0.4524, 1.0044,\n",
       "                       0.3658, 1.0307, 0.8261, 1.2676, 0.4212, 0.6231, 1.2503, 0.8898, 1.0241,\n",
       "                       0.8737, 0.7121, 0.9672, 0.4458, 0.9171, 0.8713, 1.1323, 0.3283, 0.8535,\n",
       "                       0.8672, 0.7294, 0.8566, 0.4704, 0.4473, 1.1553, 0.6909, 0.5838, 0.8247,\n",
       "                       0.8565, 0.6297, 0.5904, 0.4668, 0.7177, 0.5507, 0.4673, 0.5930, 1.1521,\n",
       "                       1.1477, 0.9834, 1.1163, 0.8445, 1.2128, 0.6119, 0.5570, 0.4917, 0.6828,\n",
       "                       0.4512, 0.6306, 0.5372, 1.7954, 1.5217, 1.0374, 1.0529, 0.3819, 0.4638,\n",
       "                       1.3246, 0.9790, 0.7339, 0.6442, 0.4463, 0.4656, 0.4952, 0.5470, 0.1455,\n",
       "                       0.5289, 0.6267, 0.7153, 0.4409, 0.5237, 0.7507, 0.9372, 0.6554, 1.0685,\n",
       "                       0.5240, 1.3314, 0.3873, 0.6729, 0.9701, 1.0071, 0.8048, 0.9027, 0.5467,\n",
       "                       1.4402, 1.1551, 0.4461, 0.6866, 0.4690, 0.8343, 0.4164, 0.7751, 0.4117,\n",
       "                       0.5441, 0.5541, 0.9174, 0.3971, 0.8043, 0.3918, 1.1514, 0.7334, 0.5848,\n",
       "                       0.7651, 0.8690, 0.6965, 0.4322, 0.9394, 1.4208, 0.3650, 0.7444, 0.3989,\n",
       "                       0.4248, 0.6365, 1.2046, 0.6290, 0.4672, 1.3227, 0.3562, 0.4770, 1.0708,\n",
       "                       0.6446, 0.4689])),\n",
       "              ('regressor.target_model.gt_block.3.bn_edge_e.bias',\n",
       "               tensor([-0.8255, -0.3635, -0.4919, -0.3905, -0.6430, -1.1581, -0.6492, -0.3400,\n",
       "                       -0.5963, -0.5360, -0.6563, -0.7093, -0.6731, -0.4628, -0.5582, -0.8476,\n",
       "                       -0.8467, -0.6521, -0.9200, -0.7550, -0.6347, -0.5433, -0.9544, -0.5925,\n",
       "                       -0.7423, -0.5345, -0.6307, -0.5080, -0.5832, -0.6065, -0.8244, -0.5058,\n",
       "                       -0.5367, -0.7562, -0.4941, -0.7062, -0.5893, -0.7925, -0.4812, -0.4461,\n",
       "                       -0.7614, -0.7347, -0.6561, -0.5787, -0.9819, -0.7657, -0.8348, -0.7857,\n",
       "                       -0.5490, -0.9523, -0.6223, -0.7249, -0.3682, -0.5930, -0.5178, -0.6786,\n",
       "                       -0.5588, -0.3437, -0.8399, -0.8563, -0.7257, -0.4646, -0.5423, -0.8942,\n",
       "                       -0.7278, -0.6735, -0.4884, -0.5853, -0.5677, -0.4947, -0.7521, -0.5767,\n",
       "                       -0.8530, -0.5320, -0.7573, -0.6940, -0.6588, -0.6980, -0.6080, -0.6912,\n",
       "                       -1.1276, -0.5751, -1.3397, -0.6168, -0.5313, -0.8945, -1.0695, -0.8361,\n",
       "                       -0.7622, -0.6518, -0.7422, -0.7652, -0.2425, -0.5637, -0.3766, -0.5721,\n",
       "                       -0.5344, -0.8346, -0.5460, -0.6749, -0.4518, -0.6264, -0.6267, -0.7004,\n",
       "                       -0.4637, -0.8133, -0.4816, -0.6114, -0.7223, -0.9790, -0.8409, -0.4053,\n",
       "                       -0.7568, -1.0757, -0.4491, -0.9954, -0.7213, -0.4258, -0.6268, -0.6791,\n",
       "                       -0.4655, -0.5805, -1.1027, -0.4680, -0.6672, -1.1848, -0.9580, -0.3355])),\n",
       "              ('regressor.target_model.gt_block.3.bn_edge_e.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "              ('regressor.target_model.gt_block.3.bn_edge_e.running_var',\n",
       "               tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1.])),\n",
       "              ('regressor.target_model.gt_block.3.bn_edge_e.num_batches_tracked',\n",
       "               tensor(0)),\n",
       "              ('regressor.target_model.gt_block.4.A.weight',\n",
       "               tensor([[-0.2717, -0.1296, -0.0053,  ..., -0.0577,  0.0073, -0.2040],\n",
       "                       [-0.0035, -0.1992,  0.1047,  ..., -0.0422,  0.1541, -0.0308],\n",
       "                       [-0.1405,  0.1079, -0.1799,  ..., -0.3542, -0.0370,  0.0139],\n",
       "                       ...,\n",
       "                       [ 0.1744, -0.1444,  0.0268,  ..., -0.3812,  0.0817, -0.0180],\n",
       "                       [ 0.1026, -0.0394,  0.1390,  ...,  0.0400, -0.4417, -0.0466],\n",
       "                       [-0.0927,  0.0889,  0.1421,  ..., -0.0451,  0.1458,  0.0205]])),\n",
       "              ('regressor.target_model.gt_block.4.A.bias',\n",
       "               tensor([ 3.1199e-02, -1.3847e-05, -4.3191e-02,  1.0186e-02,  4.7429e-02,\n",
       "                       -1.5220e-03,  2.2130e-02, -6.4958e-02, -7.9645e-02,  6.9921e-02,\n",
       "                       -5.0397e-02, -3.1414e-02,  3.5187e-03, -5.4294e-02, -1.1224e-02,\n",
       "                        5.6148e-02,  5.2801e-02, -8.1007e-02,  6.7610e-02, -4.0450e-03,\n",
       "                        7.8027e-02,  2.2205e-03, -7.4753e-02, -4.3507e-02,  7.6151e-02,\n",
       "                       -5.1846e-02,  8.1730e-02,  8.9914e-04, -3.7375e-02, -3.9504e-02,\n",
       "                        4.9518e-02, -7.5293e-02,  2.4977e-02, -8.2479e-02, -7.9643e-02,\n",
       "                        2.5600e-02, -6.3022e-02, -5.6584e-02,  2.6641e-02,  2.1926e-02,\n",
       "                       -6.0913e-02,  7.6274e-02, -1.1642e-02,  4.1392e-03, -1.8904e-02,\n",
       "                        5.6670e-02, -2.7642e-02,  1.0986e-03, -2.9648e-03, -7.2262e-02,\n",
       "                        3.6266e-02,  8.8425e-03, -7.5440e-02,  5.4401e-02,  5.9970e-02,\n",
       "                       -3.7417e-02,  2.5210e-02,  3.9913e-02,  7.1555e-02,  9.3551e-03,\n",
       "                       -7.0704e-02,  8.3071e-02, -6.6939e-02,  2.1936e-02,  4.8260e-02,\n",
       "                       -3.3231e-02,  4.0767e-03, -7.5356e-02, -4.8666e-02, -1.1103e-02,\n",
       "                        2.2935e-02, -7.3314e-02,  6.4743e-02, -3.7709e-02,  6.4792e-02,\n",
       "                        6.5902e-02,  3.0147e-02,  1.8956e-02,  4.7957e-02, -4.4833e-02,\n",
       "                       -8.6646e-02, -4.2467e-02,  7.6807e-02, -4.8099e-02, -6.7476e-02,\n",
       "                        3.6053e-02, -7.2376e-02,  4.6113e-02, -4.2995e-03, -4.7981e-02,\n",
       "                        6.6620e-02, -2.1428e-02, -7.1643e-02, -3.8577e-02,  3.5866e-02,\n",
       "                       -6.0316e-02,  3.9399e-02, -4.5661e-02,  3.0444e-02, -4.5339e-02,\n",
       "                       -1.4855e-02, -3.0436e-02, -3.3391e-02,  5.8023e-02,  8.6054e-02,\n",
       "                       -2.1878e-02, -3.5679e-02, -2.4812e-02,  2.8252e-02, -5.2181e-02,\n",
       "                        4.8019e-02,  3.7382e-02,  6.1746e-02, -4.4306e-02,  7.8576e-02,\n",
       "                       -3.0730e-02, -4.5488e-02,  6.6274e-02,  7.3408e-02,  7.8061e-02,\n",
       "                        2.9620e-02,  8.2909e-02, -9.2710e-03, -6.4994e-02, -2.8012e-02,\n",
       "                       -4.2882e-02,  3.2222e-02,  7.6896e-02])),\n",
       "              ('regressor.target_model.gt_block.4.B.weight',\n",
       "               tensor([[-0.1963,  0.0848,  0.2099,  ..., -0.3225,  0.0689, -0.4471],\n",
       "                       [ 0.1263, -0.6476, -0.2646,  ..., -0.4632,  0.0573, -0.1505],\n",
       "                       [-0.1456,  0.4879, -0.1950,  ...,  0.1636, -0.0195,  0.1013],\n",
       "                       ...,\n",
       "                       [-0.2953,  0.3380, -0.0911,  ..., -0.0840, -0.8441,  0.0677],\n",
       "                       [-0.1245, -0.1282,  0.2922,  ..., -0.9357,  0.1927,  0.1667],\n",
       "                       [-0.1662, -0.2360, -0.2744,  ...,  0.0743, -0.0856, -0.5080]])),\n",
       "              ('regressor.target_model.gt_block.4.B.bias',\n",
       "               tensor([ 5.5710e-01, -1.2687e-01, -2.0110e-01,  5.2713e-01, -3.6340e-01,\n",
       "                       -5.1276e-01,  1.7534e-01,  1.5366e-01,  1.4006e-01, -6.6517e-02,\n",
       "                       -2.6772e-01,  6.6997e-02,  1.0737e+00, -2.9123e-01, -1.9854e-01,\n",
       "                       -1.4655e-01, -1.9205e-01, -3.4731e-01,  1.7747e-01,  6.5056e-01,\n",
       "                       -5.4646e-02,  6.5304e-02, -2.7452e-01, -3.3998e-01, -3.0876e-01,\n",
       "                        6.7396e-01,  2.5710e-01, -5.8833e-01, -3.9230e-01, -6.4277e-02,\n",
       "                        6.4910e-01,  9.5690e-02, -5.5265e-01, -1.7429e-01, -2.9470e-01,\n",
       "                        9.3198e-01, -1.0299e-01, -6.8848e-01, -8.7708e-01, -3.6463e-01,\n",
       "                        1.5276e-01,  3.6888e-02,  5.3427e-02,  2.2622e-01, -1.1336e-01,\n",
       "                        2.5721e-01, -1.0253e-01,  2.6347e-01, -7.6987e-03, -1.6824e-01,\n",
       "                       -2.1453e-01, -4.1919e-01, -2.8758e-01,  1.1628e-01, -1.9276e-01,\n",
       "                       -1.7594e-02,  3.1390e-01,  6.0301e-01, -6.4128e-01, -1.0680e+00,\n",
       "                        7.4047e-01, -3.2305e-01,  4.1250e-02,  6.4558e-01,  1.5115e-01,\n",
       "                        2.2932e-01, -7.3326e-02, -1.2820e-02,  3.6317e-01,  1.8958e-01,\n",
       "                        5.0558e-01, -1.4586e+00, -3.5404e-02,  2.5950e-01, -8.4771e-01,\n",
       "                        8.6694e-01, -2.4705e-02,  2.7829e-02,  1.0351e+00,  1.8630e-01,\n",
       "                        6.6183e-01, -1.3617e-01, -2.6931e-01,  2.8896e-01, -1.1698e-01,\n",
       "                        1.4226e-01,  3.0184e-01,  6.0119e-01, -4.9457e-01,  3.5544e-01,\n",
       "                       -3.7996e-01, -8.2637e-02, -3.2713e-01,  1.9159e-01, -2.8072e-01,\n",
       "                        1.7275e-01,  1.8667e-02,  5.8788e-01, -3.0341e-02, -5.0743e-01,\n",
       "                        6.8925e-02, -6.5020e-01,  1.7237e-01,  3.5566e-01,  2.0739e-01,\n",
       "                        1.1233e-01,  6.0676e-02,  3.3388e-01,  3.9552e-01,  1.9700e-01,\n",
       "                        1.5563e-02, -5.8377e-01,  1.4606e-01,  6.8879e-01,  1.4335e-01,\n",
       "                       -5.3077e-01,  6.1664e-01, -5.3537e-04,  1.2118e-01, -4.9822e-02,\n",
       "                        1.7670e-01, -3.4608e-01, -2.7617e-03, -2.9056e-01,  5.3892e-01,\n",
       "                        4.2768e-01, -5.7578e-01, -9.0933e-01])),\n",
       "              ('regressor.target_model.gt_block.4.C.weight',\n",
       "               tensor([[ 0.1392,  0.3794, -0.1522,  ..., -0.4558, -0.4649,  0.0013],\n",
       "                       [-0.1393, -0.0983,  0.1233,  ..., -0.2006,  0.0556, -0.7900],\n",
       "                       [-0.0440, -0.0304,  0.0398,  ...,  0.0767,  0.0587, -0.2593],\n",
       "                       ...,\n",
       "                       [ 0.1490, -0.2115,  0.1268,  ...,  0.0811,  0.0126,  0.1113],\n",
       "                       [ 0.0112,  0.0016, -0.0037,  ...,  0.1960,  0.0185,  0.2158],\n",
       "                       [ 0.1659, -0.1294, -0.1565,  ..., -0.1874,  0.2038,  0.2547]])),\n",
       "              ('regressor.target_model.gt_block.4.C.bias',\n",
       "               tensor([ 8.1314e-02, -3.9673e-01,  1.2126e-02,  1.7878e-02,  4.3489e-02,\n",
       "                        2.4645e-02, -3.6587e-02, -3.4954e-02,  2.6153e-01, -1.1870e-01,\n",
       "                       -6.4074e-02, -7.9609e-02,  7.7025e-02,  8.3007e-02,  1.5083e-01,\n",
       "                       -7.4036e-01,  4.8738e-02,  1.1207e-01,  1.3144e-01, -4.4806e-02,\n",
       "                        3.5273e-02,  6.6875e-02, -5.8135e-02, -7.2267e-01,  1.8590e-01,\n",
       "                        1.0905e-01, -2.4309e-01,  1.0534e-01, -4.1858e-02,  9.7806e-02,\n",
       "                       -8.4895e-01, -6.5533e-02, -7.8526e-02,  1.5776e-01,  1.0399e-01,\n",
       "                       -4.9936e-02,  3.2206e-01, -2.7366e-01, -3.8000e-01,  1.2893e-01,\n",
       "                        8.9834e-04,  9.2135e-02, -1.4162e-01, -7.6076e-02, -9.2375e-03,\n",
       "                        2.3543e-01,  1.5591e-01, -2.5590e-02,  2.9123e-01, -2.3504e-02,\n",
       "                        3.2253e-01, -1.3091e-01,  9.0967e-02,  1.3370e-01,  4.3765e-02,\n",
       "                        1.0120e-01, -3.3371e-01,  4.7424e-01,  7.5218e-02, -3.2651e-01,\n",
       "                       -2.0243e-01,  6.3445e-02,  2.6768e-02, -1.2877e-01,  5.0292e-02,\n",
       "                       -4.7846e-02,  4.9963e-02, -9.2547e-03, -5.4894e-04, -1.3272e-02,\n",
       "                       -8.3112e-02, -4.0344e-01, -2.0267e-02, -3.7673e-02, -1.0058e-01,\n",
       "                       -1.3218e-01,  9.3833e-02, -7.1320e-02, -5.8073e-01,  3.2436e-01,\n",
       "                       -8.1089e-02,  1.0796e-01, -2.6372e-01, -1.2652e-01,  1.7408e-01,\n",
       "                       -9.6663e-02, -2.4624e-01,  2.2526e-01,  1.5717e-02, -5.1600e-01,\n",
       "                        4.8341e-02, -1.6156e-01, -4.2387e-02,  2.4228e-01, -1.9034e-01,\n",
       "                       -4.1995e-02,  1.6177e-01, -9.0119e-02, -2.3418e-01,  1.8759e-01,\n",
       "                       -5.1322e-02, -3.2094e-02,  6.0207e-01,  1.7699e-01,  6.5363e-02,\n",
       "                        1.4329e-01,  2.1867e-01, -6.6043e-02, -1.7244e-01,  6.4488e-02,\n",
       "                       -7.1802e-02, -1.1230e-01,  4.3123e-02,  1.6627e-01,  1.0059e-01,\n",
       "                        1.9532e-02,  1.3604e-02,  2.1903e-01,  5.5518e-02, -8.9116e-02,\n",
       "                       -1.0699e-01, -9.7284e-02,  1.8463e-02, -1.6583e-01,  7.6866e-02,\n",
       "                        7.4734e-02,  2.6690e-02, -6.5604e-02])),\n",
       "              ('regressor.target_model.gt_block.4.D.weight',\n",
       "               tensor([[ 0.1946,  0.0567,  0.0565,  ..., -0.2162,  0.0861,  0.0835],\n",
       "                       [-0.1776, -0.6536, -0.2463,  ..., -0.2829, -0.3071, -0.2265],\n",
       "                       [ 0.1985,  0.1789, -0.1812,  ...,  0.4881, -0.0058, -0.2044],\n",
       "                       ...,\n",
       "                       [ 0.5037,  0.0404,  0.2624,  ..., -0.1241, -0.2141,  0.1670],\n",
       "                       [-0.2399,  0.0258,  0.2925,  ...,  0.0750, -0.4526,  0.0215],\n",
       "                       [-0.1408,  0.0377, -0.3198,  ...,  0.4696, -0.1468, -0.1833]])),\n",
       "              ('regressor.target_model.gt_block.4.D.bias',\n",
       "               tensor([ 0.0151, -0.3433,  0.0374,  0.0190,  0.0051,  0.0276, -0.0513,  0.0729,\n",
       "                        0.1829, -0.1434, -0.1338, -0.0469,  0.0216,  0.1059,  0.0584, -0.6209,\n",
       "                       -0.0190,  0.1309,  0.0711, -0.0540,  0.1235,  0.1781,  0.0602, -0.7108,\n",
       "                        0.2041,  0.0400, -0.1032,  0.1481, -0.0730,  0.1187, -0.7564, -0.0285,\n",
       "                       -0.0253,  0.1337,  0.0860,  0.0602,  0.2479, -0.2036, -0.3332,  0.2035,\n",
       "                       -0.0012,  0.1523, -0.1880,  0.0091,  0.0604,  0.2039,  0.1728, -0.0018,\n",
       "                        0.3175,  0.0300,  0.3266, -0.0601,  0.2246,  0.0423, -0.0739,  0.0328,\n",
       "                       -0.3906,  0.5042,  0.0404, -0.4297, -0.2060,  0.0215,  0.0529, -0.0317,\n",
       "                        0.0928, -0.1573,  0.0584,  0.0259,  0.0802, -0.0304, -0.0964, -0.4746,\n",
       "                       -0.1167,  0.0389,  0.0261, -0.1251,  0.0838,  0.0687, -0.5501,  0.2121,\n",
       "                       -0.1734,  0.0914, -0.3981, -0.1562,  0.0130,  0.0308, -0.1047,  0.1581,\n",
       "                        0.0180, -0.5538,  0.1329, -0.0850, -0.0462,  0.2314, -0.1731,  0.0055,\n",
       "                        0.2902, -0.1049, -0.2027,  0.0402,  0.0364, -0.1499,  0.6233,  0.0437,\n",
       "                        0.0509,  0.1856,  0.2175, -0.0534, -0.1214,  0.1223,  0.0511, -0.1093,\n",
       "                        0.0329,  0.1354,  0.0584,  0.0070, -0.0228,  0.1518, -0.0094,  0.0230,\n",
       "                        0.0521, -0.0827,  0.0016, -0.0200,  0.0417,  0.0788,  0.1422, -0.0260])),\n",
       "              ('regressor.target_model.gt_block.4.E.weight',\n",
       "               tensor([[-0.1391, -0.2270, -0.1452,  ...,  0.6055,  0.0651, -0.3729],\n",
       "                       [-0.0165, -0.0132, -0.0318,  ...,  0.1219, -0.0802, -0.1228],\n",
       "                       [-0.0748,  0.0435,  0.2342,  ...,  0.0287,  0.0999,  0.0298],\n",
       "                       ...,\n",
       "                       [-0.0832,  0.0410,  0.0895,  ..., -0.1000, -0.0976, -0.1059],\n",
       "                       [-0.1260,  0.1375, -0.1853,  ..., -0.0436, -0.1931, -0.0590],\n",
       "                       [-0.0147, -0.1074, -0.1679,  ..., -0.0645, -0.2755,  0.0745]])),\n",
       "              ('regressor.target_model.gt_block.4.E.bias',\n",
       "               tensor([ 3.9874e-02, -3.2919e-01,  8.2637e-03,  3.7357e-02,  2.9559e-02,\n",
       "                        1.8948e-02, -9.9995e-02,  4.6328e-02,  2.9485e-01, -1.7248e-01,\n",
       "                       -5.5059e-02, -1.0175e-01, -2.8728e-02,  3.6766e-02,  2.2246e-01,\n",
       "                       -6.6104e-01,  4.3980e-04,  1.3055e-03,  6.3977e-02, -1.1176e-01,\n",
       "                        7.3615e-02,  1.2438e-01, -8.7898e-02, -7.9420e-01,  3.4132e-01,\n",
       "                        1.2573e-01, -9.9188e-02,  1.6723e-01, -9.0244e-02,  2.0549e-01,\n",
       "                       -7.5416e-01,  6.4302e-02, -6.5553e-02,  8.9136e-02,  1.3230e-01,\n",
       "                       -1.7860e-02,  3.1682e-01, -2.2054e-01, -3.6702e-01,  7.4882e-02,\n",
       "                        7.1695e-02,  1.5997e-01, -6.0865e-02,  1.7043e-02, -8.2747e-04,\n",
       "                        1.7586e-01,  2.5987e-01,  1.9549e-03,  2.9850e-01, -9.2540e-02,\n",
       "                        3.3358e-01, -5.0705e-02,  1.9237e-01,  7.7641e-02, -6.4138e-02,\n",
       "                        1.4902e-01, -3.0252e-01,  5.1645e-01,  1.1592e-02, -4.3793e-01,\n",
       "                       -1.6358e-01,  7.0855e-02, -3.2343e-02,  2.4558e-02,  8.2344e-02,\n",
       "                       -3.8752e-02,  7.8628e-02, -8.5121e-02,  3.3482e-02, -3.5862e-02,\n",
       "                       -3.6003e-02, -4.6322e-01, -1.0510e-01, -3.9925e-02, -2.6207e-02,\n",
       "                       -2.4823e-01,  1.0859e-01,  4.2454e-03, -5.0654e-01,  2.7777e-01,\n",
       "                       -1.6759e-01,  7.6305e-02, -3.7835e-01, -9.3896e-02,  7.5497e-03,\n",
       "                       -1.0614e-02, -1.7242e-01,  1.4616e-01,  1.1401e-01, -5.7743e-01,\n",
       "                        1.2593e-01, -9.1385e-02,  2.4005e-02,  2.1806e-01, -3.1833e-01,\n",
       "                       -6.1257e-02,  2.0981e-01, -1.0095e-01, -2.1904e-01,  7.2546e-02,\n",
       "                        4.3189e-02, -1.1610e-01,  6.2455e-01,  4.7057e-02,  6.5471e-02,\n",
       "                        1.2690e-01,  2.1335e-01, -1.0382e-01, -5.4405e-02,  1.8682e-01,\n",
       "                       -7.4882e-02, -1.2712e-01,  6.5730e-03,  1.2585e-01,  2.1724e-02,\n",
       "                        4.7167e-02,  8.3435e-05,  2.0932e-01, -3.4060e-02, -2.6515e-03,\n",
       "                        3.2256e-02, -1.6673e-02,  5.5245e-02, -1.3571e-01, -7.7850e-02,\n",
       "                        6.5861e-02,  9.8835e-04, -8.0098e-02])),\n",
       "              ('regressor.target_model.gt_block.4.bn_node_x.weight',\n",
       "               tensor([2.0943, 0.9898, 1.2121, 1.2280, 1.1408, 1.2294, 1.0799, 1.2902, 1.0210,\n",
       "                       1.0746, 0.8475, 0.8321, 0.9128, 0.8189, 1.2818, 1.1256, 0.9751, 1.0304,\n",
       "                       1.0009, 1.9086, 1.1537, 2.0325, 0.8371, 1.0398, 0.9269, 0.7739, 1.0288,\n",
       "                       1.2468, 1.1388, 1.0149, 0.9074, 1.3455, 1.0495, 0.9122, 1.5135, 1.2305,\n",
       "                       0.9677, 1.0849, 0.8492, 1.0107, 0.7463, 1.3317, 0.8333, 1.0544, 1.2236,\n",
       "                       0.8536, 1.2698, 0.8548, 1.1595, 0.9722, 2.0423, 1.0950, 1.1569, 1.2752,\n",
       "                       1.1064, 1.0982, 0.7645, 1.1139, 1.0306, 0.6154, 0.9392, 0.8241, 0.9098,\n",
       "                       0.8499, 0.8284, 1.0038, 1.0524, 1.0485, 1.3291, 0.8286, 1.1421, 0.5822,\n",
       "                       1.1652, 1.1636, 1.0447, 0.7370, 1.2492, 1.0485, 1.0303, 0.8990, 1.0211,\n",
       "                       0.7378, 0.7102, 1.2445, 0.7399, 0.8383, 1.2902, 1.0417, 0.7592, 0.9721,\n",
       "                       1.5386, 1.0727, 1.6299, 1.0384, 0.8884, 1.4445, 0.8755, 1.4993, 1.1359,\n",
       "                       0.7585, 0.9244, 1.4432, 0.7855, 0.9875, 1.2165, 0.8849, 1.5324, 0.9344,\n",
       "                       1.2844, 1.0730, 1.3191, 0.9365, 0.9949, 2.3290, 0.9932, 0.9474, 0.9409,\n",
       "                       1.0818, 0.8689, 1.0608, 1.0141, 1.6244, 0.8632, 1.0297, 0.9094, 0.9629,\n",
       "                       1.1217, 1.0655])),\n",
       "              ('regressor.target_model.gt_block.4.bn_node_x.bias',\n",
       "               tensor([ 0.2484, -0.4878, -0.7771,  0.1674, -0.3835, -0.1184, -0.6605, -0.6653,\n",
       "                       -0.1043, -0.3693, -0.4942,  0.1087, -0.2663, -0.1711, -0.6875, -0.2791,\n",
       "                       -0.1930, -0.6448, -0.0792, -0.3758, -0.8807,  0.9022,  0.1038, -0.4477,\n",
       "                       -1.1842, -0.5345, -0.6690, -1.5333, -0.5477, -1.0019, -0.2634, -0.5518,\n",
       "                       -1.0915, -0.6619, -0.2314, -0.9131, -0.6996, -0.1413,  0.0898, -0.6856,\n",
       "                       -0.6249, -0.3805, -0.3579, -0.3338, -0.9405, -0.6682, -1.1632, -0.4836,\n",
       "                       -0.8134, -0.7769, -0.0337, -1.0505, -0.5824,  0.0832, -0.5830, -1.3407,\n",
       "                       -0.6873, -0.6393, -1.2822, -0.3257, -0.3309, -0.4494, -0.6730, -1.0632,\n",
       "                       -0.0454, -1.0005, -0.1841, -0.6306,  0.0268, -0.5664, -1.4442,  0.0603,\n",
       "                       -0.3861, -0.5847, -0.6283, -0.1815, -1.0454, -0.4473, -0.7210, -0.7701,\n",
       "                       -0.5895,  0.0694, -0.7016,  0.0704, -0.5258, -0.3632, -0.7057, -0.4742,\n",
       "                       -0.6211, -1.5404, -0.3656, -0.8259, -0.0837, -0.4177, -0.5384, -1.3984,\n",
       "                       -0.1524, -0.1026, -0.1643,  0.3008, -0.0218, -1.2143,  0.1377, -0.4626,\n",
       "                       -0.3429, -0.9115,  0.4473, -0.4806, -1.5500, -0.1357, -0.5529, -0.5628,\n",
       "                       -0.6256, -0.0105, -0.6116, -1.1705,  0.0423, -0.2662, -0.1054, -0.8859,\n",
       "                       -0.8460, -0.1397, -0.4462, -0.1032, -0.8251, -0.9389, -1.0098, -0.5852])),\n",
       "              ('regressor.target_model.gt_block.4.bn_node_x.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "              ('regressor.target_model.gt_block.4.bn_node_x.running_var',\n",
       "               tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1.])),\n",
       "              ('regressor.target_model.gt_block.4.bn_node_x.num_batches_tracked',\n",
       "               tensor(0)),\n",
       "              ('regressor.target_model.gt_block.4.bn_edge_e.weight',\n",
       "               tensor([0.7938, 1.4054, 0.7082, 0.2816, 0.6344, 0.2641, 1.1726, 0.6054, 1.1719,\n",
       "                       0.4916, 1.3893, 0.6793, 0.6431, 1.3675, 0.5226, 0.4066, 0.6355, 0.6643,\n",
       "                       0.4582, 0.2749, 0.2064, 0.5061, 1.3955, 0.4358, 1.8889, 0.2968, 2.0451,\n",
       "                       0.7793, 0.1638, 0.2756, 0.8661, 0.9763, 0.7191, 1.2386, 0.5522, 0.4848,\n",
       "                       0.4699, 0.3445, 1.3895, 0.6244, 0.7709, 0.4462, 1.4576, 0.5432, 0.3493,\n",
       "                       1.3063, 0.7404, 0.3835, 1.5291, 0.4443, 1.8625, 0.6891, 1.9963, 0.4334,\n",
       "                       0.4094, 0.5320, 1.4308, 1.7439, 0.8884, 1.3447, 0.5812, 0.5162, 1.3717,\n",
       "                       2.1890, 1.0749, 0.5021, 0.9312, 0.6601, 0.5683, 0.3943, 0.5617, 1.3950,\n",
       "                       0.8325, 0.6049, 0.3089, 1.0431, 0.8671, 0.8277, 0.3432, 0.2905, 0.4077,\n",
       "                       1.1767, 0.7676, 0.9153, 0.1438, 0.5942, 0.4627, 1.0234, 0.6290, 0.1718,\n",
       "                       0.5225, 0.3681, 1.7526, 0.7809, 1.6726, 0.7529, 0.3039, 0.2763, 0.6168,\n",
       "                       0.9093, 0.7573, 0.6345, 2.0497, 0.3629, 0.7995, 1.4069, 0.4959, 0.8221,\n",
       "                       0.9680, 0.5839, 0.5406, 0.6321, 0.4794, 0.3772, 0.9140, 1.3338, 0.5694,\n",
       "                       0.5784, 1.1614, 0.5138, 0.4941, 0.7798, 0.2011, 0.3570, 0.3316, 0.7062,\n",
       "                       0.7188, 0.5515])),\n",
       "              ('regressor.target_model.gt_block.4.bn_edge_e.bias',\n",
       "               tensor([-0.6702, -0.2628, -0.7699, -0.6565, -0.6268, -0.6407, -0.6069, -0.6071,\n",
       "                       -0.5394, -0.3819, -0.6080, -0.9067, -0.5820, -0.6658, -0.4938, -0.5418,\n",
       "                       -0.5399, -0.3508, -0.3331, -0.7149,  0.0818, -0.5957, -0.6848, -0.4519,\n",
       "                       -0.6372, -0.5049, -0.2131, -0.3732, -0.6855, -0.3324, -0.6409, -0.3143,\n",
       "                       -0.5794, -0.7476, -0.6472, -0.7865, -0.8139, -0.6031, -0.5718, -0.5700,\n",
       "                       -0.6759, -0.6815, -0.3825, -0.6211, -0.4356, -1.2815, -0.5502, -0.5909,\n",
       "                       -0.0538, -0.6332, -0.6258, -0.4849, -0.8505, -0.5140, -0.2869, -0.2112,\n",
       "                       -0.5840, -0.2697, -0.3453, -1.4230, -0.3748, -0.7390, -1.1210, -0.3919,\n",
       "                        0.0441, -0.5927, -0.5634, -0.2086, -0.8189, -0.5971, -0.6398, -0.6999,\n",
       "                       -1.0346, -0.4863, -0.6309, -0.4646, -0.5024, -0.9001, -0.7910, -0.8829,\n",
       "                       -0.7081, -0.5609, -0.7553, -0.1876, -0.5151, -0.6167, -0.5837, -0.5211,\n",
       "                       -0.5433, -0.6411, -0.5798, -0.4632,  0.1169, -0.4637, -0.7402, -0.6897,\n",
       "                       -0.7118, -0.6283, -0.6918, -0.8083, -0.3963, -0.1513, -0.7741, -0.5060,\n",
       "                       -0.6729, -0.7878, -0.5915, -0.7255, -0.3962, -0.8702, -0.7329, -0.6965,\n",
       "                       -0.5823, -0.5433, -1.0435, -0.2270, -0.7795, -0.9946, -0.9080, -0.6695,\n",
       "                       -0.5813, -0.9845, -0.8342, -0.6901, -0.5013, -0.4953, -0.9266, -0.5239])),\n",
       "              ('regressor.target_model.gt_block.4.bn_edge_e.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "              ('regressor.target_model.gt_block.4.bn_edge_e.running_var',\n",
       "               tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1.])),\n",
       "              ('regressor.target_model.gt_block.4.bn_edge_e.num_batches_tracked',\n",
       "               tensor(0)),\n",
       "              ('regressor.target_model.gt_block.5.A.weight',\n",
       "               tensor([[-0.2018,  0.0158, -0.1497,  ...,  0.0453,  0.0975, -0.1914],\n",
       "                       [-0.1849, -0.0745, -0.0935,  ...,  0.0831,  0.1198, -0.0134],\n",
       "                       [ 0.1641, -0.0659,  0.0814,  ...,  0.0186, -0.1336,  0.0134],\n",
       "                       ...,\n",
       "                       [ 0.0868, -0.0262, -0.0276,  ..., -0.2165,  0.0219,  0.0933],\n",
       "                       [-0.1033,  0.1767, -0.1567,  ..., -0.1281, -0.1780,  0.0048],\n",
       "                       [-0.1550, -0.0935,  0.0596,  ..., -0.1187, -0.1248, -0.1840]])),\n",
       "              ('regressor.target_model.gt_block.5.A.bias',\n",
       "               tensor([-1.9212e-02, -2.6831e-02, -7.8818e-02,  8.2120e-02, -4.7460e-02,\n",
       "                       -2.8934e-02,  9.6033e-03,  1.7325e-02,  7.3417e-02,  5.6571e-02,\n",
       "                        3.0341e-02,  3.5249e-02,  4.5287e-02,  4.1193e-03,  7.7926e-02,\n",
       "                       -7.2906e-02,  6.9946e-02, -7.8513e-02, -3.9776e-02, -1.3411e-03,\n",
       "                        3.3622e-02, -3.9591e-02,  1.7812e-02,  1.2980e-02,  5.9421e-02,\n",
       "                        1.3460e-02, -3.2592e-02, -1.4621e-02, -3.1682e-02,  9.0456e-02,\n",
       "                        5.7575e-02,  2.9428e-02, -1.1915e-02, -4.2117e-03,  5.1798e-02,\n",
       "                       -6.7471e-02, -9.4169e-03, -4.5139e-02, -1.9758e-02,  4.8195e-02,\n",
       "                       -7.7859e-02,  8.3291e-02,  8.7672e-02,  4.5065e-02, -5.0935e-03,\n",
       "                        8.6957e-02,  4.7420e-02,  1.8686e-02, -8.2223e-02,  8.2190e-02,\n",
       "                        8.0521e-02,  1.9324e-02, -1.4214e-02,  7.8418e-02, -6.4482e-02,\n",
       "                       -2.6566e-02,  6.5938e-02,  5.7559e-02,  3.4610e-02,  5.9450e-02,\n",
       "                        8.0834e-02,  2.4266e-02,  3.7269e-02,  8.7672e-03, -4.0376e-02,\n",
       "                        8.7302e-02, -3.0556e-02,  7.3501e-02,  8.0434e-02, -3.4599e-02,\n",
       "                       -1.2774e-02,  5.4427e-02,  5.3475e-02,  2.3178e-02,  8.6164e-02,\n",
       "                        2.0878e-02, -5.0544e-02,  9.3034e-03, -8.3424e-02,  4.4598e-02,\n",
       "                        5.6846e-02, -6.0341e-02,  1.9618e-02, -3.3451e-02, -6.2010e-02,\n",
       "                       -4.4811e-02, -4.7706e-02,  3.2233e-02,  2.1411e-02, -2.0082e-02,\n",
       "                        1.8496e-02, -5.3121e-02,  8.2372e-02, -3.5555e-02, -4.3728e-02,\n",
       "                        4.9222e-02,  1.5583e-02, -4.3319e-03,  4.5401e-02, -4.6116e-02,\n",
       "                        7.4296e-03, -6.5540e-02,  4.5658e-02,  4.0397e-02,  6.0556e-02,\n",
       "                       -5.0959e-02, -3.8540e-02,  1.7603e-02, -8.0235e-02, -8.4952e-02,\n",
       "                       -6.9260e-02, -4.1963e-03, -5.0151e-02,  6.0944e-02,  3.0303e-02,\n",
       "                        6.4343e-02, -3.4754e-02,  2.7545e-03, -5.3949e-02, -8.8913e-02,\n",
       "                       -8.1469e-02,  2.8065e-02, -4.1741e-02,  5.5346e-05,  6.9607e-03,\n",
       "                       -7.9063e-02, -8.9097e-03, -1.2573e-02])),\n",
       "              ('regressor.target_model.gt_block.5.B.weight',\n",
       "               tensor([[ 0.1166, -0.0970, -0.1755,  ...,  0.0828,  0.0818,  0.0033],\n",
       "                       [ 0.1328, -0.3742,  0.0949,  ...,  0.1968, -0.2159,  0.2487],\n",
       "                       [-0.2498,  0.1313, -0.8387,  ...,  0.3305,  0.4902, -0.1828],\n",
       "                       ...,\n",
       "                       [-0.0930,  0.5291, -0.1255,  ..., -0.3431, -0.0300, -0.1759],\n",
       "                       [-0.2146,  0.0908,  0.7857,  ..., -0.1194, -0.1854,  0.0032],\n",
       "                       [-0.5621,  0.3055,  0.1297,  ...,  0.2202, -0.1594,  0.0193]])),\n",
       "              ('regressor.target_model.gt_block.5.B.bias',\n",
       "               tensor([ 7.1411e-01, -3.6178e-01, -3.9943e-02, -4.8692e-01, -1.0884e+00,\n",
       "                        7.7969e-01, -1.2885e-01,  1.5217e-01, -3.2852e-01, -1.4835e-01,\n",
       "                       -1.3414e-01, -4.8191e-01, -2.9363e-01, -5.4329e-01, -1.3842e-01,\n",
       "                       -2.9517e-01, -2.1837e-01,  1.1256e-01,  1.6460e-01,  4.8244e-01,\n",
       "                       -3.6073e-01,  4.3774e-01,  8.1277e-01,  3.0201e-01, -5.5895e-01,\n",
       "                       -7.0888e-01, -4.4221e-01,  2.8553e-01,  3.6599e-02,  8.5492e-01,\n",
       "                       -4.8259e-01,  1.8496e-01,  1.2292e+00,  4.2943e-01, -2.5771e-01,\n",
       "                        3.6958e-01,  4.1118e-01, -1.4100e-01, -1.1330e+00, -3.0031e-01,\n",
       "                       -5.0092e-01, -9.4010e-01, -1.0896e-01, -2.8286e-01, -9.2686e-01,\n",
       "                       -2.6801e-02, -9.6279e-02, -1.1454e-01,  4.3083e-02,  9.2131e-01,\n",
       "                        3.0239e-01,  7.4373e-02, -5.7610e-01, -2.7823e-02, -5.1756e-02,\n",
       "                       -1.0632e-01, -4.9590e-02, -1.5128e-01,  1.9238e-01,  9.4013e-01,\n",
       "                       -2.4843e-01,  2.6794e-02,  3.2650e-01,  4.2039e-01,  1.3511e+00,\n",
       "                        2.9253e-01,  2.4698e-01, -3.3144e-01, -1.1868e-01,  6.0589e-01,\n",
       "                       -1.1696e+00,  7.2825e-01,  2.0226e-01,  1.1652e-01, -1.1111e+00,\n",
       "                        9.1113e-02,  2.4468e-01, -9.1647e-03, -2.0645e-01, -5.8077e-02,\n",
       "                       -3.5728e-01, -5.0666e-01,  5.0371e-01,  6.6088e-03,  7.3468e-02,\n",
       "                        4.9018e-01, -2.1053e-01, -4.8797e-02, -2.0720e-01,  2.2979e+00,\n",
       "                       -1.5514e-01,  1.6558e-01,  1.6790e-01, -1.2317e-01,  2.0092e-01,\n",
       "                       -4.5643e-02, -9.8128e-01, -1.3215e-03,  6.1330e-01, -4.1978e-01,\n",
       "                        2.0144e-01, -2.0096e-01, -4.8856e-01,  3.2181e-01,  3.2450e+00,\n",
       "                        4.2069e-01, -1.5532e-01,  2.2460e-01,  3.2835e-01, -6.3371e-01,\n",
       "                        1.0044e+00,  4.0419e-02,  6.0766e-02,  2.5576e-01, -2.2888e-01,\n",
       "                       -4.6822e-01, -1.1605e-01, -2.1198e-01, -5.4707e-01, -4.5915e-01,\n",
       "                        7.5332e-02,  5.0981e-04,  2.4607e-01, -4.4096e-01,  4.2296e-01,\n",
       "                        9.6441e-02, -1.2295e-01, -6.9788e-01])),\n",
       "              ('regressor.target_model.gt_block.5.C.weight',\n",
       "               tensor([[ 0.2884,  0.0188, -0.2756,  ...,  0.0095,  0.1723, -0.0323],\n",
       "                       [-0.0917,  0.0549, -0.0475,  ...,  0.0627,  0.0940,  0.1068],\n",
       "                       [ 0.0275,  0.0968, -0.0403,  ..., -0.0170,  0.0217,  0.0824],\n",
       "                       ...,\n",
       "                       [ 0.3928, -0.1686,  0.0508,  ...,  0.0524,  0.1314,  0.1656],\n",
       "                       [ 0.2836,  0.1078,  0.0545,  ...,  0.0104, -0.3186, -0.4951],\n",
       "                       [-0.0818,  0.0102, -0.0547,  ..., -0.0289,  0.2297, -0.0527]])),\n",
       "              ('regressor.target_model.gt_block.5.C.bias',\n",
       "               tensor([ 0.0148, -0.2944, -0.2560, -0.1091,  0.1444, -0.0040, -0.3878,  0.0714,\n",
       "                        0.0366,  0.0690,  0.0708, -0.1294, -0.1106, -0.1077,  0.0055,  0.1259,\n",
       "                       -0.1816,  0.0114, -0.1179, -0.0776, -0.1443,  0.0490, -0.1669, -0.0126,\n",
       "                       -0.0674,  0.1740,  0.0780,  0.1973, -0.2203, -0.2061, -0.0434, -0.4208,\n",
       "                       -0.0130, -0.6218, -0.0364,  0.1841, -0.0795, -0.3243,  0.0696, -0.0627,\n",
       "                        0.1825, -0.0034,  0.0310,  0.1000,  0.0838, -0.0495, -0.0454,  0.2605,\n",
       "                       -0.1126, -0.0718, -0.1228, -0.0670, -0.0058,  0.0216, -0.1049, -0.1119,\n",
       "                       -0.0537,  0.1646, -0.0587, -0.2310,  0.2069, -0.0650, -0.8352, -0.1107,\n",
       "                       -0.1405, -0.0832, -0.4664, -0.3843, -0.0216, -0.0954, -0.0582,  0.1796,\n",
       "                       -0.2777,  0.1230,  0.0473, -0.0795, -0.2595, -0.0064, -0.2058, -0.1523,\n",
       "                       -0.0223, -0.0440, -0.0465, -0.3055, -0.0387, -0.0537, -0.2643,  0.0668,\n",
       "                        0.2353,  0.0986, -0.0642,  0.0646, -0.1920, -0.0124, -0.1990, -0.0120,\n",
       "                       -0.1217,  0.0958,  0.0227, -0.0617, -0.0589,  0.0668,  0.0039,  0.0914,\n",
       "                       -0.1444, -0.0509,  0.0806,  0.1083,  0.1320, -0.1312, -0.0313,  0.0038,\n",
       "                        0.1310,  0.0235, -0.1886,  0.1852, -0.0301,  0.0854, -0.0648, -0.2572,\n",
       "                       -0.0258, -0.3022, -0.1439, -0.0827, -0.1254, -0.0556,  0.0017, -0.0089])),\n",
       "              ('regressor.target_model.gt_block.5.D.weight',\n",
       "               tensor([[-0.5901, -0.0317,  0.1896,  ..., -0.2317,  0.4454, -0.0526],\n",
       "                       [ 0.1567, -0.6520,  0.0396,  ...,  0.2936,  0.0486, -0.1212],\n",
       "                       [ 0.1875,  0.0097,  0.4326,  ..., -0.0759, -0.5453,  0.1032],\n",
       "                       ...,\n",
       "                       [ 0.1440, -0.1560, -0.1256,  ...,  0.5764, -0.1392, -0.1312],\n",
       "                       [-0.4726, -0.1496, -0.0405,  ...,  0.1779, -0.1235,  0.0811],\n",
       "                       [-0.3474, -0.4499,  0.1237,  ...,  0.1145, -0.0092, -0.1018]])),\n",
       "              ('regressor.target_model.gt_block.5.D.bias',\n",
       "               tensor([ 0.0489, -0.3861, -0.1534, -0.0434,  0.0979, -0.1269, -0.3520,  0.0256,\n",
       "                       -0.0636,  0.1103,  0.2165, -0.0319, -0.0779, -0.1044,  0.0181,  0.1367,\n",
       "                       -0.0994, -0.0645, -0.0424, -0.0286, -0.0026,  0.1176, -0.0124, -0.0586,\n",
       "                       -0.0552,  0.1276,  0.0405,  0.1070, -0.1867, -0.2173, -0.1402, -0.4603,\n",
       "                        0.0835, -0.6432,  0.0764,  0.2718, -0.0332, -0.3688, -0.0161, -0.0411,\n",
       "                        0.1782, -0.0588, -0.0277,  0.1217,  0.1131, -0.0756, -0.0642,  0.2061,\n",
       "                       -0.2318, -0.1750, -0.0557, -0.0663, -0.0494, -0.1364, -0.0830, -0.0901,\n",
       "                       -0.1726,  0.2504,  0.0502, -0.2252,  0.2589, -0.0645, -0.7370, -0.0906,\n",
       "                       -0.2300, -0.0773, -0.4735, -0.3961, -0.1185, -0.1589, -0.0716,  0.1307,\n",
       "                       -0.2248,  0.0741,  0.0021, -0.0142, -0.1714, -0.0876, -0.0820, -0.0763,\n",
       "                       -0.0369, -0.0362, -0.0291, -0.2324, -0.0551, -0.0569, -0.2960,  0.0610,\n",
       "                        0.2123,  0.0058, -0.1099,  0.0938, -0.2176,  0.0318, -0.2307,  0.0078,\n",
       "                       -0.1879,  0.1410,  0.0768, -0.0370,  0.0321,  0.0707, -0.0638,  0.0623,\n",
       "                       -0.2183,  0.0422,  0.0704,  0.0109,  0.0906, -0.0440,  0.0043,  0.0563,\n",
       "                        0.0490, -0.0181, -0.2539,  0.1326,  0.0695, -0.0485, -0.1261, -0.2610,\n",
       "                       -0.0133, -0.3259, -0.2369, -0.1512, -0.2003, -0.0890, -0.0457, -0.1132])),\n",
       "              ('regressor.target_model.gt_block.5.E.weight',\n",
       "               tensor([[-0.0126,  0.0344,  0.0053,  ..., -0.0258, -0.0924,  0.0271],\n",
       "                       [-0.1085,  0.0498, -0.0666,  ..., -0.1097,  0.0190,  0.0091],\n",
       "                       [-0.1119, -0.0192, -0.0832,  ..., -0.0417,  0.0384,  0.0663],\n",
       "                       ...,\n",
       "                       [ 0.0014,  0.1264,  0.0852,  ...,  0.1338, -0.1043,  0.0508],\n",
       "                       [-0.0794,  0.0505, -0.0407,  ...,  0.1637, -0.0072, -0.0026],\n",
       "                       [-0.0822,  0.0428,  0.0322,  ..., -0.2637,  0.0943, -0.1715]])),\n",
       "              ('regressor.target_model.gt_block.5.E.bias',\n",
       "               tensor([ 0.1671, -0.2781, -0.2758, -0.0135,  0.0726, -0.0647, -0.3617,  0.0127,\n",
       "                        0.0009,  0.0916,  0.1019, -0.0473, -0.2404, -0.1001,  0.0062,  0.0602,\n",
       "                       -0.0261, -0.0508, -0.0359,  0.0125, -0.1174,  0.1921, -0.0777,  0.0523,\n",
       "                       -0.0544,  0.1207,  0.0768,  0.1330, -0.2279, -0.0671, -0.0096, -0.4367,\n",
       "                        0.0090, -0.6017, -0.0493,  0.2654,  0.0251, -0.3683,  0.0453, -0.0011,\n",
       "                        0.2385,  0.0457,  0.0140,  0.1438,  0.1153, -0.0389,  0.0093,  0.1914,\n",
       "                       -0.1435, -0.1670, -0.1412, -0.1566,  0.0044, -0.0932, -0.1416, -0.0889,\n",
       "                       -0.0562,  0.1747, -0.0705, -0.0889,  0.2350, -0.0827, -0.8172, -0.0624,\n",
       "                       -0.1215, -0.1608, -0.3960, -0.3483,  0.0077, -0.1676,  0.0137,  0.2515,\n",
       "                       -0.1466,  0.0750,  0.0660, -0.0061, -0.1554,  0.0094, -0.1541, -0.1105,\n",
       "                       -0.0353,  0.0606, -0.1819, -0.1877, -0.0426, -0.0855, -0.3353, -0.0360,\n",
       "                        0.1204,  0.1166, -0.0142, -0.0184, -0.1668, -0.1161, -0.1635, -0.1230,\n",
       "                       -0.1584, -0.0199,  0.1898,  0.0305,  0.0785,  0.0806, -0.1296,  0.0703,\n",
       "                       -0.0848,  0.0649,  0.0683,  0.0328,  0.1518, -0.0161,  0.0435,  0.1448,\n",
       "                        0.0441,  0.0349, -0.1567,  0.0515,  0.0164,  0.0603, -0.0585, -0.1978,\n",
       "                        0.0860, -0.3195, -0.1898, -0.1593, -0.0731, -0.0047,  0.0028, -0.0909])),\n",
       "              ('regressor.target_model.gt_block.5.bn_node_x.weight',\n",
       "               tensor([2.4808, 1.6903, 1.1973, 1.4612, 1.5611, 1.9852, 1.5074, 1.9843, 1.6450,\n",
       "                       1.8271, 2.5526, 1.6118, 1.3758, 0.8132, 1.1493, 1.4448, 1.1732, 1.2469,\n",
       "                       1.4852, 1.9642, 2.8386, 2.3182, 1.9861, 1.1584, 0.9839, 2.3160, 1.0145,\n",
       "                       1.0576, 1.2825, 1.2155, 1.3845, 1.5694, 1.4335, 1.1586, 1.3441, 1.3103,\n",
       "                       1.7245, 2.2794, 1.7906, 1.6235, 2.3703, 2.2542, 1.0977, 1.5763, 3.0448,\n",
       "                       1.3561, 1.3340, 1.4036, 1.2084, 1.4030, 1.7456, 1.0724, 1.2013, 1.9785,\n",
       "                       1.6026, 1.2533, 1.6227, 1.1860, 1.2555, 3.2187, 1.5032, 0.9851, 0.8335,\n",
       "                       1.4281, 2.0775, 1.3715, 1.5130, 1.4171, 1.6231, 1.5776, 0.8329, 2.2816,\n",
       "                       1.7733, 1.5064, 2.1320, 1.6787, 1.4587, 1.1523, 1.5657, 1.5103, 1.3208,\n",
       "                       1.4925, 2.0513, 1.0734, 1.3301, 2.0592, 1.2672, 1.3767, 1.4456, 1.2809,\n",
       "                       2.0313, 1.6176, 1.4408, 1.5387, 2.2504, 1.2433, 1.7289, 1.5558, 2.2579,\n",
       "                       1.2195, 1.5602, 1.5082, 0.9754, 1.2506, 1.8872, 1.6974, 1.4627, 1.6929,\n",
       "                       1.2277, 1.2628, 0.9209, 2.1792, 1.3872, 1.2492, 1.5022, 1.2977, 0.9428,\n",
       "                       1.3611, 1.6559, 2.4767, 1.3336, 1.3351, 2.1031, 1.3852, 2.4838, 1.4593,\n",
       "                       1.2913, 1.5051])),\n",
       "              ('regressor.target_model.gt_block.5.bn_node_x.bias',\n",
       "               tensor([ 4.8986e-01, -7.5849e-01, -1.7107e-01,  1.6056e-01, -2.9201e-01,\n",
       "                       -4.0944e-01, -8.8362e-01, -7.6380e-02, -8.2523e-01, -6.5972e-01,\n",
       "                        1.6082e-01, -2.6691e-01, -4.2846e-01, -6.0674e-02, -5.8530e-01,\n",
       "                        1.7398e-01, -7.8245e-01, -1.2746e+00, -3.1575e-01, -5.6213e-01,\n",
       "                        9.5472e-02,  2.0218e-01,  7.1132e-01, -6.5237e-01, -1.2713e-02,\n",
       "                       -5.0600e-01, -7.5502e-01, -1.3978e+00, -5.6205e-01, -1.2463e+00,\n",
       "                       -2.1523e-01,  2.0927e-01, -1.0710e+00, -9.6323e-01, -5.5944e-01,\n",
       "                       -7.4035e-01, -4.5187e-01,  1.4495e-01, -4.3884e-01, -9.5502e-01,\n",
       "                        1.6310e-01, -8.5784e-01, -6.0651e-01, -6.0198e-01, -1.0133e-01,\n",
       "                       -8.6924e-01, -1.2531e+00, -6.7472e-01, -8.2184e-01,  9.7757e-02,\n",
       "                       -9.8245e-02, -7.2626e-01,  2.1162e-03, -2.6691e-01, -6.8385e-01,\n",
       "                       -4.4942e-02, -6.7240e-01, -5.5377e-01, -1.1252e+00,  4.1627e-01,\n",
       "                       -6.5990e-01, -3.8053e-01, -7.8183e-01, -8.2691e-01, -5.7398e-01,\n",
       "                       -6.2878e-01,  4.7035e-01, -4.7607e-01, -2.0474e-01, -1.0576e+00,\n",
       "                       -6.8222e-01,  1.3412e-01,  2.4735e-01, -3.9522e-01, -2.0875e-01,\n",
       "                        7.2048e-01, -1.6145e-01, -5.7476e-01,  4.6927e-03, -8.4160e-01,\n",
       "                       -1.1363e+00,  4.0060e-02, -2.8616e-01, -6.6863e-02, -6.8279e-01,\n",
       "                       -6.3007e-01, -6.9089e-01, -1.0754e+00, -2.2381e-01, -9.1544e-01,\n",
       "                        2.3354e-01, -6.1729e-01,  2.5650e-01,  8.3891e-02,  4.0953e-01,\n",
       "                       -9.7653e-01, -1.6248e-01, -8.7091e-01, -6.5057e-01, -3.3253e-01,\n",
       "                       -9.9440e-01, -1.5058e+00, -1.4584e-02, -9.2952e-01, -2.1400e+00,\n",
       "                       -6.5688e-01, -3.2279e-01, -1.2241e+00, -1.7427e+00, -1.9583e-01,\n",
       "                       -3.1344e-01, -5.9774e-01, -5.6686e-01,  2.8577e-01, -8.3408e-01,\n",
       "                       -1.8023e-01, -1.6576e-01, -4.3306e-01, -1.8434e-01, -2.3092e-01,\n",
       "                       -4.9963e-01, -2.5799e-01,  3.2027e-02, -2.3087e-01,  2.2184e-01,\n",
       "                       -9.1862e-01, -1.1141e+00, -3.0663e-01])),\n",
       "              ('regressor.target_model.gt_block.5.bn_node_x.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "              ('regressor.target_model.gt_block.5.bn_node_x.running_var',\n",
       "               tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1.])),\n",
       "              ('regressor.target_model.gt_block.5.bn_node_x.num_batches_tracked',\n",
       "               tensor(0)),\n",
       "              ('regressor.target_model.gt_block.5.bn_edge_e.weight',\n",
       "               tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1.])),\n",
       "              ('regressor.target_model.gt_block.5.bn_edge_e.bias',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "              ('regressor.target_model.gt_block.5.bn_edge_e.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "              ('regressor.target_model.gt_block.5.bn_edge_e.running_var',\n",
       "               tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1.])),\n",
       "              ('regressor.target_model.gt_block.5.bn_edge_e.num_batches_tracked',\n",
       "               tensor(0)),\n",
       "              ('regressor.MLP.0.weight',\n",
       "               tensor([[-0.0308,  0.1801,  0.0400,  ..., -0.3532, -0.0697,  0.1582],\n",
       "                       [ 0.0535, -0.0004,  0.0943,  ..., -0.0693, -0.0412,  0.0478],\n",
       "                       [ 0.0899, -0.1038, -0.0824,  ..., -0.2172,  0.0806,  0.1117],\n",
       "                       ...,\n",
       "                       [-0.0392, -0.0709, -0.2245,  ..., -0.3981, -0.3008, -0.2351],\n",
       "                       [ 0.0450,  0.1717,  0.0816,  ..., -0.3265, -0.1211, -0.1565],\n",
       "                       [ 0.1411,  0.1908,  0.0576,  ...,  0.0938,  0.1248,  0.1824]])),\n",
       "              ('regressor.MLP.0.bias',\n",
       "               tensor([ 5.2406e-03,  5.9172e-02, -4.4396e-02,  6.1725e-02, -1.1355e-02,\n",
       "                        3.5655e-02,  3.5936e-02, -5.1323e-03,  3.4733e-02, -1.1081e-02,\n",
       "                       -4.0774e-02, -4.8439e-02,  1.6911e-02, -1.1442e-02, -4.3544e-02,\n",
       "                        5.7245e-02,  1.5281e-02, -2.5576e-02, -4.8954e-03,  5.1937e-02,\n",
       "                       -4.2291e-02,  6.1863e-02,  1.1740e-02, -6.5018e-03, -5.5505e-03,\n",
       "                       -3.6110e-02,  5.3030e-02,  4.1724e-02,  5.3120e-04,  1.9284e-02,\n",
       "                        2.4658e-02, -5.1626e-02, -1.3097e-02, -5.2555e-02,  3.2634e-02,\n",
       "                       -1.9614e-02,  3.5004e-03, -1.0288e-02, -5.1466e-02,  2.2624e-02,\n",
       "                       -9.4545e-03, -1.9816e-02,  4.8963e-02,  3.6299e-02,  4.1414e-02,\n",
       "                       -5.4399e-02,  6.2700e-02, -8.0126e-03,  5.1446e-02,  2.7526e-02,\n",
       "                       -6.9479e-03,  5.3700e-03, -1.0877e-02, -6.0732e-02,  2.9282e-02,\n",
       "                       -7.9270e-05, -3.6312e-02, -5.1957e-03, -2.2038e-02, -1.5690e-03,\n",
       "                        1.8674e-02, -2.1382e-02, -2.5547e-02, -2.3761e-02, -6.0233e-02,\n",
       "                        4.3874e-02,  6.1217e-02,  5.0008e-02, -1.7325e-03,  4.3123e-02,\n",
       "                        2.2901e-02,  1.1721e-02,  5.4116e-02, -2.8598e-02,  5.8939e-02,\n",
       "                       -5.2932e-02,  3.1161e-02, -1.3109e-02, -3.7898e-02,  1.6392e-02,\n",
       "                       -1.7180e-02, -4.6794e-02, -3.0833e-02, -2.0752e-02,  1.0277e-02,\n",
       "                       -1.1722e-02,  8.7101e-05,  4.5862e-02, -4.1037e-02, -2.2572e-02,\n",
       "                        3.5823e-02, -4.9265e-02, -5.9598e-02, -4.1072e-02, -2.8920e-02,\n",
       "                        3.4218e-02,  1.2975e-02,  2.3520e-02,  2.1724e-02, -5.9813e-02,\n",
       "                        3.4247e-02,  1.6698e-02, -3.6410e-02, -3.2929e-02,  5.3323e-02,\n",
       "                        5.8487e-02,  3.0848e-02,  4.9079e-02, -2.5483e-02, -3.5299e-02,\n",
       "                        3.6498e-02, -4.8401e-02,  1.6376e-02,  4.8298e-03,  2.6206e-02,\n",
       "                        3.9718e-02,  3.8039e-02,  1.7228e-04,  5.1189e-03,  5.5737e-02,\n",
       "                       -6.3034e-03,  4.1282e-02,  5.1524e-02, -1.8379e-03,  4.6526e-02,\n",
       "                       -3.6403e-02,  2.1742e-03,  2.1041e-02])),\n",
       "              ('regressor.MLP.1.weight',\n",
       "               tensor([0.4737, 5.6560, 6.2534, 5.4941, 3.1353, 0.3619, 5.1477, 3.7843, 4.1825,\n",
       "                       4.6571, 7.6877, 1.0413, 5.0042, 1.0893, 5.9625, 4.4757, 0.4557, 5.2418,\n",
       "                       2.6159, 1.5102, 4.3087, 9.3164, 0.9816, 5.8122, 4.8369, 6.5884, 3.8253,\n",
       "                       4.2317, 1.3993, 4.6381, 7.6327, 1.6663, 0.3750, 0.4325, 5.6809, 7.4232,\n",
       "                       2.4333, 0.6841, 6.2147, 4.5623, 3.8312, 7.5890, 5.9757, 4.8475, 4.3674,\n",
       "                       0.3968, 0.4309, 1.0579, 0.7995, 3.8224, 3.0186, 4.7572, 2.2381, 3.0340,\n",
       "                       8.6895, 4.7538, 0.4634, 0.5353, 0.3933, 3.6485, 4.0584, 4.7422, 3.8283,\n",
       "                       2.2514, 0.4300, 5.5919, 4.2314, 2.4379, 8.8472, 5.3529, 0.4248, 1.4706,\n",
       "                       3.6325, 7.7735, 4.2092, 6.0430, 0.4356, 6.9672, 5.4693, 2.9834, 2.5634,\n",
       "                       3.1933, 5.2670, 3.1451, 5.7284, 4.6446, 3.6599, 3.4539, 4.4801, 0.4957,\n",
       "                       3.8243, 4.5761, 0.4721, 0.3245, 0.5463, 6.8336, 7.8301, 3.4978, 0.7725,\n",
       "                       8.7260, 6.1220, 1.5171, 4.5240, 4.8936, 5.0255, 1.5841, 0.8518, 2.1653,\n",
       "                       3.5437, 3.6491, 5.4048, 5.4742, 7.9696, 4.4038, 2.7477, 4.8614, 0.3301,\n",
       "                       0.3759, 4.7119, 0.7604, 5.3271, 1.3991, 0.2792, 0.4045, 1.2166, 4.9741,\n",
       "                       7.0069, 3.9003])),\n",
       "              ('regressor.MLP.1.bias',\n",
       "               tensor([ 0.2896, -2.9066, -1.4062, -3.4407, -1.0474,  0.2761, -3.6838, -0.3665,\n",
       "                       -1.8162, -3.2144,  1.1252, -0.0266,  0.8379, -0.1421, -0.1825, -2.3956,\n",
       "                        0.3176, -2.8931, -0.6740, -0.4319, -3.4504, -1.0404, -0.5935, -3.5328,\n",
       "                       -2.7546, -0.1267, -1.4815,  0.9323,  0.0576, -2.5017,  0.5218, -0.1262,\n",
       "                        0.3158,  0.0433, -3.9957, -0.3876, -1.6652, -0.0177,  0.5405, -2.0113,\n",
       "                       -1.9771, -0.7110, -1.3145, -2.9102, -3.0638,  0.1529, -0.1436, -0.4634,\n",
       "                        0.6208, -2.2512,  0.5430, -2.7075, -0.4438, -1.8136, -0.1641,  0.8674,\n",
       "                        0.0857,  0.0054,  0.0514, -0.2244, -2.6473, -2.7318, -2.4763,  0.0223,\n",
       "                        0.3487, -2.8015, -2.1181, -0.9991,  0.2278, -4.2766,  0.3209, -1.3475,\n",
       "                       -0.8047, -1.1976, -3.2381,  1.6117, -0.1447,  0.1927, -0.5920, -2.1107,\n",
       "                       -0.5957, -1.7091, -2.1632, -2.8360, -0.5662, -0.0594, -2.4394, -1.5776,\n",
       "                       -2.9715,  0.5389, -2.7470, -1.4982,  0.3083,  0.0386,  0.6553, -0.9828,\n",
       "                       -0.8064,  0.5465,  0.2734,  0.7942, -2.0194,  0.3577, -2.8561, -3.4088,\n",
       "                       -1.4841,  0.2804,  0.8155,  0.1519, -2.0057,  0.1138, -2.4073, -3.8549,\n",
       "                       -0.5087, -4.0338, -1.0952, -3.2090,  0.1134,  0.0164,  0.1744,  0.0869,\n",
       "                       -1.8825, -0.1042,  0.0478,  0.2020,  0.1331, -3.1625,  0.7992, -3.9059])),\n",
       "              ('regressor.MLP.1.running_mean',\n",
       "               tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                       0., 0., 0., 0., 0., 0., 0., 0.])),\n",
       "              ('regressor.MLP.1.running_var',\n",
       "               tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                       1., 1.])),\n",
       "              ('regressor.MLP.1.num_batches_tracked', tensor(0)),\n",
       "              ('regressor.z_pi.weight',\n",
       "               tensor([[ 0.2154,  0.1207,  0.0635,  ...,  0.1034, -0.0245,  0.0710],\n",
       "                       [-0.2068,  0.1389, -0.0149,  ...,  0.1123, -0.1259,  0.0967],\n",
       "                       [-0.0109, -0.0344,  0.1222,  ...,  0.0038,  0.0806, -0.0474],\n",
       "                       ...,\n",
       "                       [ 0.0075, -0.0006,  0.1297,  ...,  0.0340,  0.0666,  0.0533],\n",
       "                       [ 0.1874,  0.1618, -0.0721,  ...,  0.1173, -0.1367,  0.0734],\n",
       "                       [-0.2640,  0.0500,  0.0820,  ...,  0.0644,  0.0258,  0.1082]])),\n",
       "              ('regressor.z_pi.bias',\n",
       "               tensor([ 0.1648, -0.3755, -0.1387, -0.3033, -0.4921, -3.7139,  0.2417,  0.9387,\n",
       "                       -1.6226,  0.5710])),\n",
       "              ('regressor.z_sigma.weight',\n",
       "               tensor([[ 0.1966, -0.0065, -0.0007,  ..., -0.0064,  0.0022, -0.0061],\n",
       "                       [ 0.0507, -0.0078,  0.0111,  ..., -0.0089,  0.0032, -0.0065],\n",
       "                       [ 0.0221, -0.0065, -0.0092,  ...,  0.0256, -0.0203,  0.0075],\n",
       "                       ...,\n",
       "                       [-0.0445,  0.0034, -0.0095,  ...,  0.0185, -0.0012,  0.0249],\n",
       "                       [-0.0103, -0.0018,  0.0059,  ..., -0.0012,  0.0111, -0.0074],\n",
       "                       [-0.0010, -0.0053, -0.0027,  ..., -0.0141, -0.0050,  0.0026]])),\n",
       "              ('regressor.z_sigma.bias',\n",
       "               tensor([-1.3213, -1.4151, -1.7561, -1.9866, -1.3405, -0.1831, -1.4538, -1.2238,\n",
       "                       -1.8681, -1.2167])),\n",
       "              ('regressor.z_mu.weight',\n",
       "               tensor([[ 0.1175, -0.0100,  0.0169,  ..., -0.0157,  0.0158, -0.0057],\n",
       "                       [ 0.0117, -0.0023,  0.0164,  ..., -0.0038,  0.0112, -0.0020],\n",
       "                       [ 0.0068, -0.0033, -0.0007,  ..., -0.0020,  0.0032, -0.0066],\n",
       "                       ...,\n",
       "                       [ 0.0104,  0.0016,  0.0031,  ..., -0.0025,  0.0005, -0.0083],\n",
       "                       [ 0.0027, -0.0012,  0.0081,  ..., -0.0012,  0.0069,  0.0007],\n",
       "                       [ 0.0169, -0.0128,  0.0109,  ..., -0.0287,  0.0116, -0.0044]])),\n",
       "              ('regressor.z_mu.bias',\n",
       "               tensor([3.9047, 3.1303, 5.4534, 5.8405, 3.2364, 1.3036, 4.4272, 5.1297, 2.7570,\n",
       "                       4.1724])),\n",
       "              ('regressor.atom_types.weight',\n",
       "               tensor([[ 6.0937e-02,  1.1579e-01,  1.8028e-01,  ...,  1.2027e-01,\n",
       "                         1.8706e-01, -2.4232e-02],\n",
       "                       [ 6.3865e-01,  7.3800e-02, -1.3485e-01,  ..., -6.3689e-01,\n",
       "                        -3.3370e-01, -3.2365e-01],\n",
       "                       [-1.9980e-01,  6.2776e-02,  1.2272e-01,  ...,  1.7549e-01,\n",
       "                         3.0488e-01,  1.5298e-01],\n",
       "                       ...,\n",
       "                       [-3.5279e-04, -1.0183e-01, -1.2792e-01,  ..., -8.1483e-02,\n",
       "                        -7.2053e-02, -6.2029e-02],\n",
       "                       [-1.0284e-02,  1.2279e-02, -1.6103e-01,  ..., -1.7378e-01,\n",
       "                        -9.2940e-02, -5.5460e-02],\n",
       "                       [-7.1023e-02,  5.8346e-02, -1.9981e-02,  ..., -7.7796e-02,\n",
       "                         2.0286e-02, -3.8714e-01]])),\n",
       "              ('regressor.atom_types.bias',\n",
       "               tensor([ 3.3862, -1.8394, -2.1454, -0.7288, -0.6168, -0.2096, -0.1341, -0.2652,\n",
       "                       -0.2489, -0.3175, -0.0713, -0.5768, -0.0535, -0.0925, -0.1451, -0.0175,\n",
       "                       -0.5090])),\n",
       "              ('regressor.bond_types.weight',\n",
       "               tensor([[ 0.0351,  0.0268,  0.0762,  ...,  0.1623, -0.1032,  0.0491],\n",
       "                       [-0.2113,  0.1957, -0.0808,  ..., -0.0395, -0.1817, -0.0646],\n",
       "                       [-0.1354,  0.4306, -0.4048,  ...,  0.3035, -0.2604, -0.1834],\n",
       "                       [ 0.0851, -0.2115, -0.0412,  ..., -0.1519,  0.2406, -0.1174]])),\n",
       "              ('regressor.bond_types.bias',\n",
       "               tensor([ 0.4744, -2.2646, -0.4044,  1.0557])),\n",
       "              ('lrs.0', tensor(1.0000e-04)),\n",
       "              ('lrs.1', tensor(5.5331)),\n",
       "              ('lrs.2', tensor(3.8277)),\n",
       "              ('lrs.3', tensor(3.3027)),\n",
       "              ('lrs.4', tensor(5.5408)),\n",
       "              ('lrs.5', tensor(0.0100)),\n",
       "              ('lrs.6', tensor(6.3837)),\n",
       "              ('lrs.7', tensor(0.0187)),\n",
       "              ('lrs.8', tensor(9.6496)),\n",
       "              ('lrs.9', tensor(3.0839)),\n",
       "              ('lrs.10', tensor(14.6229)),\n",
       "              ('lrs.11', tensor(3.0839)),\n",
       "              ('lrs.12', tensor(12.4577)),\n",
       "              ('lrs.13', tensor(3.0839)),\n",
       "              ('lrs.14', tensor(3.8295)),\n",
       "              ('lrs.15', tensor(3.9881)),\n",
       "              ('lrs.16', tensor(10.8208)),\n",
       "              ('lrs.17', tensor(10.6189)),\n",
       "              ('lrs.18', tensor(6.3662)),\n",
       "              ('lrs.19', tensor(0.0100)),\n",
       "              ('lrs.20', tensor(6.8202)),\n",
       "              ('lrs.21', tensor(0.4255)),\n",
       "              ('lrs.22', tensor(12.5958)),\n",
       "              ('lrs.23', tensor(1.9743)),\n",
       "              ('lrs.24', tensor(14.1590)),\n",
       "              ('lrs.25', tensor(1.9743)),\n",
       "              ('lrs.26', tensor(12.1574)),\n",
       "              ('lrs.27', tensor(1.9743)),\n",
       "              ('lrs.28', tensor(4.9868)),\n",
       "              ('lrs.29', tensor(3.7933)),\n",
       "              ('lrs.30', tensor(10.2610)),\n",
       "              ('lrs.31', tensor(9.1379)),\n",
       "              ('lrs.32', tensor(8.3197)),\n",
       "              ('lrs.33', tensor(0.0100)),\n",
       "              ('lrs.34', tensor(7.9628)),\n",
       "              ('lrs.35', tensor(1.0159)),\n",
       "              ('lrs.36', tensor(15.9635)),\n",
       "              ('lrs.37', tensor(3.1337)),\n",
       "              ('lrs.38', tensor(14.5509)),\n",
       "              ('lrs.39', tensor(3.1337)),\n",
       "              ('lrs.40', tensor(13.7422)),\n",
       "              ('lrs.41', tensor(3.1337)),\n",
       "              ('lrs.42', tensor(7.1073)),\n",
       "              ('lrs.43', tensor(4.5637)),\n",
       "              ('lrs.44', tensor(9.2569)),\n",
       "              ('lrs.45', tensor(7.9776)),\n",
       "              ('lrs.46', tensor(7.2465)),\n",
       "              ('lrs.47', tensor(0.0100)),\n",
       "              ('lrs.48', tensor(6.7098)),\n",
       "              ('lrs.49', tensor(2.7547)),\n",
       "              ('lrs.50', tensor(15.9165)),\n",
       "              ('lrs.51', tensor(1.4736)),\n",
       "              ('lrs.52', tensor(11.3715)),\n",
       "              ('lrs.53', tensor(1.4736)),\n",
       "              ('lrs.54', tensor(13.1101)),\n",
       "              ('lrs.55', tensor(1.4736)),\n",
       "              ('lrs.56', tensor(7.8351)),\n",
       "              ('lrs.57', tensor(4.9058)),\n",
       "              ('lrs.58', tensor(8.0304)),\n",
       "              ('lrs.59', tensor(6.6277)),\n",
       "              ('lrs.60', tensor(11.3259)),\n",
       "              ('lrs.61', tensor(0.0100)),\n",
       "              ('lrs.62', tensor(10.5952)),\n",
       "              ('lrs.63', tensor(3.0326)),\n",
       "              ('lrs.64', tensor(18.0154)),\n",
       "              ('lrs.65', tensor(1.7266)),\n",
       "              ('lrs.66', tensor(12.6505)),\n",
       "              ('lrs.67', tensor(1.7266)),\n",
       "              ('lrs.68', tensor(15.4212)),\n",
       "              ('lrs.69', tensor(1.7266)),\n",
       "              ('lrs.70', tensor(14.0655)),\n",
       "              ('lrs.71', tensor(9.1961)),\n",
       "              ('lrs.72', tensor(6.4888)),\n",
       "              ('lrs.73', tensor(5.3724)),\n",
       "              ('lrs.74', tensor(17.6900)),\n",
       "              ('lrs.75', tensor(0.0100)),\n",
       "              ('lrs.76', tensor(16.7107)),\n",
       "              ('lrs.77', tensor(6.0174)),\n",
       "              ('lrs.78', tensor(18.4521)),\n",
       "              ('lrs.79', tensor(1.4493)),\n",
       "              ('lrs.80', tensor(14.4688)),\n",
       "              ('lrs.81', tensor(1.4493)),\n",
       "              ('lrs.82', tensor(19.3486)),\n",
       "              ('lrs.83', tensor(1.4493)),\n",
       "              ('lrs.84', tensor(17.4597)),\n",
       "              ('lrs.85', tensor(12.6901)),\n",
       "              ('lrs.86', tensor(0.0100)),\n",
       "              ('lrs.87', tensor(0.0100)),\n",
       "              ('lrs.88', tensor(0.0348)),\n",
       "              ('lrs.89', tensor(1.5935)),\n",
       "              ('lrs.90', tensor(0.0138)),\n",
       "              ('lrs.91', tensor(0.3944)),\n",
       "              ('lrs.92', tensor(3.9648)),\n",
       "              ('lrs.93', tensor(0.0100)),\n",
       "              ('lrs.94', tensor(4.7133)),\n",
       "              ('lrs.95', tensor(0.0172)),\n",
       "              ('lrs.96', tensor(3.9482)),\n",
       "              ('lrs.97', tensor(0.6363)),\n",
       "              ('lrs.98', tensor(4.0026)),\n",
       "              ('lrs.99', tensor(0.6363)),\n",
       "              ('lrs.100', tensor(6.0537)),\n",
       "              ('lrs.101', tensor(0.6363)),\n",
       "              ('lrs.102', tensor(2.0829)),\n",
       "              ('lrs.103', tensor(2.3522)),\n",
       "              ('lrs.104', tensor(2.1606)),\n",
       "              ('lrs.105', tensor(3.2023)),\n",
       "              ('lrs.106', tensor(2.3663)),\n",
       "              ('lrs.107', tensor(0.0100)),\n",
       "              ('lrs.108', tensor(3.1221)),\n",
       "              ('lrs.109', tensor(0.0115)),\n",
       "              ('lrs.110', tensor(2.6402)),\n",
       "              ('lrs.111', tensor(1.3560)),\n",
       "              ('lrs.112', tensor(1.4801)),\n",
       "              ('lrs.113', tensor(1.3560)),\n",
       "              ('lrs.114', tensor(4.1819)),\n",
       "              ('lrs.115', tensor(1.3560)),\n",
       "              ('lrs.116', tensor(2.3975)),\n",
       "              ('lrs.117', tensor(1.9891)),\n",
       "              ('lrs.118', tensor(1.6419)),\n",
       "              ('lrs.119', tensor(0.4998)),\n",
       "              ('lrs.120', tensor(0.7205)),\n",
       "              ('lrs.121', tensor(0.0100)),\n",
       "              ('lrs.122', tensor(1.8146)),\n",
       "              ('lrs.123', tensor(1.0000e-04)),\n",
       "              ('lrs.124', tensor(0.3985)),\n",
       "              ('lrs.125', tensor(1.0000e-04)),\n",
       "              ('lrs.126', tensor(1.0000e-04)),\n",
       "              ('lrs.127', tensor(1.0000e-04)),\n",
       "              ('lrs.128', tensor(0.1294)),\n",
       "              ('lrs.129', tensor(1.0000e-04)),\n",
       "              ('lrs.130', tensor(2.6068)),\n",
       "              ('lrs.131', tensor(2.1754)),\n",
       "              ('lrs.132', tensor(2.7555)),\n",
       "              ('lrs.133', tensor(2.3584)),\n",
       "              ('lrs.134', tensor(1.5262)),\n",
       "              ('lrs.135', tensor(0.0100)),\n",
       "              ('lrs.136', tensor(0.0001)),\n",
       "              ('lrs.137', tensor(0.4018)),\n",
       "              ('lrs.138', tensor(4.5314)),\n",
       "              ('lrs.139', tensor(0.0028)),\n",
       "              ('lrs.140', tensor(2.6700)),\n",
       "              ('lrs.141', tensor(0.0028)),\n",
       "              ('lrs.142', tensor(1.1507)),\n",
       "              ('lrs.143', tensor(0.0028)),\n",
       "              ('lrs.144', tensor(3.2158)),\n",
       "              ('lrs.145', tensor(2.3751)),\n",
       "              ('lrs.146', tensor(0.0179)),\n",
       "              ('lrs.147', tensor(0.1070)),\n",
       "              ('lrs.148', tensor(1.8667)),\n",
       "              ('lrs.149', tensor(0.0100)),\n",
       "              ('lrs.150', tensor(0.0002)),\n",
       "              ('lrs.151', tensor(0.1268)),\n",
       "              ('lrs.152', tensor(3.7900)),\n",
       "              ('lrs.153', tensor(1.8994)),\n",
       "              ('lrs.154', tensor(2.7075)),\n",
       "              ('lrs.155', tensor(1.8994)),\n",
       "              ('lrs.156', tensor(1.0423)),\n",
       "              ('lrs.157', tensor(1.8994)),\n",
       "              ('lrs.158', tensor(2.3498)),\n",
       "              ('lrs.159', tensor(1.5716)),\n",
       "              ('lrs.160', tensor(1.0000e-04)),\n",
       "              ('lrs.161', tensor(0.0004)),\n",
       "              ('lrs.162', tensor(0.7773)),\n",
       "              ('lrs.163', tensor(0.0100)),\n",
       "              ('lrs.164', tensor(1.1001)),\n",
       "              ('lrs.165', tensor(2.2053)),\n",
       "              ('lrs.166', tensor(3.9388)),\n",
       "              ('lrs.167', tensor(2.5304)),\n",
       "              ('lrs.168', tensor(1.6926)),\n",
       "              ('lrs.169', tensor(2.5304)),\n",
       "              ('lrs.170', tensor(0.0353)),\n",
       "              ('lrs.171', tensor(2.5304)),\n",
       "              ('lrs.172', tensor(0.6192)),\n",
       "              ('lrs.173', tensor(0.0263)),\n",
       "              ('lrs.174', tensor(0.0100)),\n",
       "              ('lrs.175', tensor(0.0100)),\n",
       "              ('lrs.176', tensor(0.3068)),\n",
       "              ('lrs.177', tensor(0.0100)),\n",
       "              ('lrs.178', tensor(1.6931)),\n",
       "              ('lrs.179', tensor(0.9813)),\n",
       "              ('lrs.180', tensor(0.1069)),\n",
       "              ('lrs.181', tensor(0.0004)),\n",
       "              ('lrs.182', tensor(0.1164)),\n",
       "              ('lrs.183', tensor(0.1137)),\n",
       "              ('lrs.184', tensor(0.0001)),\n",
       "              ('lrs.185', tensor(0.0225)),\n",
       "              ('lrs.186', tensor(0.1197)),\n",
       "              ('lrs.187', tensor(0.0186)),\n",
       "              ('lrs.188', tensor(1.8785)),\n",
       "              ('lrs.189', tensor(0.0217))]),\n",
       " 'per_epoch_statistics': {'train_total_loss_mean': [np.float64(1.9050027090863753),\n",
       "   np.float64(1.521846811720296),\n",
       "   np.float64(1.4718752055463986),\n",
       "   np.float64(1.434602911877693),\n",
       "   np.float64(1.410266362989733),\n",
       "   np.float64(1.3835456546066585),\n",
       "   np.float64(1.3703739848951186),\n",
       "   np.float64(1.3615481713447866),\n",
       "   np.float64(1.3535707290928236),\n",
       "   np.float64(1.3434640771255306),\n",
       "   np.float64(1.3396694262971311),\n",
       "   np.float64(1.3330540206556487),\n",
       "   np.float64(1.3257397576067895),\n",
       "   np.float64(1.321092536177479),\n",
       "   np.float64(1.3145684192713594),\n",
       "   np.float64(1.314831837459501),\n",
       "   np.float64(1.3085630414658862),\n",
       "   np.float64(1.3028928993664919),\n",
       "   np.float64(1.2985461428558926),\n",
       "   np.float64(1.2953890566154616),\n",
       "   np.float64(1.2917461162148145),\n",
       "   np.float64(1.2846027128067545),\n",
       "   np.float64(1.2829532593136659),\n",
       "   np.float64(1.2753092972022104),\n",
       "   np.float64(1.2731083651377832),\n",
       "   np.float64(1.2703719857752105),\n",
       "   np.float64(1.2637695045456525),\n",
       "   np.float64(1.2620395388581496),\n",
       "   np.float64(1.2607733663644298),\n",
       "   np.float64(1.2525910533924627),\n",
       "   np.float64(1.2490927321556564),\n",
       "   np.float64(1.2476005632027374),\n",
       "   np.float64(1.2451298234524133),\n",
       "   np.float64(1.2401485337805593),\n",
       "   np.float64(1.2379895070880287),\n",
       "   np.float64(1.2283298229840427),\n",
       "   np.float64(1.225902949846209),\n",
       "   np.float64(1.2234269859904827),\n",
       "   np.float64(1.22302767418253),\n",
       "   np.float64(1.2162574555884584),\n",
       "   np.float64(1.2147546182725961),\n",
       "   np.float64(1.2136104823115181),\n",
       "   np.float64(1.2079457717591964),\n",
       "   np.float64(1.2040849485405767),\n",
       "   np.float64(1.2018342945508917),\n",
       "   np.float64(1.199548766955028),\n",
       "   np.float64(1.1936230288489886),\n",
       "   np.float64(1.1896861528217115),\n",
       "   np.float64(1.1791399617676341),\n",
       "   np.float64(1.1789846044125496),\n",
       "   np.float64(1.1803028946345189),\n",
       "   np.float64(1.1728629595527829),\n",
       "   np.float64(1.170818644204876),\n",
       "   np.float64(1.1673872379507975),\n",
       "   np.float64(1.1670668471552026),\n",
       "   np.float64(1.1679517285331158),\n",
       "   np.float64(1.1645252743508325),\n",
       "   np.float64(1.1605740043976756),\n",
       "   np.float64(1.1557418076107744),\n",
       "   np.float64(1.1518096180618864),\n",
       "   np.float64(1.1496516310409088),\n",
       "   np.float64(1.1442853796476014),\n",
       "   np.float64(1.1459774156488498),\n",
       "   np.float64(1.1471561148878877),\n",
       "   np.float64(1.1439285221128117),\n",
       "   np.float64(1.136180469167213),\n",
       "   np.float64(1.134650919698833),\n",
       "   np.float64(1.1312358215617797),\n",
       "   np.float64(1.1302576568024865),\n",
       "   np.float64(1.1298193793756057),\n",
       "   np.float64(1.1230197208546937),\n",
       "   np.float64(1.1223425218346974),\n",
       "   np.float64(1.11967973175377),\n",
       "   np.float64(1.1162407448684941),\n",
       "   np.float64(1.1136040591004264),\n",
       "   np.float64(1.11737965594412),\n",
       "   np.float64(1.1130259831358547),\n",
       "   np.float64(1.1142880021069523),\n",
       "   np.float64(1.1071499423305746),\n",
       "   np.float64(1.1019666535294257),\n",
       "   np.float64(1.1069227119127953),\n",
       "   np.float64(1.106639647655621),\n",
       "   np.float64(1.0985105892073086),\n",
       "   np.float64(1.101241377749275),\n",
       "   np.float64(1.1022588969486156),\n",
       "   np.float64(1.0922172712916989),\n",
       "   np.float64(1.0932870274747564),\n",
       "   np.float64(1.0934703049849306),\n",
       "   np.float64(1.0868159445329961),\n",
       "   np.float64(1.0876921139270928),\n",
       "   np.float64(1.08695499752087),\n",
       "   np.float64(1.0837383470089537),\n",
       "   np.float64(1.0856702279627204),\n",
       "   np.float64(1.0779972862847684),\n",
       "   np.float64(1.0824772993901366),\n",
       "   np.float64(1.082307293641733),\n",
       "   np.float64(1.0787776822143957),\n",
       "   np.float64(1.0722070594263438),\n",
       "   np.float64(1.0779554120360255),\n",
       "   np.float64(1.063825080566465),\n",
       "   np.float64(1.0613533451419561),\n",
       "   np.float64(1.0559936453949499),\n",
       "   np.float64(1.0576872081908),\n",
       "   np.float64(1.0561728441750178),\n",
       "   np.float64(1.0518482205293185),\n",
       "   np.float64(1.0590893152407808),\n",
       "   np.float64(1.0498957327966774),\n",
       "   np.float64(1.0510275228142878),\n",
       "   np.float64(1.0498305368289074),\n",
       "   np.float64(1.044771584591971),\n",
       "   np.float64(1.0441393279108926),\n",
       "   np.float64(1.0489085244127059),\n",
       "   np.float64(1.037208916385331),\n",
       "   np.float64(1.0357271413613902),\n",
       "   np.float64(1.0421446282934996),\n",
       "   np.float64(1.0386765174668187),\n",
       "   np.float64(1.0437712877172136),\n",
       "   np.float64(1.0356894627269029),\n",
       "   np.float64(1.0321742390353383),\n",
       "   np.float64(1.0343370538864705),\n",
       "   np.float64(1.0314995972715497),\n",
       "   np.float64(1.030460224238126),\n",
       "   np.float64(1.0303180937662348),\n",
       "   np.float64(1.0245032216959185),\n",
       "   np.float64(1.0239470165285425),\n",
       "   np.float64(1.0249973371751284),\n",
       "   np.float64(1.0268199560468037),\n",
       "   np.float64(1.0180707909006332),\n",
       "   np.float64(1.0242574925578767),\n",
       "   np.float64(1.0219665473732287),\n",
       "   np.float64(1.0147775055541353),\n",
       "   np.float64(1.0218566725821647),\n",
       "   np.float64(1.0136618246399254),\n",
       "   np.float64(1.0208836531912224),\n",
       "   np.float64(1.016526384306799),\n",
       "   np.float64(1.0166575958022728),\n",
       "   np.float64(1.0179449292209781),\n",
       "   np.float64(1.0190545420031),\n",
       "   np.float64(1.012834400419561),\n",
       "   np.float64(1.0157572637953072),\n",
       "   np.float64(1.0085017015261635),\n",
       "   np.float64(1.0034923341889208),\n",
       "   np.float64(1.0101461665525722),\n",
       "   np.float64(1.0085192914987011),\n",
       "   np.float64(1.007617030468501),\n",
       "   np.float64(1.003229533302762),\n",
       "   np.float64(1.0038591180959875),\n",
       "   np.float64(1.0003172575025534),\n",
       "   np.float64(1.0040581162079145),\n",
       "   np.float64(1.0067695399547372),\n",
       "   np.float64(0.9987124223650257),\n",
       "   np.float64(1.004186585579797),\n",
       "   np.float64(1.0020157413700885),\n",
       "   np.float64(1.0074644324166036),\n",
       "   np.float64(0.9978756986758678),\n",
       "   np.float64(0.9935554110177016),\n",
       "   np.float64(1.000117376110451),\n",
       "   np.float64(1.0037433427411944),\n",
       "   np.float64(0.9989408920014423),\n",
       "   np.float64(0.990425351016072),\n",
       "   np.float64(0.9931576910644702),\n",
       "   np.float64(0.9886981074288471),\n",
       "   np.float64(0.9960468848743234),\n",
       "   np.float64(0.9900587394005743),\n",
       "   np.float64(0.9899015471122682),\n",
       "   np.float64(0.996004272142688),\n",
       "   np.float64(0.9912695345571784),\n",
       "   np.float64(0.9870616962310542),\n",
       "   np.float64(0.9846441410586292),\n",
       "   np.float64(0.9895768084388868),\n",
       "   np.float64(0.9794750993216222),\n",
       "   np.float64(0.9834812401837253),\n",
       "   np.float64(0.9883578204215493),\n",
       "   np.float64(0.9896896535397248),\n",
       "   np.float64(0.9901152492874136),\n",
       "   np.float64(0.9874222946784728),\n",
       "   np.float64(0.9856429194458944),\n",
       "   np.float64(0.9785288571977577),\n",
       "   np.float64(0.9864243283121771),\n",
       "   np.float64(0.9777473250472842),\n",
       "   np.float64(0.9743378170784789),\n",
       "   np.float64(0.9843369353884804),\n",
       "   np.float64(0.9762712179184488),\n",
       "   np.float64(0.9782414084674061),\n",
       "   np.float64(0.9798144097653914),\n",
       "   np.float64(0.9764331967425008),\n",
       "   np.float64(0.9802263093595872),\n",
       "   np.float64(0.975278052559484),\n",
       "   np.float64(0.9731175205633547),\n",
       "   np.float64(0.977117887743201),\n",
       "   np.float64(0.9700712813030694),\n",
       "   np.float64(0.9631505977744722),\n",
       "   np.float64(0.9723982488007941),\n",
       "   np.float64(0.9709980948117615),\n",
       "   np.float64(0.9705742675448135),\n",
       "   np.float64(0.9673498226137082),\n",
       "   np.float64(0.9784326392047393),\n",
       "   np.float64(0.9666902532920243),\n",
       "   np.float64(0.9774520831162559),\n",
       "   np.float64(0.9684725633719841),\n",
       "   np.float64(0.9620610392665655),\n",
       "   np.float64(0.9696475044578444),\n",
       "   np.float64(0.9627217816811072),\n",
       "   np.float64(0.9573647512287415),\n",
       "   np.float64(0.9678981957399347),\n",
       "   np.float64(0.9638922108706293),\n",
       "   np.float64(0.9727426959899548),\n",
       "   np.float64(0.9600391681586824),\n",
       "   np.float64(0.9613312779372453),\n",
       "   np.float64(0.9633722029780174),\n",
       "   np.float64(0.9694001242572751),\n",
       "   np.float64(0.9710056782679262),\n",
       "   np.float64(0.9563526584682542),\n",
       "   np.float64(0.9552219718039678),\n",
       "   np.float64(0.9603640953548598),\n",
       "   np.float64(0.9612703439696834),\n",
       "   np.float64(0.9546171458041441),\n",
       "   np.float64(0.9581091932788545),\n",
       "   np.float64(0.960117041679811),\n",
       "   np.float64(0.9596491687236562),\n",
       "   np.float64(0.9574728815099148),\n",
       "   np.float64(0.9572733474741826),\n",
       "   np.float64(0.9640218376689081),\n",
       "   np.float64(0.9655056151715458),\n",
       "   np.float64(0.9502879420285214),\n",
       "   np.float64(0.9636508289162197),\n",
       "   np.float64(0.9576329156762837),\n",
       "   np.float64(0.9525854962015221),\n",
       "   np.float64(0.950600329751451),\n",
       "   np.float64(0.9480701279816793),\n",
       "   np.float64(0.9571149356061768),\n",
       "   np.float64(0.9448416625368384),\n",
       "   np.float64(0.9479699592636017),\n",
       "   np.float64(0.9553866453496055),\n",
       "   np.float64(0.9398213189555782),\n",
       "   np.float64(0.9530429197340314),\n",
       "   np.float64(0.9522052322026678),\n",
       "   np.float64(0.9516981172965042),\n",
       "   np.float64(0.9514949639048842),\n",
       "   np.float64(0.9499017729735795),\n",
       "   np.float64(0.9528734303325771),\n",
       "   np.float64(0.9438171728435488),\n",
       "   np.float64(0.9496813673582887),\n",
       "   np.float64(0.9568307205556372),\n",
       "   np.float64(0.942319827308368),\n",
       "   np.float64(0.9481157721926564),\n",
       "   np.float64(0.9478038417646455),\n",
       "   np.float64(0.9498323610577237),\n",
       "   np.float64(0.94473000006488),\n",
       "   np.float64(0.9523934526935942),\n",
       "   np.float64(0.9551641718895217),\n",
       "   np.float64(0.943035040009623),\n",
       "   np.float64(0.9535921780352615),\n",
       "   np.float64(0.9333372328577195),\n",
       "   np.float64(0.9426483989876523),\n",
       "   np.float64(0.9399040772975173),\n",
       "   np.float64(0.9433650430141918),\n",
       "   np.float64(0.9504523792540158),\n",
       "   np.float64(0.9390209392698834),\n",
       "   np.float64(0.9442962350506791),\n",
       "   np.float64(0.942171295573511),\n",
       "   np.float64(0.9453989234658826),\n",
       "   np.float64(0.9463875445414186),\n",
       "   np.float64(0.9363321033425902),\n",
       "   np.float64(0.9445286053743057),\n",
       "   np.float64(0.9390507259143613),\n",
       "   np.float64(0.9462431149368029),\n",
       "   np.float64(0.9426819824534417),\n",
       "   np.float64(0.9409727255046327),\n",
       "   np.float64(0.9407798202213867),\n",
       "   np.float64(0.9373853451176154),\n",
       "   np.float64(0.938378587947301),\n",
       "   np.float64(0.9391842250197704),\n",
       "   np.float64(0.9448410138820832),\n",
       "   np.float64(0.933032635634399),\n",
       "   np.float64(0.941950543487091),\n",
       "   np.float64(0.9324805141330689),\n",
       "   np.float64(0.9366498433287724),\n",
       "   np.float64(0.934719835218985),\n",
       "   np.float64(0.9334046300668045),\n",
       "   np.float64(0.9332064170361918),\n",
       "   np.float64(0.9314976286855405),\n",
       "   np.float64(0.9416680973587461),\n",
       "   np.float64(0.945154220514317),\n",
       "   np.float64(0.9339267719844675),\n",
       "   np.float64(0.9389074882962646),\n",
       "   np.float64(0.9311242259619626),\n",
       "   np.float64(0.936342314742365),\n",
       "   np.float64(0.9381046628757664),\n",
       "   np.float64(0.9324326770740278),\n",
       "   np.float64(0.9333804927909545),\n",
       "   np.float64(0.9294389619063251),\n",
       "   np.float64(0.9289514962196951),\n",
       "   np.float64(0.9261170985319874),\n",
       "   np.float64(0.9347504187598052),\n",
       "   np.float64(0.9295726914699627),\n",
       "   np.float64(0.9221484901798963),\n",
       "   np.float64(0.9271634096606101),\n",
       "   np.float64(0.9263152858616841),\n",
       "   np.float64(0.9311298281847425),\n",
       "   np.float64(0.9321371349713146),\n",
       "   np.float64(0.9244782835813141),\n",
       "   np.float64(0.9281768964931039),\n",
       "   np.float64(0.920134883654733),\n",
       "   np.float64(0.923211736717206),\n",
       "   np.float64(0.9178381872616842),\n",
       "   np.float64(0.9266708428277856),\n",
       "   np.float64(0.9298968310599058),\n",
       "   np.float64(0.9192970089485961),\n",
       "   np.float64(0.9247673004497237),\n",
       "   np.float64(0.9186514844364484),\n",
       "   np.float64(0.9241159350984323),\n",
       "   np.float64(0.927113296413267),\n",
       "   np.float64(0.9174919228671576),\n",
       "   np.float64(0.9224381642607186),\n",
       "   np.float64(0.9192298401968629),\n",
       "   np.float64(0.9211536010970588),\n",
       "   np.float64(0.9286853644071177),\n",
       "   np.float64(0.918200018456762),\n",
       "   np.float64(0.9256609811597677),\n",
       "   np.float64(0.9151050959109923),\n",
       "   np.float64(0.9159109288531792),\n",
       "   np.float64(0.9233924307418852),\n",
       "   np.float64(0.9246880696373817),\n",
       "   np.float64(0.9149520576742275),\n",
       "   np.float64(0.9114701877364985),\n",
       "   np.float64(0.9115846982635253),\n",
       "   np.float64(0.9138655947884766),\n",
       "   np.float64(0.9149815575532956),\n",
       "   np.float64(0.9155154046639834),\n",
       "   np.float64(0.9176185765964139),\n",
       "   np.float64(0.9053552079066047),\n",
       "   np.float64(0.9157765731999877),\n",
       "   np.float64(0.9114755423883742),\n",
       "   np.float64(0.9106599741745908),\n",
       "   np.float64(0.9109421893679709),\n",
       "   np.float64(0.9076170992561279),\n",
       "   np.float64(0.908996372710437),\n",
       "   np.float64(0.9075790618722173),\n",
       "   np.float64(0.9116335064516169),\n",
       "   np.float64(0.9042322324984928),\n",
       "   np.float64(0.9108953678065594),\n",
       "   np.float64(0.9035077070499914),\n",
       "   np.float64(0.9080594101368229),\n",
       "   np.float64(0.9085934907471461),\n",
       "   np.float64(0.9095606975320352),\n",
       "   np.float64(0.9003451152205528),\n",
       "   np.float64(0.9061631754426713),\n",
       "   np.float64(0.9099536648419756),\n",
       "   np.float64(0.906822195826436),\n",
       "   np.float64(0.9016343337273857),\n",
       "   np.float64(0.9062470333024935),\n",
       "   np.float64(0.9009810055755995),\n",
       "   np.float64(0.9039436835258272),\n",
       "   np.float64(0.8993840092008939),\n",
       "   np.float64(0.8948971995866836),\n",
       "   np.float64(0.9048505276376292),\n",
       "   np.float64(0.9030729949710049),\n",
       "   np.float64(0.8982512216000212),\n",
       "   np.float64(0.9092663499203774),\n",
       "   np.float64(0.8989673336985533),\n",
       "   np.float64(0.8956962599765126),\n",
       "   np.float64(0.9014662777170293),\n",
       "   np.float64(0.8886104321754401),\n",
       "   np.float64(0.8964483642361837),\n",
       "   np.float64(0.9008748151463064),\n",
       "   np.float64(0.8984243875365533),\n",
       "   np.float64(0.9055559668590851),\n",
       "   np.float64(0.9006635146916847),\n",
       "   np.float64(0.8928470475586626),\n",
       "   np.float64(0.8943015362811727),\n",
       "   np.float64(0.8961591584370464),\n",
       "   np.float64(0.8842442939433588),\n",
       "   np.float64(0.8985873592884461),\n",
       "   np.float64(0.8988797388098384),\n",
       "   np.float64(0.8975846118245597),\n",
       "   np.float64(0.889739021680629),\n",
       "   np.float64(0.8933089280719932),\n",
       "   np.float64(0.8921303040054208),\n",
       "   np.float64(0.8968943272335792),\n",
       "   np.float64(0.8964155143272713),\n",
       "   np.float64(0.8913845047393303),\n",
       "   np.float64(0.8953598660862165),\n",
       "   np.float64(0.8977144308123123),\n",
       "   np.float64(0.89396938637001),\n",
       "   np.float64(0.8957646000634226),\n",
       "   np.float64(0.8867075735904767),\n",
       "   np.float64(0.8896578978435863),\n",
       "   np.float64(0.8849271159618362),\n",
       "   np.float64(0.8880156460392734),\n",
       "   np.float64(0.8896124026782246),\n",
       "   np.float64(0.8979114944656555),\n",
       "   np.float64(0.8892192894937206),\n",
       "   np.float64(0.8973182195424517),\n",
       "   np.float64(0.8973254814619193),\n",
       "   np.float64(0.8943233732946144),\n",
       "   np.float64(0.8933545858367097),\n",
       "   np.float64(0.8893329273085177),\n",
       "   np.float64(0.8883502862665719),\n",
       "   np.float64(0.8922282886172229),\n",
       "   np.float64(0.8878472447088932),\n",
       "   np.float64(0.8805771259554115),\n",
       "   np.float64(0.8895181351636602),\n",
       "   np.float64(0.8898064812593318),\n",
       "   np.float64(0.8836029683050174),\n",
       "   np.float64(0.8851076389626962),\n",
       "   np.float64(0.8814171853703268),\n",
       "   np.float64(0.8904948126496329),\n",
       "   np.float64(0.8930921004104837),\n",
       "   np.float64(0.8776832507926939),\n",
       "   np.float64(0.8837148862193548),\n",
       "   np.float64(0.8915497705633443),\n",
       "   np.float64(0.8882268663073084),\n",
       "   np.float64(0.879799123699031),\n",
       "   np.float64(0.8840504983609233),\n",
       "   np.float64(0.8893717488129389),\n",
       "   np.float64(0.883970801733616),\n",
       "   np.float64(0.884814326900505),\n",
       "   np.float64(0.8854483130282033),\n",
       "   np.float64(0.881296150105596),\n",
       "   np.float64(0.8798910286459636),\n",
       "   np.float64(0.8834972053172827),\n",
       "   np.float64(0.8805747269740841),\n",
       "   np.float64(0.8800513877020144),\n",
       "   np.float64(0.8846228740253811),\n",
       "   np.float64(0.8923374903352418),\n",
       "   np.float64(0.8822821452566737),\n",
       "   np.float64(0.8781800055842329),\n",
       "   np.float64(0.8755544828711712),\n",
       "   np.float64(0.8821814664246544),\n",
       "   np.float64(0.8778070556707773),\n",
       "   np.float64(0.8793851044579742),\n",
       "   np.float64(0.8793619546543359),\n",
       "   np.float64(0.8816823322908557),\n",
       "   np.float64(0.8722313500179162),\n",
       "   np.float64(0.8779696974309057),\n",
       "   np.float64(0.8807511135813414)],\n",
       "  'train_total_loss_std': [np.float64(0.6764261177223779),\n",
       "   np.float64(0.025088741030617603),\n",
       "   np.float64(0.016792658239738148),\n",
       "   np.float64(0.014102795736549729),\n",
       "   np.float64(0.014115483392019774),\n",
       "   np.float64(0.0137437654808764),\n",
       "   np.float64(0.012102142557978968),\n",
       "   np.float64(0.012580203391174008),\n",
       "   np.float64(0.011367043016493746),\n",
       "   np.float64(0.011905472400210051),\n",
       "   np.float64(0.011286157737136916),\n",
       "   np.float64(0.013137901880891527),\n",
       "   np.float64(0.014347722727554324),\n",
       "   np.float64(0.012746074120417276),\n",
       "   np.float64(0.01417594681926663),\n",
       "   np.float64(0.013223326379004488),\n",
       "   np.float64(0.013061121452994779),\n",
       "   np.float64(0.013482498566504929),\n",
       "   np.float64(0.01370407683476825),\n",
       "   np.float64(0.013853965622759746),\n",
       "   np.float64(0.014321360232120727),\n",
       "   np.float64(0.014931481356805396),\n",
       "   np.float64(0.014645269645608106),\n",
       "   np.float64(0.01637376117357221),\n",
       "   np.float64(0.01576383990956243),\n",
       "   np.float64(0.014781622948382245),\n",
       "   np.float64(0.016099977724162585),\n",
       "   np.float64(0.016168864281819843),\n",
       "   np.float64(0.014833663823767256),\n",
       "   np.float64(0.01614278493663816),\n",
       "   np.float64(0.015616761653810944),\n",
       "   np.float64(0.015354463619170753),\n",
       "   np.float64(0.016584495643884305),\n",
       "   np.float64(0.013772488907004849),\n",
       "   np.float64(0.017839782415635542),\n",
       "   np.float64(0.018069636374107967),\n",
       "   np.float64(0.018915123272410267),\n",
       "   np.float64(0.018364730847052608),\n",
       "   np.float64(0.01832695742822758),\n",
       "   np.float64(0.019124428288445072),\n",
       "   np.float64(0.018479712195179056),\n",
       "   np.float64(0.01702749094066643),\n",
       "   np.float64(0.01859397665512111),\n",
       "   np.float64(0.016410929746049508),\n",
       "   np.float64(0.018263210725161853),\n",
       "   np.float64(0.021195631471534863),\n",
       "   np.float64(0.01811446245034923),\n",
       "   np.float64(0.020243626606418964),\n",
       "   np.float64(0.02167391635667276),\n",
       "   np.float64(0.020787560838829475),\n",
       "   np.float64(0.02155682919759054),\n",
       "   np.float64(0.021589145298671956),\n",
       "   np.float64(0.02419015761786535),\n",
       "   np.float64(0.022841291521951804),\n",
       "   np.float64(0.020959594380356897),\n",
       "   np.float64(0.021716328999226854),\n",
       "   np.float64(0.020328822066848422),\n",
       "   np.float64(0.023494536406291098),\n",
       "   np.float64(0.02442372193171632),\n",
       "   np.float64(0.022978028899170715),\n",
       "   np.float64(0.021247292885914676),\n",
       "   np.float64(0.02462295577040345),\n",
       "   np.float64(0.021559483642339856),\n",
       "   np.float64(0.0222647441718594),\n",
       "   np.float64(0.020488749215701586),\n",
       "   np.float64(0.0234439273552821),\n",
       "   np.float64(0.023250053382187823),\n",
       "   np.float64(0.027588708999079763),\n",
       "   np.float64(0.022834310875546868),\n",
       "   np.float64(0.02256273060213038),\n",
       "   np.float64(0.02448483838658851),\n",
       "   np.float64(0.026739677695968155),\n",
       "   np.float64(0.025128074426030586),\n",
       "   np.float64(0.024747122535336195),\n",
       "   np.float64(0.02293639848148372),\n",
       "   np.float64(0.025976290774718993),\n",
       "   np.float64(0.029871793535019897),\n",
       "   np.float64(0.027342868975611882),\n",
       "   np.float64(0.02717374782894068),\n",
       "   np.float64(0.02576782115841194),\n",
       "   np.float64(0.027813899333242582),\n",
       "   np.float64(0.027038095598140105),\n",
       "   np.float64(0.027950411837646485),\n",
       "   np.float64(0.02713149243579437),\n",
       "   np.float64(0.0260968335119092),\n",
       "   np.float64(0.02664533008150979),\n",
       "   np.float64(0.03061906662869577),\n",
       "   np.float64(0.029811073245793584),\n",
       "   np.float64(0.028918003476014777),\n",
       "   np.float64(0.03204654129602627),\n",
       "   np.float64(0.02232689552593008),\n",
       "   np.float64(0.029639038674617403),\n",
       "   np.float64(0.028210068984200604),\n",
       "   np.float64(0.028791881205489745),\n",
       "   np.float64(0.02737361778028275),\n",
       "   np.float64(0.034048643444849425),\n",
       "   np.float64(0.025379093373531805),\n",
       "   np.float64(0.03321842963921592),\n",
       "   np.float64(0.03097807679755285),\n",
       "   np.float64(0.030486096199762922),\n",
       "   np.float64(0.026974196213366938),\n",
       "   np.float64(0.03235614105651577),\n",
       "   np.float64(0.03282534903510342),\n",
       "   np.float64(0.03526679142792955),\n",
       "   np.float64(0.03247180662608875),\n",
       "   np.float64(0.03255974098801993),\n",
       "   np.float64(0.03408910165477758),\n",
       "   np.float64(0.03158660429641514),\n",
       "   np.float64(0.03255138505871313),\n",
       "   np.float64(0.031426200557863174),\n",
       "   np.float64(0.03232275481112388),\n",
       "   np.float64(0.029578797313257896),\n",
       "   np.float64(0.030496760817264004),\n",
       "   np.float64(0.029514006667750566),\n",
       "   np.float64(0.031080941207059544),\n",
       "   np.float64(0.034225184378879114),\n",
       "   np.float64(0.03557706157352933),\n",
       "   np.float64(0.03253262870841819),\n",
       "   np.float64(0.0361272333707542),\n",
       "   np.float64(0.0364436177755764),\n",
       "   np.float64(0.033343128273683735),\n",
       "   np.float64(0.030436013332758357),\n",
       "   np.float64(0.03474761304169502),\n",
       "   np.float64(0.03079712389923075),\n",
       "   np.float64(0.033977339920515874),\n",
       "   np.float64(0.035139143941001205),\n",
       "   np.float64(0.03933347141780017),\n",
       "   np.float64(0.03929849398533786),\n",
       "   np.float64(0.0323831626567491),\n",
       "   np.float64(0.033904805892129054),\n",
       "   np.float64(0.027350031535365738),\n",
       "   np.float64(0.03576768267861414),\n",
       "   np.float64(0.03615683326609994),\n",
       "   np.float64(0.03416745624553661),\n",
       "   np.float64(0.029166458697121724),\n",
       "   np.float64(0.036652388195317374),\n",
       "   np.float64(0.036995780521025555),\n",
       "   np.float64(0.04074930250893329),\n",
       "   np.float64(0.0374449259083836),\n",
       "   np.float64(0.039369865831556515),\n",
       "   np.float64(0.03174099612624891),\n",
       "   np.float64(0.035332741756932254),\n",
       "   np.float64(0.039086895092867426),\n",
       "   np.float64(0.03043898658962332),\n",
       "   np.float64(0.031196154793395263),\n",
       "   np.float64(0.034348149132660204),\n",
       "   np.float64(0.03636930194659659),\n",
       "   np.float64(0.032241381485511776),\n",
       "   np.float64(0.031540842879047064),\n",
       "   np.float64(0.03619199413529955),\n",
       "   np.float64(0.037785247026565515),\n",
       "   np.float64(0.035538066548959635),\n",
       "   np.float64(0.03266137277986936),\n",
       "   np.float64(0.035374532143203324),\n",
       "   np.float64(0.03970315874248624),\n",
       "   np.float64(0.035293450891582825),\n",
       "   np.float64(0.043102518720272626),\n",
       "   np.float64(0.03217591617059767),\n",
       "   np.float64(0.03260449049400455),\n",
       "   np.float64(0.04532446722491228),\n",
       "   np.float64(0.036539160553185464),\n",
       "   np.float64(0.036665953722357356),\n",
       "   np.float64(0.039570266650437856),\n",
       "   np.float64(0.038286355936297804),\n",
       "   np.float64(0.03815117673397878),\n",
       "   np.float64(0.034372107120473214),\n",
       "   np.float64(0.039496759543481766),\n",
       "   np.float64(0.038596491233997623),\n",
       "   np.float64(0.03770660964978096),\n",
       "   np.float64(0.037532117500070496),\n",
       "   np.float64(0.03724686366816874),\n",
       "   np.float64(0.03864494986198968),\n",
       "   np.float64(0.038039024047542225),\n",
       "   np.float64(0.034697852511646664),\n",
       "   np.float64(0.041619941707579405),\n",
       "   np.float64(0.036404069198522473),\n",
       "   np.float64(0.0396992290246656),\n",
       "   np.float64(0.03908696925898881),\n",
       "   np.float64(0.04301787627133045),\n",
       "   np.float64(0.03573396683268344),\n",
       "   np.float64(0.03528638199254756),\n",
       "   np.float64(0.03685474946897037),\n",
       "   np.float64(0.03588358664853038),\n",
       "   np.float64(0.036383767223911755),\n",
       "   np.float64(0.043874415999605496),\n",
       "   np.float64(0.042295301285122),\n",
       "   np.float64(0.039406900175289424),\n",
       "   np.float64(0.03587824821156578),\n",
       "   np.float64(0.037568590274367215),\n",
       "   np.float64(0.03763936437139569),\n",
       "   np.float64(0.039366774739712125),\n",
       "   np.float64(0.04243220831239631),\n",
       "   np.float64(0.04052156761076108),\n",
       "   np.float64(0.042347199666733525),\n",
       "   np.float64(0.04444037499401621),\n",
       "   np.float64(0.03680285126318357),\n",
       "   np.float64(0.04373688607648292),\n",
       "   np.float64(0.04380025329611675),\n",
       "   np.float64(0.037955866046036464),\n",
       "   np.float64(0.03835634417624119),\n",
       "   np.float64(0.04142528844552255),\n",
       "   np.float64(0.041663146082374265),\n",
       "   np.float64(0.046091335958028765),\n",
       "   np.float64(0.03932122998265563),\n",
       "   np.float64(0.0427149808493583),\n",
       "   np.float64(0.04236359157077624),\n",
       "   np.float64(0.03991685526835171),\n",
       "   np.float64(0.04226895851133975),\n",
       "   np.float64(0.04401096434324538),\n",
       "   np.float64(0.04302451935594805),\n",
       "   np.float64(0.03877897270788288),\n",
       "   np.float64(0.037435383997985544),\n",
       "   np.float64(0.04141706360953239),\n",
       "   np.float64(0.04526674744645023),\n",
       "   np.float64(0.04371202855768341),\n",
       "   np.float64(0.045700773046930446),\n",
       "   np.float64(0.040764267794558016),\n",
       "   np.float64(0.040514868332683064),\n",
       "   np.float64(0.04278268654660072),\n",
       "   np.float64(0.03875794454478244),\n",
       "   np.float64(0.042361578981337265),\n",
       "   np.float64(0.044121727494227526),\n",
       "   np.float64(0.03822705985800881),\n",
       "   np.float64(0.04040176629605291),\n",
       "   np.float64(0.03782789893955871),\n",
       "   np.float64(0.04252505786231728),\n",
       "   np.float64(0.04727201787616743),\n",
       "   np.float64(0.04136839639486233),\n",
       "   np.float64(0.03943883596957326),\n",
       "   np.float64(0.04475990296886272),\n",
       "   np.float64(0.046392366154918145),\n",
       "   np.float64(0.040785749135792256),\n",
       "   np.float64(0.04382217831359705),\n",
       "   np.float64(0.03901800074910953),\n",
       "   np.float64(0.04232883895631249),\n",
       "   np.float64(0.04685996868002207),\n",
       "   np.float64(0.03786030759878606),\n",
       "   np.float64(0.03968638616784241),\n",
       "   np.float64(0.037220849369762804),\n",
       "   np.float64(0.04333942671270484),\n",
       "   np.float64(0.0410503619130671),\n",
       "   np.float64(0.03622501861067505),\n",
       "   np.float64(0.0464840896814731),\n",
       "   np.float64(0.03859192702868209),\n",
       "   np.float64(0.04014473342165567),\n",
       "   np.float64(0.043718287956461876),\n",
       "   np.float64(0.04474387560509203),\n",
       "   np.float64(0.047984874434994836),\n",
       "   np.float64(0.043199280227819443),\n",
       "   np.float64(0.04254965877236538),\n",
       "   np.float64(0.03953395371434018),\n",
       "   np.float64(0.03755871105136536),\n",
       "   np.float64(0.048346600635248586),\n",
       "   np.float64(0.04933316662163105),\n",
       "   np.float64(0.04083397560515523),\n",
       "   np.float64(0.04563533474134228),\n",
       "   np.float64(0.04642202644607571),\n",
       "   np.float64(0.04657558780444416),\n",
       "   np.float64(0.04579541832063563),\n",
       "   np.float64(0.04311750778626634),\n",
       "   np.float64(0.052052153856737816),\n",
       "   np.float64(0.04679035584538309),\n",
       "   np.float64(0.04544795065315668),\n",
       "   np.float64(0.04347694640744349),\n",
       "   np.float64(0.03973393102708401),\n",
       "   np.float64(0.04362472579728803),\n",
       "   np.float64(0.04775497476380608),\n",
       "   np.float64(0.04332903852401383),\n",
       "   np.float64(0.042521833294342525),\n",
       "   np.float64(0.0475074729340294),\n",
       "   np.float64(0.04394166550198788),\n",
       "   np.float64(0.045427954839012415),\n",
       "   np.float64(0.04276085715819938),\n",
       "   np.float64(0.0463620583944668),\n",
       "   np.float64(0.043599933301602285),\n",
       "   np.float64(0.04249799932726611),\n",
       "   np.float64(0.04684011860677145),\n",
       "   np.float64(0.04269957973727329),\n",
       "   np.float64(0.04650811068735449),\n",
       "   np.float64(0.045428770809834484),\n",
       "   np.float64(0.043520952394444534),\n",
       "   np.float64(0.045870731858735414),\n",
       "   np.float64(0.04981785049352526),\n",
       "   np.float64(0.046579816686347725),\n",
       "   np.float64(0.03874875739060456),\n",
       "   np.float64(0.045634040611155675),\n",
       "   np.float64(0.04687427567351665),\n",
       "   np.float64(0.04997328106183959),\n",
       "   np.float64(0.04499171991223068),\n",
       "   np.float64(0.046974068818888715),\n",
       "   np.float64(0.044017527969313165),\n",
       "   np.float64(0.043135866926797314),\n",
       "   np.float64(0.04513763643023058),\n",
       "   np.float64(0.045670044514964814),\n",
       "   np.float64(0.044651180400742715),\n",
       "   np.float64(0.045212952249583646),\n",
       "   np.float64(0.050794467215814834),\n",
       "   np.float64(0.04414100814085785),\n",
       "   np.float64(0.048602726477715415),\n",
       "   np.float64(0.046837948369983264),\n",
       "   np.float64(0.0418527627028016),\n",
       "   np.float64(0.046541486570378855),\n",
       "   np.float64(0.039703524649715716),\n",
       "   np.float64(0.0413375883009644),\n",
       "   np.float64(0.04555175859525331),\n",
       "   np.float64(0.04302751554582992),\n",
       "   np.float64(0.045692751821698664),\n",
       "   np.float64(0.0544186445776753),\n",
       "   np.float64(0.04887772546834361),\n",
       "   np.float64(0.039509240164793256),\n",
       "   np.float64(0.045765348217156325),\n",
       "   np.float64(0.047930844350851334),\n",
       "   np.float64(0.04760712502027661),\n",
       "   np.float64(0.04627854623589664),\n",
       "   np.float64(0.046793634729043264),\n",
       "   np.float64(0.050107624490767576),\n",
       "   np.float64(0.0445555239354891),\n",
       "   np.float64(0.04594905388840265),\n",
       "   np.float64(0.04717363093182513),\n",
       "   np.float64(0.04488158438391095),\n",
       "   np.float64(0.0482785961128228),\n",
       "   np.float64(0.04998543935125681),\n",
       "   np.float64(0.04508030364469314),\n",
       "   np.float64(0.04778228131396593),\n",
       "   np.float64(0.03931261676741904),\n",
       "   np.float64(0.04655137097686103),\n",
       "   np.float64(0.04364870263654042),\n",
       "   np.float64(0.04670859768063387),\n",
       "   np.float64(0.04693010927183844),\n",
       "   np.float64(0.04456027903624473),\n",
       "   np.float64(0.04556311864601099),\n",
       "   np.float64(0.04657876289659143),\n",
       "   np.float64(0.05004110986752857),\n",
       "   np.float64(0.04395082399580818),\n",
       "   np.float64(0.04807465957124076),\n",
       "   np.float64(0.046176960214744114),\n",
       "   np.float64(0.045506303589526474),\n",
       "   np.float64(0.04269342150384801),\n",
       "   np.float64(0.04185657879845186),\n",
       "   np.float64(0.04309356042057243),\n",
       "   np.float64(0.04690531322748212),\n",
       "   np.float64(0.04322552762925792),\n",
       "   np.float64(0.0478916476660661),\n",
       "   np.float64(0.04779926717728371),\n",
       "   np.float64(0.047791371331789935),\n",
       "   np.float64(0.04068067301235532),\n",
       "   np.float64(0.04947201125179564),\n",
       "   np.float64(0.048282416259254016),\n",
       "   np.float64(0.04543023144575397),\n",
       "   np.float64(0.044291847735772494),\n",
       "   np.float64(0.05251151960694104),\n",
       "   np.float64(0.05290317106391099),\n",
       "   np.float64(0.055208047301113415),\n",
       "   np.float64(0.04597939287582886),\n",
       "   np.float64(0.047647691178391106),\n",
       "   np.float64(0.0432049862620727),\n",
       "   np.float64(0.04374696919469939),\n",
       "   np.float64(0.043488173744090336),\n",
       "   np.float64(0.05490404978703173),\n",
       "   np.float64(0.04709764387877523),\n",
       "   np.float64(0.04629609879934752),\n",
       "   np.float64(0.04912438358707307),\n",
       "   np.float64(0.042233782497070514),\n",
       "   np.float64(0.048068424212540733),\n",
       "   np.float64(0.0480340008839863),\n",
       "   np.float64(0.05018044748577402),\n",
       "   np.float64(0.04835011649722777),\n",
       "   np.float64(0.04881734918778374),\n",
       "   np.float64(0.04862592266779758),\n",
       "   np.float64(0.04718646780759919),\n",
       "   np.float64(0.04607014702801893),\n",
       "   np.float64(0.044302579800099454),\n",
       "   np.float64(0.050895525049158806),\n",
       "   np.float64(0.04666050404942286),\n",
       "   np.float64(0.04486015614604298),\n",
       "   np.float64(0.04393608534973803),\n",
       "   np.float64(0.048727381832903614),\n",
       "   np.float64(0.04763816755254408),\n",
       "   np.float64(0.0495228601713842),\n",
       "   np.float64(0.05094371039643591),\n",
       "   np.float64(0.05005833362879907),\n",
       "   np.float64(0.055250527173597686),\n",
       "   np.float64(0.050321273998079084),\n",
       "   np.float64(0.04934971219296739),\n",
       "   np.float64(0.048619857407541156),\n",
       "   np.float64(0.04444846174982206),\n",
       "   np.float64(0.04544947992086976),\n",
       "   np.float64(0.052378294876873954),\n",
       "   np.float64(0.0502079516893061),\n",
       "   np.float64(0.048926695948011786),\n",
       "   np.float64(0.04915175288281948),\n",
       "   np.float64(0.048418831904436674),\n",
       "   np.float64(0.05131407091198021),\n",
       "   np.float64(0.05461006084106119),\n",
       "   np.float64(0.04314017804864507),\n",
       "   np.float64(0.054042905994077864),\n",
       "   np.float64(0.051511121267104415),\n",
       "   np.float64(0.045160783185726756),\n",
       "   np.float64(0.04743819314114393),\n",
       "   np.float64(0.05204099767351931),\n",
       "   np.float64(0.05604073107808187),\n",
       "   np.float64(0.05088311237572093),\n",
       "   np.float64(0.050990298764972415),\n",
       "   np.float64(0.04622191805490636),\n",
       "   np.float64(0.04923445824029698),\n",
       "   np.float64(0.0425617046401342),\n",
       "   np.float64(0.048350419468710916),\n",
       "   np.float64(0.05024556019575355),\n",
       "   np.float64(0.0477486957879357),\n",
       "   np.float64(0.050140980346016814),\n",
       "   np.float64(0.04511800243817408),\n",
       "   np.float64(0.046854960980258195),\n",
       "   np.float64(0.049176878827153575),\n",
       "   np.float64(0.05038461845607689),\n",
       "   np.float64(0.05021153800071813),\n",
       "   np.float64(0.04211228532165678),\n",
       "   np.float64(0.04818641997058887),\n",
       "   np.float64(0.04606020163087664),\n",
       "   np.float64(0.051054482673458614),\n",
       "   np.float64(0.05378122178454977),\n",
       "   np.float64(0.051852474344035386),\n",
       "   np.float64(0.05145664725371959),\n",
       "   np.float64(0.050525235201499485),\n",
       "   np.float64(0.04992588384607508),\n",
       "   np.float64(0.04161452620466494),\n",
       "   np.float64(0.04765166062416203),\n",
       "   np.float64(0.04670985808758605),\n",
       "   np.float64(0.05189321702369435),\n",
       "   np.float64(0.05099885894712123),\n",
       "   np.float64(0.045848998574882865),\n",
       "   np.float64(0.04751189211635297),\n",
       "   np.float64(0.04703879587664986),\n",
       "   np.float64(0.05301441130473204),\n",
       "   np.float64(0.047224234547215166),\n",
       "   np.float64(0.05738582611766289),\n",
       "   np.float64(0.04553128099645677),\n",
       "   np.float64(0.051835963144097094)],\n",
       "  'train_mdn_loss_mean': [np.float64(1.9048484754562378),\n",
       "   np.float64(1.521775052547455),\n",
       "   np.float64(1.471820158958435),\n",
       "   np.float64(1.43455743432045),\n",
       "   np.float64(1.410227873325348),\n",
       "   np.float64(1.383511563539505),\n",
       "   np.float64(1.370344696044922),\n",
       "   np.float64(1.3615226638317108),\n",
       "   np.float64(1.353546258211136),\n",
       "   np.float64(1.343441710472107),\n",
       "   np.float64(1.33964897274971),\n",
       "   np.float64(1.333033881187439),\n",
       "   np.float64(1.325721755027771),\n",
       "   np.float64(1.3210748863220214),\n",
       "   np.float64(1.3145511603355409),\n",
       "   np.float64(1.3148162543773652),\n",
       "   np.float64(1.3085474097728729),\n",
       "   np.float64(1.3028765523433685),\n",
       "   np.float64(1.298530980348587),\n",
       "   np.float64(1.2953738152980805),\n",
       "   np.float64(1.2917325603961944),\n",
       "   np.float64(1.284589411020279),\n",
       "   np.float64(1.2829398703575134),\n",
       "   np.float64(1.2752962589263916),\n",
       "   np.float64(1.273094892501831),\n",
       "   np.float64(1.2703585290908814),\n",
       "   np.float64(1.2637556576728821),\n",
       "   np.float64(1.2620266485214233),\n",
       "   np.float64(1.2607598543167113),\n",
       "   np.float64(1.2525780093669892),\n",
       "   np.float64(1.2490801644325256),\n",
       "   np.float64(1.247588266134262),\n",
       "   np.float64(1.2451172351837159),\n",
       "   np.float64(1.240136148929596),\n",
       "   np.float64(1.2379771852493286),\n",
       "   np.float64(1.2283161854743958),\n",
       "   np.float64(1.2258895826339722),\n",
       "   np.float64(1.2234141004085541),\n",
       "   np.float64(1.2230150747299193),\n",
       "   np.float64(1.216244739294052),\n",
       "   np.float64(1.2147413551807404),\n",
       "   np.float64(1.2135974586009979),\n",
       "   np.float64(1.2079325723648071),\n",
       "   np.float64(1.2040717923641204),\n",
       "   np.float64(1.2018208527565002),\n",
       "   np.float64(1.1995355582237244),\n",
       "   np.float64(1.1936095917224885),\n",
       "   np.float64(1.18967271566391),\n",
       "   np.float64(1.1791264140605926),\n",
       "   np.float64(1.1789712131023407),\n",
       "   np.float64(1.180289386510849),\n",
       "   np.float64(1.1728496754169464),\n",
       "   np.float64(1.170806005001068),\n",
       "   np.float64(1.16737402677536),\n",
       "   np.float64(1.1670546102523804),\n",
       "   np.float64(1.1679388344287873),\n",
       "   np.float64(1.16451220870018),\n",
       "   np.float64(1.1605606126785277),\n",
       "   np.float64(1.1557287228107453),\n",
       "   np.float64(1.151796245574951),\n",
       "   np.float64(1.1496385514736176),\n",
       "   np.float64(1.144272733926773),\n",
       "   np.float64(1.1459650218486785),\n",
       "   np.float64(1.147143898010254),\n",
       "   np.float64(1.1439163863658905),\n",
       "   np.float64(1.1361679029464722),\n",
       "   np.float64(1.1346386504173278),\n",
       "   np.float64(1.1312228167057037),\n",
       "   np.float64(1.1302445936203003),\n",
       "   np.float64(1.1298068428039552),\n",
       "   np.float64(1.1230068516731262),\n",
       "   np.float64(1.122329831123352),\n",
       "   np.float64(1.1196674919128418),\n",
       "   np.float64(1.1162284088134766),\n",
       "   np.float64(1.1135916316509247),\n",
       "   np.float64(1.117367947101593),\n",
       "   np.float64(1.1130140364170074),\n",
       "   np.float64(1.1142754888534545),\n",
       "   np.float64(1.10713755607605),\n",
       "   np.float64(1.101954597234726),\n",
       "   np.float64(1.1069105005264281),\n",
       "   np.float64(1.1066277086734773),\n",
       "   np.float64(1.0984986758232116),\n",
       "   np.float64(1.1012302398681642),\n",
       "   np.float64(1.1022472190856933),\n",
       "   np.float64(1.0922056305408478),\n",
       "   np.float64(1.0932753789424896),\n",
       "   np.float64(1.093459002971649),\n",
       "   np.float64(1.0868049454689026),\n",
       "   np.float64(1.0876811456680298),\n",
       "   np.float64(1.086943565607071),\n",
       "   np.float64(1.0837269580364228),\n",
       "   np.float64(1.0856587505340576),\n",
       "   np.float64(1.077985670566559),\n",
       "   np.float64(1.0824660468101501),\n",
       "   np.float64(1.0822960543632507),\n",
       "   np.float64(1.078765777349472),\n",
       "   np.float64(1.0721954321861267),\n",
       "   np.float64(1.0779435992240907),\n",
       "   np.float64(1.063812239766121),\n",
       "   np.float64(1.061340856552124),\n",
       "   np.float64(1.0559808683395386),\n",
       "   np.float64(1.0576746410131455),\n",
       "   np.float64(1.056160334944725),\n",
       "   np.float64(1.0518356513977052),\n",
       "   np.float64(1.0590769720077515),\n",
       "   np.float64(1.0498830389976501),\n",
       "   np.float64(1.051014958024025),\n",
       "   np.float64(1.0498183900117875),\n",
       "   np.float64(1.0447587770223619),\n",
       "   np.float64(1.0441269892454148),\n",
       "   np.float64(1.048896991610527),\n",
       "   np.float64(1.037196670770645),\n",
       "   np.float64(1.0357149308919906),\n",
       "   np.float64(1.0421325695514678),\n",
       "   np.float64(1.0386646622419358),\n",
       "   np.float64(1.0437590503692626),\n",
       "   np.float64(1.0356772565841674),\n",
       "   np.float64(1.032161567211151),\n",
       "   np.float64(1.0343242925405502),\n",
       "   np.float64(1.031487573981285),\n",
       "   np.float64(1.0304483956098556),\n",
       "   np.float64(1.0303057593107223),\n",
       "   np.float64(1.0244908732175828),\n",
       "   np.float64(1.0239344036579132),\n",
       "   np.float64(1.0249852353334428),\n",
       "   np.float64(1.0268072587251664),\n",
       "   np.float64(1.018057262301445),\n",
       "   np.float64(1.024245336651802),\n",
       "   np.float64(1.0219539296627045),\n",
       "   np.float64(1.0147650933265686),\n",
       "   np.float64(1.0218438571691513),\n",
       "   np.float64(1.0136496329307556),\n",
       "   np.float64(1.0208716946840286),\n",
       "   np.float64(1.0165141814947127),\n",
       "   np.float64(1.016645498275757),\n",
       "   np.float64(1.0179320883750915),\n",
       "   np.float64(1.0190420210361482),\n",
       "   np.float64(1.0128221559524535),\n",
       "   np.float64(1.0157448440790176),\n",
       "   np.float64(1.0084893119335174),\n",
       "   np.float64(1.0034795063734054),\n",
       "   np.float64(1.010133027434349),\n",
       "   np.float64(1.0085065007209777),\n",
       "   np.float64(1.0076043450832366),\n",
       "   np.float64(1.0032169610261916),\n",
       "   np.float64(1.0038460797071458),\n",
       "   np.float64(1.0003049385547638),\n",
       "   np.float64(1.0040460652112961),\n",
       "   np.float64(1.0067571550607681),\n",
       "   np.float64(0.9987002497911454),\n",
       "   np.float64(1.0041745311021806),\n",
       "   np.float64(1.0020032268762589),\n",
       "   np.float64(1.007452444434166),\n",
       "   np.float64(0.9978633588552475),\n",
       "   np.float64(0.9935432666540146),\n",
       "   np.float64(1.000104884505272),\n",
       "   np.float64(1.0037308758497239),\n",
       "   np.float64(0.9989284855127335),\n",
       "   np.float64(0.9904129362106323),\n",
       "   np.float64(0.9931455910205841),\n",
       "   np.float64(0.988686089515686),\n",
       "   np.float64(0.9960350745916366),\n",
       "   np.float64(0.9900473004579544),\n",
       "   np.float64(0.9898899519443511),\n",
       "   np.float64(0.9959927052259445),\n",
       "   np.float64(0.9912576580047607),\n",
       "   np.float64(0.9870490419864655),\n",
       "   np.float64(0.9846318435668945),\n",
       "   np.float64(0.989564169049263),\n",
       "   np.float64(0.9794623225927352),\n",
       "   np.float64(0.983468524813652),\n",
       "   np.float64(0.9883453518152236),\n",
       "   np.float64(0.9896766954660415),\n",
       "   np.float64(0.9901027601957321),\n",
       "   np.float64(0.9874098539352417),\n",
       "   np.float64(0.9856304681301117),\n",
       "   np.float64(0.9785162925720214),\n",
       "   np.float64(0.9864123851060868),\n",
       "   np.float64(0.9777351468801498),\n",
       "   np.float64(0.9743256342411041),\n",
       "   np.float64(0.9843245506286621),\n",
       "   np.float64(0.9762587726116181),\n",
       "   np.float64(0.9782292759418487),\n",
       "   np.float64(0.9798019701242446),\n",
       "   np.float64(0.9764210796356201),\n",
       "   np.float64(0.9802143013477326),\n",
       "   np.float64(0.9752661103010177),\n",
       "   np.float64(0.9731055551767349),\n",
       "   np.float64(0.9771054023504258),\n",
       "   np.float64(0.9700587058067321),\n",
       "   np.float64(0.9631381750106811),\n",
       "   np.float64(0.9723863512277603),\n",
       "   np.float64(0.9709859371185303),\n",
       "   np.float64(0.9705619394779206),\n",
       "   np.float64(0.9673376876115799),\n",
       "   np.float64(0.9784211337566375),\n",
       "   np.float64(0.9666785794496536),\n",
       "   np.float64(0.9774406027793884),\n",
       "   np.float64(0.9684604823589325),\n",
       "   np.float64(0.9620492166280746),\n",
       "   np.float64(0.9696351915597916),\n",
       "   np.float64(0.9627094316482544),\n",
       "   np.float64(0.957352586388588),\n",
       "   np.float64(0.9678857612609864),\n",
       "   np.float64(0.9638800513744354),\n",
       "   np.float64(0.9727303367853165),\n",
       "   np.float64(0.9600268113613128),\n",
       "   np.float64(0.961319083571434),\n",
       "   np.float64(0.9633605766296387),\n",
       "   np.float64(0.9693879354000091),\n",
       "   np.float64(0.9709938097000123),\n",
       "   np.float64(0.9563404697179795),\n",
       "   np.float64(0.9552100372314453),\n",
       "   np.float64(0.9603517967462539),\n",
       "   np.float64(0.9612578922510147),\n",
       "   np.float64(0.9546048092842102),\n",
       "   np.float64(0.95809676527977),\n",
       "   np.float64(0.9601054000854492),\n",
       "   np.float64(0.9596370500326157),\n",
       "   np.float64(0.9574606120586395),\n",
       "   np.float64(0.957261148095131),\n",
       "   np.float64(0.9640096467733383),\n",
       "   np.float64(0.9654938459396363),\n",
       "   np.float64(0.9502760434150695),\n",
       "   np.float64(0.9636388850212098),\n",
       "   np.float64(0.9576211750507355),\n",
       "   np.float64(0.9525731664896011),\n",
       "   np.float64(0.9505879187583923),\n",
       "   np.float64(0.9480580556392669),\n",
       "   np.float64(0.9571033120155334),\n",
       "   np.float64(0.9448294895887375),\n",
       "   np.float64(0.9479577279090882),\n",
       "   np.float64(0.9553745919466019),\n",
       "   np.float64(0.9398090970516205),\n",
       "   np.float64(0.9530306452512741),\n",
       "   np.float64(0.9521933448314667),\n",
       "   np.float64(0.9516862285137176),\n",
       "   np.float64(0.9514829075336456),\n",
       "   np.float64(0.9498895633220673),\n",
       "   np.float64(0.9528616774082184),\n",
       "   np.float64(0.9438045746088028),\n",
       "   np.float64(0.9496693676710128),\n",
       "   np.float64(0.9568187755346298),\n",
       "   np.float64(0.9423077243566513),\n",
       "   np.float64(0.9481041401624679),\n",
       "   np.float64(0.9477921241521835),\n",
       "   np.float64(0.9498201304674149),\n",
       "   np.float64(0.9447182404994965),\n",
       "   np.float64(0.9523815715312958),\n",
       "   np.float64(0.9551521342992783),\n",
       "   np.float64(0.943023049235344),\n",
       "   np.float64(0.9535801410675049),\n",
       "   np.float64(0.9333249390125274),\n",
       "   np.float64(0.9426360809803009),\n",
       "   np.float64(0.9398920106887817),\n",
       "   np.float64(0.9433532470464706),\n",
       "   np.float64(0.9504406642913819),\n",
       "   np.float64(0.9390092384815216),\n",
       "   np.float64(0.9442842322587967),\n",
       "   np.float64(0.942159293293953),\n",
       "   np.float64(0.945387025475502),\n",
       "   np.float64(0.9463755428791046),\n",
       "   np.float64(0.9363203620910645),\n",
       "   np.float64(0.9445172417163848),\n",
       "   np.float64(0.9390390419960022),\n",
       "   np.float64(0.9462313121557235),\n",
       "   np.float64(0.9426704120635986),\n",
       "   np.float64(0.9409610068798065),\n",
       "   np.float64(0.9407683134078979),\n",
       "   np.float64(0.9373732143640519),\n",
       "   np.float64(0.9383672100305557),\n",
       "   np.float64(0.939172511100769),\n",
       "   np.float64(0.9448293483257294),\n",
       "   np.float64(0.9330203855037689),\n",
       "   np.float64(0.9419385319948197),\n",
       "   np.float64(0.9324691236019135),\n",
       "   np.float64(0.9366384607553482),\n",
       "   np.float64(0.9347081398963928),\n",
       "   np.float64(0.9333928126096726),\n",
       "   np.float64(0.9331948345899582),\n",
       "   np.float64(0.9314858931303024),\n",
       "   np.float64(0.9416567480564118),\n",
       "   np.float64(0.9451423889398575),\n",
       "   np.float64(0.933914903998375),\n",
       "   np.float64(0.9388956212997437),\n",
       "   np.float64(0.9311125767230988),\n",
       "   np.float64(0.9363308942317963),\n",
       "   np.float64(0.9380936598777772),\n",
       "   np.float64(0.9324217712879181),\n",
       "   np.float64(0.9333691787719727),\n",
       "   np.float64(0.9294275879859925),\n",
       "   np.float64(0.9289401614665985),\n",
       "   np.float64(0.9261052495241165),\n",
       "   np.float64(0.9347389769554139),\n",
       "   np.float64(0.92956099152565),\n",
       "   np.float64(0.9221368908882142),\n",
       "   np.float64(0.9271519732475281),\n",
       "   np.float64(0.9263037931919098),\n",
       "   np.float64(0.9311182135343552),\n",
       "   np.float64(0.9321255034208298),\n",
       "   np.float64(0.9244673770666122),\n",
       "   np.float64(0.928165186047554),\n",
       "   np.float64(0.9201232242584229),\n",
       "   np.float64(0.9232004296779632),\n",
       "   np.float64(0.9178263372182847),\n",
       "   np.float64(0.9266593915224075),\n",
       "   np.float64(0.9298854053020478),\n",
       "   np.float64(0.9192854171991348),\n",
       "   np.float64(0.9247561180591584),\n",
       "   np.float64(0.9186401963233948),\n",
       "   np.float64(0.9241048687696457),\n",
       "   np.float64(0.9271018421649933),\n",
       "   np.float64(0.9174803745746613),\n",
       "   np.float64(0.9224269276857376),\n",
       "   np.float64(0.9192182689905166),\n",
       "   np.float64(0.9211420750617981),\n",
       "   np.float64(0.9286743861436844),\n",
       "   np.float64(0.918188505768776),\n",
       "   np.float64(0.9256499975919723),\n",
       "   np.float64(0.9150940614938736),\n",
       "   np.float64(0.9158998322486878),\n",
       "   np.float64(0.9233812236785889),\n",
       "   np.float64(0.9246767669916153),\n",
       "   np.float64(0.9149412137269973),\n",
       "   np.float64(0.9114591902494431),\n",
       "   np.float64(0.9115735340118408),\n",
       "   np.float64(0.9138548517227173),\n",
       "   np.float64(0.9149709296226501),\n",
       "   np.float64(0.9155043715238571),\n",
       "   np.float64(0.9176074767112732),\n",
       "   np.float64(0.9053438842296601),\n",
       "   np.float64(0.9157653599977493),\n",
       "   np.float64(0.911464210152626),\n",
       "   np.float64(0.9106487089395523),\n",
       "   np.float64(0.9109312003850937),\n",
       "   np.float64(0.9076061487197876),\n",
       "   np.float64(0.9089850449562072),\n",
       "   np.float64(0.9075678211450576),\n",
       "   np.float64(0.9116222488880158),\n",
       "   np.float64(0.9042210727930069),\n",
       "   np.float64(0.9108844929933548),\n",
       "   np.float64(0.9034964847564697),\n",
       "   np.float64(0.908048865199089),\n",
       "   np.float64(0.9085826319456101),\n",
       "   np.float64(0.9095499223470688),\n",
       "   np.float64(0.9003341859579086),\n",
       "   np.float64(0.9061520832777024),\n",
       "   np.float64(0.909942597746849),\n",
       "   np.float64(0.9068116235733032),\n",
       "   np.float64(0.9016233140230179),\n",
       "   np.float64(0.9062361145019531),\n",
       "   np.float64(0.900970299243927),\n",
       "   np.float64(0.9039325505495072),\n",
       "   np.float64(0.8993732059001922),\n",
       "   np.float64(0.8948862218856811),\n",
       "   np.float64(0.9048398709297181),\n",
       "   np.float64(0.9030623328685761),\n",
       "   np.float64(0.8982406145334244),\n",
       "   np.float64(0.9092562198638916),\n",
       "   np.float64(0.8989564353227615),\n",
       "   np.float64(0.8956852877140045),\n",
       "   np.float64(0.9014552688598633),\n",
       "   np.float64(0.8885995626449585),\n",
       "   np.float64(0.8964377546310425),\n",
       "   np.float64(0.9008641481399536),\n",
       "   np.float64(0.8984140485525132),\n",
       "   np.float64(0.905545557141304),\n",
       "   np.float64(0.9006530565023422),\n",
       "   np.float64(0.8928363734483719),\n",
       "   np.float64(0.894290851354599),\n",
       "   np.float64(0.8961488217115402),\n",
       "   np.float64(0.884233437180519),\n",
       "   np.float64(0.8985773384571075),\n",
       "   np.float64(0.8988694137334824),\n",
       "   np.float64(0.8975735419988632),\n",
       "   np.float64(0.8897283625602722),\n",
       "   np.float64(0.8932980364561081),\n",
       "   np.float64(0.8921196341514588),\n",
       "   np.float64(0.8968838155269623),\n",
       "   np.float64(0.8964046913385392),\n",
       "   np.float64(0.8913737779855728),\n",
       "   np.float64(0.8953489500284195),\n",
       "   np.float64(0.8977040088176728),\n",
       "   np.float64(0.8939586675167084),\n",
       "   np.float64(0.8957536160945893),\n",
       "   np.float64(0.8866967976093292),\n",
       "   np.float64(0.8896474874019623),\n",
       "   np.float64(0.8849156302213669),\n",
       "   np.float64(0.8880045908689499),\n",
       "   np.float64(0.8896015608310699),\n",
       "   np.float64(0.8979007673263549),\n",
       "   np.float64(0.8892085599899292),\n",
       "   np.float64(0.8973078066110611),\n",
       "   np.float64(0.8973148053884507),\n",
       "   np.float64(0.8943125247955322),\n",
       "   np.float64(0.8933439391851425),\n",
       "   np.float64(0.889322429895401),\n",
       "   np.float64(0.8883397078514099),\n",
       "   np.float64(0.8922181612253189),\n",
       "   np.float64(0.8878364711999893),\n",
       "   np.float64(0.8805664259195328),\n",
       "   np.float64(0.889507361650467),\n",
       "   np.float64(0.8897958940267563),\n",
       "   np.float64(0.883592409491539),\n",
       "   np.float64(0.8850969672203064),\n",
       "   np.float64(0.8814071875810623),\n",
       "   np.float64(0.8904845970869064),\n",
       "   np.float64(0.8930813497304917),\n",
       "   np.float64(0.8776723456382751),\n",
       "   np.float64(0.883704559803009),\n",
       "   np.float64(0.8915395915508271),\n",
       "   np.float64(0.8882166075706482),\n",
       "   np.float64(0.8797885125875473),\n",
       "   np.float64(0.884040259718895),\n",
       "   np.float64(0.8893617111444473),\n",
       "   np.float64(0.8839604032039642),\n",
       "   np.float64(0.8848039519786834),\n",
       "   np.float64(0.8854382294416427),\n",
       "   np.float64(0.8812856954336167),\n",
       "   np.float64(0.8798807561397552),\n",
       "   np.float64(0.8834868353605271),\n",
       "   np.float64(0.8805641716718674),\n",
       "   np.float64(0.8800415074825287),\n",
       "   np.float64(0.8846126407384872),\n",
       "   np.float64(0.8923277032375335),\n",
       "   np.float64(0.8822720509767532),\n",
       "   np.float64(0.8781693613529206),\n",
       "   np.float64(0.8755441272258758),\n",
       "   np.float64(0.8821709614992141),\n",
       "   np.float64(0.8777968394756317),\n",
       "   np.float64(0.8793742382526397),\n",
       "   np.float64(0.8793516433238984),\n",
       "   np.float64(0.8816716712713242),\n",
       "   np.float64(0.8722206848859787),\n",
       "   np.float64(0.8779591345787048),\n",
       "   np.float64(0.8807408171892166)],\n",
       "  'train_mdn_loss_std': [np.float64(0.6763103792657091),\n",
       "   np.float64(0.02508472950025115),\n",
       "   np.float64(0.016791270007859217),\n",
       "   np.float64(0.014102519150847467),\n",
       "   np.float64(0.014115701178399038),\n",
       "   np.float64(0.013745606056483332),\n",
       "   np.float64(0.012102773718814814),\n",
       "   np.float64(0.012581144206564629),\n",
       "   np.float64(0.01136772174864554),\n",
       "   np.float64(0.011906403639179495),\n",
       "   np.float64(0.011287463044660538),\n",
       "   np.float64(0.01313901600256998),\n",
       "   np.float64(0.014348764938014257),\n",
       "   np.float64(0.012747218285610784),\n",
       "   np.float64(0.014177142428978074),\n",
       "   np.float64(0.013224394029592608),\n",
       "   np.float64(0.013061865072973767),\n",
       "   np.float64(0.013483139852610059),\n",
       "   np.float64(0.013705194507299111),\n",
       "   np.float64(0.013854823342749194),\n",
       "   np.float64(0.014322114709567142),\n",
       "   np.float64(0.01493253599149292),\n",
       "   np.float64(0.014646000259664617),\n",
       "   np.float64(0.016374869654135493),\n",
       "   np.float64(0.01576495030759459),\n",
       "   np.float64(0.01478248935194534),\n",
       "   np.float64(0.016100611607900727),\n",
       "   np.float64(0.016169622698828816),\n",
       "   np.float64(0.014834622818700207),\n",
       "   np.float64(0.016143361472328296),\n",
       "   np.float64(0.015617809064781045),\n",
       "   np.float64(0.01535513702068599),\n",
       "   np.float64(0.01658490451709768),\n",
       "   np.float64(0.013773228319189784),\n",
       "   np.float64(0.017840882309674004),\n",
       "   np.float64(0.01807023996405635),\n",
       "   np.float64(0.018916256479830636),\n",
       "   np.float64(0.018366030733238246),\n",
       "   np.float64(0.018327719130365478),\n",
       "   np.float64(0.019125019113303637),\n",
       "   np.float64(0.018480660493880766),\n",
       "   np.float64(0.017028385742434517),\n",
       "   np.float64(0.018594921077433293),\n",
       "   np.float64(0.01641148539640708),\n",
       "   np.float64(0.018264114321524676),\n",
       "   np.float64(0.021196805945581484),\n",
       "   np.float64(0.018115121958144326),\n",
       "   np.float64(0.020244935107932117),\n",
       "   np.float64(0.02167464310665449),\n",
       "   np.float64(0.02078827572373908),\n",
       "   np.float64(0.02155779672661134),\n",
       "   np.float64(0.02158986899457755),\n",
       "   np.float64(0.02419089883042915),\n",
       "   np.float64(0.02284229970069798),\n",
       "   np.float64(0.02096008962204453),\n",
       "   np.float64(0.021717093215797716),\n",
       "   np.float64(0.020329859347055827),\n",
       "   np.float64(0.023495486063629497),\n",
       "   np.float64(0.024424378847031164),\n",
       "   np.float64(0.022978542497777442),\n",
       "   np.float64(0.021248122020758122),\n",
       "   np.float64(0.024623814462275975),\n",
       "   np.float64(0.021560120566421245),\n",
       "   np.float64(0.022264964179805738),\n",
       "   np.float64(0.020489325779333236),\n",
       "   np.float64(0.02344469140988863),\n",
       "   np.float64(0.023250714222413753),\n",
       "   np.float64(0.02758990495041126),\n",
       "   np.float64(0.022834817755823696),\n",
       "   np.float64(0.02256369415512634),\n",
       "   np.float64(0.024485321138373574),\n",
       "   np.float64(0.026740359877028667),\n",
       "   np.float64(0.025128548932745996),\n",
       "   np.float64(0.02474788707339171),\n",
       "   np.float64(0.02293716060983928),\n",
       "   np.float64(0.02597678979750614),\n",
       "   np.float64(0.029872838052604763),\n",
       "   np.float64(0.02734385781558782),\n",
       "   np.float64(0.027174626486186595),\n",
       "   np.float64(0.025768543573103292),\n",
       "   np.float64(0.02781441760072702),\n",
       "   np.float64(0.027038415934262042),\n",
       "   np.float64(0.027951222615879772),\n",
       "   np.float64(0.02713233497047952),\n",
       "   np.float64(0.026097482137377902),\n",
       "   np.float64(0.026646118858152497),\n",
       "   np.float64(0.030619997421484697),\n",
       "   np.float64(0.029811653477764894),\n",
       "   np.float64(0.028918878814203398),\n",
       "   np.float64(0.03204713255233508),\n",
       "   np.float64(0.022327941091078045),\n",
       "   np.float64(0.029639690361475176),\n",
       "   np.float64(0.028210879096328387),\n",
       "   np.float64(0.028792423739132728),\n",
       "   np.float64(0.02737407159792259),\n",
       "   np.float64(0.03404937534185696),\n",
       "   np.float64(0.02537996241243565),\n",
       "   np.float64(0.033218879316039944),\n",
       "   np.float64(0.03097905717961137),\n",
       "   np.float64(0.03048649079292002),\n",
       "   np.float64(0.02697475519806928),\n",
       "   np.float64(0.03235666465241929),\n",
       "   np.float64(0.032825954381829586),\n",
       "   np.float64(0.03526724414789243),\n",
       "   np.float64(0.03247303161607259),\n",
       "   np.float64(0.03256053505619293),\n",
       "   np.float64(0.034089798601436384),\n",
       "   np.float64(0.03158783892607762),\n",
       "   np.float64(0.032552412338953625),\n",
       "   np.float64(0.031427513731400264),\n",
       "   np.float64(0.03232309552912292),\n",
       "   np.float64(0.029579431793628718),\n",
       "   np.float64(0.03049747029477962),\n",
       "   np.float64(0.029514237906821464),\n",
       "   np.float64(0.031081760089040643),\n",
       "   np.float64(0.03422646214603659),\n",
       "   np.float64(0.03557819738019458),\n",
       "   np.float64(0.0325335353450624),\n",
       "   np.float64(0.03612796691535997),\n",
       "   np.float64(0.03644412925991486),\n",
       "   np.float64(0.03334385587630042),\n",
       "   np.float64(0.030436241963130696),\n",
       "   np.float64(0.03474807714984212),\n",
       "   np.float64(0.03079772655500216),\n",
       "   np.float64(0.03397808386398419),\n",
       "   np.float64(0.035139800024970545),\n",
       "   np.float64(0.039334374402288874),\n",
       "   np.float64(0.03929900192620807),\n",
       "   np.float64(0.03238369314171531),\n",
       "   np.float64(0.03390545634242023),\n",
       "   np.float64(0.02735033602436069),\n",
       "   np.float64(0.03576851205116413),\n",
       "   np.float64(0.03615743247828969),\n",
       "   np.float64(0.034168051400813824),\n",
       "   np.float64(0.029166750236955617),\n",
       "   np.float64(0.0366530040326755),\n",
       "   np.float64(0.03699663381573862),\n",
       "   np.float64(0.04075073830690589),\n",
       "   np.float64(0.03744562021066109),\n",
       "   np.float64(0.03937042903768429),\n",
       "   np.float64(0.0317419808414448),\n",
       "   np.float64(0.035333452172735154),\n",
       "   np.float64(0.03908807741728001),\n",
       "   np.float64(0.030439196304133817),\n",
       "   np.float64(0.031196659864974188),\n",
       "   np.float64(0.03434883828547793),\n",
       "   np.float64(0.036369872523335005),\n",
       "   np.float64(0.03224201939914949),\n",
       "   np.float64(0.031541140010623615),\n",
       "   np.float64(0.03619277228674328),\n",
       "   np.float64(0.03778604122572756),\n",
       "   np.float64(0.03553854227333703),\n",
       "   np.float64(0.03266206582069642),\n",
       "   np.float64(0.03537509822991565),\n",
       "   np.float64(0.03970387760330138),\n",
       "   np.float64(0.0352937755131354),\n",
       "   np.float64(0.043102865705068535),\n",
       "   np.float64(0.0321767027985431),\n",
       "   np.float64(0.032604792205752776),\n",
       "   np.float64(0.04532573135564579),\n",
       "   np.float64(0.0365397975627745),\n",
       "   np.float64(0.03666643396533699),\n",
       "   np.float64(0.039571192819986695),\n",
       "   np.float64(0.03828706044743224),\n",
       "   np.float64(0.038151874557776744),\n",
       "   np.float64(0.034372461129665105),\n",
       "   np.float64(0.039497419220797456),\n",
       "   np.float64(0.03859668084540373),\n",
       "   np.float64(0.03770753939494191),\n",
       "   np.float64(0.03753266478811496),\n",
       "   np.float64(0.03724733706150404),\n",
       "   np.float64(0.03864564500790281),\n",
       "   np.float64(0.038039329242947444),\n",
       "   np.float64(0.03469830520697313),\n",
       "   np.float64(0.04162095835275068),\n",
       "   np.float64(0.03640487671753559),\n",
       "   np.float64(0.03970001613806681),\n",
       "   np.float64(0.03908743414216612),\n",
       "   np.float64(0.04301845647891735),\n",
       "   np.float64(0.03573443554012909),\n",
       "   np.float64(0.03528713524611424),\n",
       "   np.float64(0.0368554209781807),\n",
       "   np.float64(0.035884090921681),\n",
       "   np.float64(0.03638416315879533),\n",
       "   np.float64(0.0438747765070362),\n",
       "   np.float64(0.04229593681975555),\n",
       "   np.float64(0.0394075471619553),\n",
       "   np.float64(0.03587916938168895),\n",
       "   np.float64(0.03756947360464544),\n",
       "   np.float64(0.03763999113641215),\n",
       "   np.float64(0.03936766148764182),\n",
       "   np.float64(0.04243321904408395),\n",
       "   np.float64(0.040522385304113964),\n",
       "   np.float64(0.042347818096966645),\n",
       "   np.float64(0.04444101119707061),\n",
       "   np.float64(0.0368029652959102),\n",
       "   np.float64(0.04373763911862372),\n",
       "   np.float64(0.043801097400009675),\n",
       "   np.float64(0.03795637458823427),\n",
       "   np.float64(0.03835701322324223),\n",
       "   np.float64(0.04142609054519438),\n",
       "   np.float64(0.041663699998610346),\n",
       "   np.float64(0.0460920490320067),\n",
       "   np.float64(0.03932187035883987),\n",
       "   np.float64(0.04271562096308127),\n",
       "   np.float64(0.042364116602190686),\n",
       "   np.float64(0.03991757330048385),\n",
       "   np.float64(0.0422693147619395),\n",
       "   np.float64(0.04401159059007652),\n",
       "   np.float64(0.043024850069547024),\n",
       "   np.float64(0.038779893051934855),\n",
       "   np.float64(0.037435899576132255),\n",
       "   np.float64(0.04141762132990872),\n",
       "   np.float64(0.04526750592376917),\n",
       "   np.float64(0.04371277599384585),\n",
       "   np.float64(0.04570110542092486),\n",
       "   np.float64(0.04076497015285258),\n",
       "   np.float64(0.04051572109914047),\n",
       "   np.float64(0.04278311934510584),\n",
       "   np.float64(0.038758401908866646),\n",
       "   np.float64(0.0423622245072187),\n",
       "   np.float64(0.04412245810358507),\n",
       "   np.float64(0.03822753102246295),\n",
       "   np.float64(0.04040217535079647),\n",
       "   np.float64(0.037828702642020276),\n",
       "   np.float64(0.042525580424925255),\n",
       "   np.float64(0.04727281846296653),\n",
       "   np.float64(0.04136859826312204),\n",
       "   np.float64(0.03943928435712621),\n",
       "   np.float64(0.04476071677679547),\n",
       "   np.float64(0.04639284422532101),\n",
       "   np.float64(0.04078609294844982),\n",
       "   np.float64(0.04382274424028673),\n",
       "   np.float64(0.039018280851431844),\n",
       "   np.float64(0.04232913116326411),\n",
       "   np.float64(0.046860642298964146),\n",
       "   np.float64(0.03786063492493875),\n",
       "   np.float64(0.03968693468128419),\n",
       "   np.float64(0.03722134728285114),\n",
       "   np.float64(0.04333978139274204),\n",
       "   np.float64(0.04105111592133867),\n",
       "   np.float64(0.03622575042661546),\n",
       "   np.float64(0.04648479537552676),\n",
       "   np.float64(0.0385921745248571),\n",
       "   np.float64(0.04014535914018385),\n",
       "   np.float64(0.043718810773862304),\n",
       "   np.float64(0.04474441133104796),\n",
       "   np.float64(0.047985957786328255),\n",
       "   np.float64(0.04319985127674074),\n",
       "   np.float64(0.04254993629888262),\n",
       "   np.float64(0.039534218694994606),\n",
       "   np.float64(0.03755944004923143),\n",
       "   np.float64(0.04834752878632163),\n",
       "   np.float64(0.04933401585072363),\n",
       "   np.float64(0.04083468640691316),\n",
       "   np.float64(0.04563609189281191),\n",
       "   np.float64(0.04642239312961262),\n",
       "   np.float64(0.046575862796204084),\n",
       "   np.float64(0.04579611765277035),\n",
       "   np.float64(0.04311777790880092),\n",
       "   np.float64(0.05205236876150641),\n",
       "   np.float64(0.04679060187529144),\n",
       "   np.float64(0.04544850348453913),\n",
       "   np.float64(0.04347752063115577),\n",
       "   np.float64(0.03973435130311683),\n",
       "   np.float64(0.04362528541341809),\n",
       "   np.float64(0.047756036036153186),\n",
       "   np.float64(0.04333008698016006),\n",
       "   np.float64(0.042522281713023434),\n",
       "   np.float64(0.047508300746025335),\n",
       "   np.float64(0.043942107264286163),\n",
       "   np.float64(0.045428352542733266),\n",
       "   np.float64(0.04276152500267367),\n",
       "   np.float64(0.04636244614742748),\n",
       "   np.float64(0.04360050952284288),\n",
       "   np.float64(0.04249813475124869),\n",
       "   np.float64(0.04684044278750546),\n",
       "   np.float64(0.04269974405345449),\n",
       "   np.float64(0.04650835577256634),\n",
       "   np.float64(0.045429109155351456),\n",
       "   np.float64(0.04352167553574811),\n",
       "   np.float64(0.04587130240177965),\n",
       "   np.float64(0.049818383780672555),\n",
       "   np.float64(0.04658042213089485),\n",
       "   np.float64(0.03874905637832758),\n",
       "   np.float64(0.04563466991807066),\n",
       "   np.float64(0.04687472651937907),\n",
       "   np.float64(0.04997363141520366),\n",
       "   np.float64(0.04499206472758847),\n",
       "   np.float64(0.046974314204112314),\n",
       "   np.float64(0.04401830878179817),\n",
       "   np.float64(0.04313610000922875),\n",
       "   np.float64(0.04513828292964681),\n",
       "   np.float64(0.04567064055102022),\n",
       "   np.float64(0.0446514413460408),\n",
       "   np.float64(0.045213447501556286),\n",
       "   np.float64(0.05079516963943739),\n",
       "   np.float64(0.044141708821145535),\n",
       "   np.float64(0.04860323681978788),\n",
       "   np.float64(0.04683875128345703),\n",
       "   np.float64(0.04185313749980983),\n",
       "   np.float64(0.04654197340622547),\n",
       "   np.float64(0.03970390726275768),\n",
       "   np.float64(0.04133814621621878),\n",
       "   np.float64(0.04555244743011242),\n",
       "   np.float64(0.043027821605865366),\n",
       "   np.float64(0.045693634646576504),\n",
       "   np.float64(0.05441931858212937),\n",
       "   np.float64(0.048878301912950034),\n",
       "   np.float64(0.03950966438870923),\n",
       "   np.float64(0.04576596435638568),\n",
       "   np.float64(0.04793112098960244),\n",
       "   np.float64(0.04760769996720932),\n",
       "   np.float64(0.0462788316267718),\n",
       "   np.float64(0.04679405076623854),\n",
       "   np.float64(0.05010819473347623),\n",
       "   np.float64(0.044555715907370265),\n",
       "   np.float64(0.045949794584790336),\n",
       "   np.float64(0.04717419167338259),\n",
       "   np.float64(0.044882386044936724),\n",
       "   np.float64(0.048279184170134604),\n",
       "   np.float64(0.04998594495151123),\n",
       "   np.float64(0.045080615284111426),\n",
       "   np.float64(0.047782809747147666),\n",
       "   np.float64(0.03931310374138158),\n",
       "   np.float64(0.046552045357199937),\n",
       "   np.float64(0.043649364883444),\n",
       "   np.float64(0.04670883418474281),\n",
       "   np.float64(0.04693082307831473),\n",
       "   np.float64(0.04456096413924381),\n",
       "   np.float64(0.04556375086434006),\n",
       "   np.float64(0.04657909036823941),\n",
       "   np.float64(0.050041556963810865),\n",
       "   np.float64(0.04395144002901092),\n",
       "   np.float64(0.048074984274080475),\n",
       "   np.float64(0.04617745557868218),\n",
       "   np.float64(0.0455067307844137),\n",
       "   np.float64(0.04269403898103263),\n",
       "   np.float64(0.04185731304153865),\n",
       "   np.float64(0.04309375271688122),\n",
       "   np.float64(0.04690558353813377),\n",
       "   np.float64(0.043226235889396565),\n",
       "   np.float64(0.04789221333734312),\n",
       "   np.float64(0.0478000429240544),\n",
       "   np.float64(0.0477916980034585),\n",
       "   np.float64(0.040680778568215614),\n",
       "   np.float64(0.04947262476415679),\n",
       "   np.float64(0.048282920367502896),\n",
       "   np.float64(0.045430945960508055),\n",
       "   np.float64(0.04429210365921047),\n",
       "   np.float64(0.05251216973879188),\n",
       "   np.float64(0.05290368437897667),\n",
       "   np.float64(0.05520875247110403),\n",
       "   np.float64(0.045979872768813025),\n",
       "   np.float64(0.04764804547478158),\n",
       "   np.float64(0.04320532005610456),\n",
       "   np.float64(0.043747481590984794),\n",
       "   np.float64(0.04348877158167866),\n",
       "   np.float64(0.05490464839967935),\n",
       "   np.float64(0.04709809206085903),\n",
       "   np.float64(0.04629669015067589),\n",
       "   np.float64(0.049125104987272995),\n",
       "   np.float64(0.042233985071176164),\n",
       "   np.float64(0.048069014801574994),\n",
       "   np.float64(0.04803447066622082),\n",
       "   np.float64(0.050181263871200095),\n",
       "   np.float64(0.04835045701328716),\n",
       "   np.float64(0.04881802132622424),\n",
       "   np.float64(0.04862622075890216),\n",
       "   np.float64(0.04718669119707046),\n",
       "   np.float64(0.04607057027538811),\n",
       "   np.float64(0.04430295835054467),\n",
       "   np.float64(0.05089598308412318),\n",
       "   np.float64(0.046660795235178604),\n",
       "   np.float64(0.04486080491912219),\n",
       "   np.float64(0.04393697590894326),\n",
       "   np.float64(0.04872778750822794),\n",
       "   np.float64(0.047638407628424),\n",
       "   np.float64(0.049523662007828304),\n",
       "   np.float64(0.05094429786583461),\n",
       "   np.float64(0.05005862672006588),\n",
       "   np.float64(0.05525130391110874),\n",
       "   np.float64(0.050321521299370835),\n",
       "   np.float64(0.04935033243018418),\n",
       "   np.float64(0.04862019779572997),\n",
       "   np.float64(0.0444487928031328),\n",
       "   np.float64(0.04544975575973449),\n",
       "   np.float64(0.05237857507136749),\n",
       "   np.float64(0.05020845130599764),\n",
       "   np.float64(0.04892703948814475),\n",
       "   np.float64(0.049151830953784746),\n",
       "   np.float64(0.048419336522552724),\n",
       "   np.float64(0.05131463839678505),\n",
       "   np.float64(0.054610554140228626),\n",
       "   np.float64(0.0431407844829241),\n",
       "   np.float64(0.05404308151807803),\n",
       "   np.float64(0.05151157505401385),\n",
       "   np.float64(0.045161198401581476),\n",
       "   np.float64(0.04743857109899827),\n",
       "   np.float64(0.052041084270301935),\n",
       "   np.float64(0.056041286537169846),\n",
       "   np.float64(0.0508839502893805),\n",
       "   np.float64(0.050991014771827936),\n",
       "   np.float64(0.046222463222597),\n",
       "   np.float64(0.04923485761353874),\n",
       "   np.float64(0.04256203289506265),\n",
       "   np.float64(0.048350747734529645),\n",
       "   np.float64(0.05024616582873264),\n",
       "   np.float64(0.0477492518233223),\n",
       "   np.float64(0.05014153819831957),\n",
       "   np.float64(0.04511863438628336),\n",
       "   np.float64(0.046855412580428364),\n",
       "   np.float64(0.04917721394057832),\n",
       "   np.float64(0.05038502976821891),\n",
       "   np.float64(0.050212042819385946),\n",
       "   np.float64(0.04211242648124452),\n",
       "   np.float64(0.04818651303264535),\n",
       "   np.float64(0.04606091333806345),\n",
       "   np.float64(0.051055004325630954),\n",
       "   np.float64(0.053781854843412115),\n",
       "   np.float64(0.05185317551877434),\n",
       "   np.float64(0.05145709491002946),\n",
       "   np.float64(0.050525847493688415),\n",
       "   np.float64(0.049926266982378696),\n",
       "   np.float64(0.041615100932436075),\n",
       "   np.float64(0.04765223818757184),\n",
       "   np.float64(0.04671017962221249),\n",
       "   np.float64(0.051893972763377164),\n",
       "   np.float64(0.05099912307018807),\n",
       "   np.float64(0.04584988528191433),\n",
       "   np.float64(0.04751247743982128),\n",
       "   np.float64(0.04703939703101),\n",
       "   np.float64(0.053014815115908796),\n",
       "   np.float64(0.047224471316045034),\n",
       "   np.float64(0.057386252454967125),\n",
       "   np.float64(0.04553165077362441),\n",
       "   np.float64(0.05183656126560883)],\n",
       "  'train_affi_coeffs_mean': [np.float64(0.3380505297705531),\n",
       "   np.float64(0.33176758620887997),\n",
       "   np.float64(0.32797982171177864),\n",
       "   np.float64(0.3396976679936051),\n",
       "   np.float64(0.3261166164278984),\n",
       "   np.float64(0.3438388596102595),\n",
       "   np.float64(0.3188192930445075),\n",
       "   np.float64(0.3583414638414979),\n",
       "   np.float64(0.32602594353258607),\n",
       "   np.float64(0.33664086170494556),\n",
       "   np.float64(0.32624022468924524),\n",
       "   np.float64(0.33676761880517003),\n",
       "   np.float64(0.3429825768992305),\n",
       "   np.float64(0.3261746461689472),\n",
       "   np.float64(0.34102931164205075),\n",
       "   np.float64(0.34504669476300476),\n",
       "   np.float64(0.3512825825810432),\n",
       "   np.float64(0.33041355475783346),\n",
       "   np.float64(0.35093347672373054),\n",
       "   np.float64(0.35851746223866937),\n",
       "   np.float64(0.3383745499700308),\n",
       "   np.float64(0.3395137473940849),\n",
       "   np.float64(0.347823626101017),\n",
       "   np.float64(0.36387380223721266),\n",
       "   np.float64(0.3675508021004498),\n",
       "   np.float64(0.3455671909451485),\n",
       "   np.float64(0.33033505752682685),\n",
       "   np.float64(0.34403164152055976),\n",
       "   np.float64(0.34305427320301535),\n",
       "   np.float64(0.38079106964170933),\n",
       "   np.float64(0.32074853900820016),\n",
       "   np.float64(0.3447011977806687),\n",
       "   np.float64(0.36512393519282343),\n",
       "   np.float64(0.3565525717288256),\n",
       "   np.float64(0.3556820737570524),\n",
       "   np.float64(0.36817839082330467),\n",
       "   np.float64(0.3557140451297164),\n",
       "   np.float64(0.36383480273187163),\n",
       "   np.float64(0.3545921428501606),\n",
       "   np.float64(0.37159730985760686),\n",
       "   np.float64(0.36040421910583975),\n",
       "   np.float64(0.33826207457110286),\n",
       "   np.float64(0.35286096937954425),\n",
       "   np.float64(0.35913263849914073),\n",
       "   np.float64(0.38895655900239945),\n",
       "   np.float64(0.334832443613559),\n",
       "   np.float64(0.32549126006662843),\n",
       "   np.float64(0.3366773546487093),\n",
       "   np.float64(0.3560764617472887),\n",
       "   np.float64(0.35542939260601997),\n",
       "   np.float64(0.35416391626000404),\n",
       "   np.float64(0.3554662333056331),\n",
       "   np.float64(0.38737920425832273),\n",
       "   np.float64(0.37827448554337023),\n",
       "   np.float64(0.3522741032391787),\n",
       "   np.float64(0.3510526709258556),\n",
       "   np.float64(0.34326694265007973),\n",
       "   np.float64(0.38485556036233903),\n",
       "   np.float64(0.37406036477535964),\n",
       "   np.float64(0.38332550149410966),\n",
       "   np.float64(0.34196231722831727),\n",
       "   np.float64(0.37550957486033437),\n",
       "   np.float64(0.3328548474237323),\n",
       "   np.float64(0.39297035995870827),\n",
       "   np.float64(0.35703988451510665),\n",
       "   np.float64(0.36860975801944734),\n",
       "   np.float64(0.35792234919965266),\n",
       "   np.float64(0.3769559478759766),\n",
       "   np.float64(0.3889757815748453),\n",
       "   np.float64(0.3571519102156162),\n",
       "   np.float64(0.37422769993543625),\n",
       "   np.float64(0.35076514892280103),\n",
       "   np.float64(0.3504408665932715),\n",
       "   np.float64(0.3386013719439507),\n",
       "   np.float64(0.3402599550411105),\n",
       "   np.float64(0.36396847512573005),\n",
       "   np.float64(0.37707833081483844),\n",
       "   np.float64(0.37581554671749473),\n",
       "   np.float64(0.3652065708115697),\n",
       "   np.float64(0.3668878263235092),\n",
       "   np.float64(0.33674570042639973),\n",
       "   np.float64(0.3745446239411831),\n",
       "   np.float64(0.3383487331867218),\n",
       "   np.float64(0.35207152776420114),\n",
       "   np.float64(0.3649238428100944),\n",
       "   np.float64(0.3867780191451311),\n",
       "   np.float64(0.3635856306180358),\n",
       "   np.float64(0.3602741253748536),\n",
       "   np.float64(0.3530866456776857),\n",
       "   np.float64(0.36594880929216744),\n",
       "   np.float64(0.35490620457567273),\n",
       "   np.float64(0.3535214210674167),\n",
       "   np.float64(0.3601559107378125),\n",
       "   np.float64(0.373875624537468),\n",
       "   np.float64(0.3727497400343418),\n",
       "   np.float64(0.34913434807211163),\n",
       "   np.float64(0.36085786286741495),\n",
       "   np.float64(0.372821855917573),\n",
       "   np.float64(0.3646073650196195),\n",
       "   np.float64(0.3785196153074503),\n",
       "   np.float64(0.3688630505651236),\n",
       "   np.float64(0.35704305581748486),\n",
       "   np.float64(0.36382136076688765),\n",
       "   np.float64(0.3482906927913427),\n",
       "   np.float64(0.3693019200116396),\n",
       "   np.float64(0.362013535797596),\n",
       "   np.float64(0.35200986966490744),\n",
       "   np.float64(0.35356519736349584),\n",
       "   np.float64(0.37182887353003025),\n",
       "   np.float64(0.34060974687337875),\n",
       "   np.float64(0.383970302939415),\n",
       "   np.float64(0.32831636890769006),\n",
       "   np.float64(0.335946342702955),\n",
       "   np.float64(0.37980663672089576),\n",
       "   np.float64(0.3297469987347722),\n",
       "   np.float64(0.366743872910738),\n",
       "   np.float64(0.35037576243281365),\n",
       "   np.float64(0.3582989849150181),\n",
       "   np.float64(0.32328191988170146),\n",
       "   np.float64(0.3444053691253066),\n",
       "   np.float64(0.33968229688704016),\n",
       "   np.float64(0.32817298293113706),\n",
       "   np.float64(0.35689849987626077),\n",
       "   np.float64(0.36538168344646693),\n",
       "   np.float64(0.35926869690418245),\n",
       "   np.float64(0.3688058803230524),\n",
       "   np.float64(0.3596220505610108),\n",
       "   np.float64(0.37156333981081846),\n",
       "   np.float64(0.3667595597729087),\n",
       "   np.float64(0.34840886570513246),\n",
       "   np.float64(0.34683000344783066),\n",
       "   np.float64(0.3391115964576602),\n",
       "   np.float64(0.3493086659908295),\n",
       "   np.float64(0.3378247019648552),\n",
       "   np.float64(0.345752152428031),\n",
       "   np.float64(0.34056667640805244),\n",
       "   np.float64(0.32869906306266783),\n",
       "   np.float64(0.3365737345814705),\n",
       "   np.float64(0.3541592584550381),\n",
       "   np.float64(0.33145329028368),\n",
       "   np.float64(0.34211747292429207),\n",
       "   np.float64(0.3456525604426861),\n",
       "   np.float64(0.3530756602436304),\n",
       "   np.float64(0.3550059966742992),\n",
       "   np.float64(0.3295595935359597),\n",
       "   np.float64(0.3577174324542284),\n",
       "   np.float64(0.34758996315300467),\n",
       "   np.float64(0.3290575194358826),\n",
       "   np.float64(0.33770973823964595),\n",
       "   np.float64(0.3439688992127776),\n",
       "   np.float64(0.36279170952737333),\n",
       "   np.float64(0.3268652991950512),\n",
       "   np.float64(0.36497192941606044),\n",
       "   np.float64(0.3288958351314068),\n",
       "   np.float64(0.35028345307335257),\n",
       "   np.float64(0.31975144855678084),\n",
       "   np.float64(0.32682600133121015),\n",
       "   np.float64(0.35325188111513856),\n",
       "   np.float64(0.32843037314713003),\n",
       "   np.float64(0.3510989257320762),\n",
       "   np.float64(0.3518572974577546),\n",
       "   np.float64(0.34682089410722255),\n",
       "   np.float64(0.35253201439976695),\n",
       "   np.float64(0.32142315834760665),\n",
       "   np.float64(0.3440369327366352),\n",
       "   np.float64(0.3400092425197363),\n",
       "   np.float64(0.3730684757605195),\n",
       "   np.float64(0.3420440532267094),\n",
       "   np.float64(0.3296551572158933),\n",
       "   np.float64(0.3860079016536474),\n",
       "   np.float64(0.3477360308542848),\n",
       "   np.float64(0.3258550162240863),\n",
       "   np.float64(0.33638293609023096),\n",
       "   np.float64(0.34621146373450756),\n",
       "   np.float64(0.3176469644904137),\n",
       "   np.float64(0.31875173225998876),\n",
       "   np.float64(0.3349584309197962),\n",
       "   np.float64(0.3457122599706054),\n",
       "   np.float64(0.3228720775991678),\n",
       "   np.float64(0.3442924770340323),\n",
       "   np.float64(0.3377376714721322),\n",
       "   np.float64(0.3386364843323827),\n",
       "   np.float64(0.33829946037381886),\n",
       "   np.float64(0.3470442406833172),\n",
       "   np.float64(0.3511885406821966),\n",
       "   np.float64(0.3210283623635769),\n",
       "   np.float64(0.3410275848209858),\n",
       "   np.float64(0.3390335188060999),\n",
       "   np.float64(0.3254712225869298),\n",
       "   np.float64(0.34996461026370523),\n",
       "   np.float64(0.36008928932249545),\n",
       "   np.float64(0.3248492483049631),\n",
       "   np.float64(0.34821744937449695),\n",
       "   np.float64(0.35609171491116287),\n",
       "   np.float64(0.35926760986447337),\n",
       "   np.float64(0.32025857027620075),\n",
       "   np.float64(0.3757247366011143),\n",
       "   np.float64(0.33824315935373306),\n",
       "   np.float64(0.33823547683656213),\n",
       "   np.float64(0.30658547453582286),\n",
       "   np.float64(0.3227014639601111),\n",
       "   np.float64(0.2922877075895667),\n",
       "   np.float64(0.34633861381560566),\n",
       "   np.float64(0.3193935874849558),\n",
       "   np.float64(0.3472370723262429),\n",
       "   np.float64(0.344374518096447),\n",
       "   np.float64(0.3546639451012015),\n",
       "   np.float64(0.36327112328261135),\n",
       "   np.float64(0.32834094196558),\n",
       "   np.float64(0.3221647199243307),\n",
       "   np.float64(0.36648247741162776),\n",
       "   np.float64(0.34681667782366277),\n",
       "   np.float64(0.35611115008592603),\n",
       "   np.float64(0.33679931215941905),\n",
       "   np.float64(0.3392349860072136),\n",
       "   np.float64(0.3378221636451781),\n",
       "   np.float64(0.351333390660584),\n",
       "   np.float64(0.3480400478094816),\n",
       "   np.float64(0.3316490963473916),\n",
       "   np.float64(0.34424558028578756),\n",
       "   np.float64(0.33493134766817095),\n",
       "   np.float64(0.3384818374738097),\n",
       "   np.float64(0.33637629218399523),\n",
       "   np.float64(0.331681759878993),\n",
       "   np.float64(0.3328154453635216),\n",
       "   np.float64(0.33786008168011905),\n",
       "   np.float64(0.29502673145383596),\n",
       "   np.float64(0.3435931607708335),\n",
       "   np.float64(0.3473318234086037),\n",
       "   np.float64(0.3337606866657734),\n",
       "   np.float64(0.32158549696207045),\n",
       "   np.float64(0.31427899669855835),\n",
       "   np.float64(0.3132600655406714),\n",
       "   np.float64(0.3458914263546467),\n",
       "   np.float64(0.33880937747657297),\n",
       "   np.float64(0.33211630437523126),\n",
       "   np.float64(0.31972057443112134),\n",
       "   np.float64(0.31708485262468455),\n",
       "   np.float64(0.3481604843400419),\n",
       "   np.float64(0.28958521312102675),\n",
       "   np.float64(0.3577870763093233),\n",
       "   np.float64(0.330821034014225),\n",
       "   np.float64(0.3449127519875765),\n",
       "   np.float64(0.30169825565069913),\n",
       "   np.float64(0.36307519309222697),\n",
       "   np.float64(0.3552588423341513),\n",
       "   np.float64(0.30489507341757416),\n",
       "   np.float64(0.3448521392419934),\n",
       "   np.float64(0.3349555703997612),\n",
       "   np.float64(0.31876509442925455),\n",
       "   np.float64(0.3207857556268573),\n",
       "   np.float64(0.31459991704672574),\n",
       "   np.float64(0.3311170146614313),\n",
       "   np.float64(0.3284595509618521),\n",
       "   np.float64(0.336815019287169),\n",
       "   np.float64(0.32588615382090214),\n",
       "   np.float64(0.31998830154538155),\n",
       "   np.float64(0.32147672984749076),\n",
       "   np.float64(0.3337262238189578),\n",
       "   np.float64(0.3353365998715162),\n",
       "   np.float64(0.2941428862139583),\n",
       "   np.float64(0.32655702186748387),\n",
       "   np.float64(0.3268136278167367),\n",
       "   np.float64(0.3277721630781889),\n",
       "   np.float64(0.3420542387291789),\n",
       "   np.float64(0.33057326450943947),\n",
       "   np.float64(0.3452200691401959),\n",
       "   np.float64(0.34687137067317964),\n",
       "   np.float64(0.30747577479109167),\n",
       "   np.float64(0.3150765013694763),\n",
       "   np.float64(0.3201815443299711),\n",
       "   np.float64(0.33258369788527486),\n",
       "   np.float64(0.3414402912557125),\n",
       "   np.float64(0.31394201710820197),\n",
       "   np.float64(0.3424715689569712),\n",
       "   np.float64(0.32363623593002555),\n",
       "   np.float64(0.33908473640680314),\n",
       "   np.float64(0.31728742517530917),\n",
       "   np.float64(0.3023305794596672),\n",
       "   np.float64(0.3002261628303677),\n",
       "   np.float64(0.3000821953546256),\n",
       "   np.float64(0.3132926968112588),\n",
       "   np.float64(0.3318012825027108),\n",
       "   np.float64(0.3310846173763275),\n",
       "   np.float64(0.30943750746548176),\n",
       "   np.float64(0.35339976400136947),\n",
       "   np.float64(0.3368236958421767),\n",
       "   np.float64(0.3431971654295921),\n",
       "   np.float64(0.3316358831897378),\n",
       "   np.float64(0.34520198695361615),\n",
       "   np.float64(0.2965302226133645),\n",
       "   np.float64(0.3208692031353712),\n",
       "   np.float64(0.32051427019760015),\n",
       "   np.float64(0.34266584768891334),\n",
       "   np.float64(0.3456556163728237),\n",
       "   np.float64(0.31332029901444913),\n",
       "   np.float64(0.3421086975932121),\n",
       "   np.float64(0.3373210378736258),\n",
       "   np.float64(0.31699610462412237),\n",
       "   np.float64(0.30324640810489656),\n",
       "   np.float64(0.30419170212000607),\n",
       "   np.float64(0.3189357593283057),\n",
       "   np.float64(0.3205789369530976),\n",
       "   np.float64(0.29268158923834564),\n",
       "   np.float64(0.32421488847583535),\n",
       "   np.float64(0.3337710371054709),\n",
       "   np.float64(0.35430492646992207),\n",
       "   np.float64(0.33044200763106346),\n",
       "   np.float64(0.3419103716313839),\n",
       "   np.float64(0.32672036211937666),\n",
       "   np.float64(0.3346977280266583),\n",
       "   np.float64(0.34663498677313326),\n",
       "   np.float64(0.32681119322776797),\n",
       "   np.float64(0.3364912639558315),\n",
       "   np.float64(0.3349726400151849),\n",
       "   np.float64(0.3360539964400232),\n",
       "   np.float64(0.31703884303569796),\n",
       "   np.float64(0.31317395832389594),\n",
       "   np.float64(0.33141798969358205),\n",
       "   np.float64(0.3495676440745592),\n",
       "   np.float64(0.31313610687851906),\n",
       "   np.float64(0.3132878924161196),\n",
       "   np.float64(0.3368525841832161),\n",
       "   np.float64(0.33796392049640417),\n",
       "   np.float64(0.3301900961995125),\n",
       "   np.float64(0.3574321806430817),\n",
       "   np.float64(0.32479756627231837),\n",
       "   np.float64(0.344604006446898),\n",
       "   np.float64(0.34352366119623184),\n",
       "   np.float64(0.3205686626583338),\n",
       "   np.float64(0.3192407174408436),\n",
       "   np.float64(0.32807886600494385),\n",
       "   np.float64(0.32079748859629037),\n",
       "   np.float64(0.34366720240563153),\n",
       "   np.float64(0.3497733693569899),\n",
       "   np.float64(0.35183672508224845),\n",
       "   np.float64(0.3091739123314619),\n",
       "   np.float64(0.33797292176634075),\n",
       "   np.float64(0.34703575402498243),\n",
       "   np.float64(0.31713350232690574),\n",
       "   np.float64(0.32046816643327475),\n",
       "   np.float64(0.3308708498626947),\n",
       "   np.float64(0.33102538660168646),\n",
       "   np.float64(0.33958681413903835),\n",
       "   np.float64(0.3309149605408311),\n",
       "   np.float64(0.3077002625539899),\n",
       "   np.float64(0.32332430422306063),\n",
       "   np.float64(0.33759809404611585),\n",
       "   np.float64(0.303809569850564),\n",
       "   np.float64(0.32219499014317987),\n",
       "   np.float64(0.3155462933704257),\n",
       "   np.float64(0.32810500241816043),\n",
       "   np.float64(0.3393809723854065),\n",
       "   np.float64(0.3211944341287017),\n",
       "   np.float64(0.3331423017941415),\n",
       "   np.float64(0.3268811147660017),\n",
       "   np.float64(0.30351713933050634),\n",
       "   np.float64(0.32056121699512),\n",
       "   np.float64(0.3385653307288885),\n",
       "   np.float64(0.3208111494034529),\n",
       "   np.float64(0.33019437711685895),\n",
       "   np.float64(0.31367414209991695),\n",
       "   np.float64(0.3162502317503095),\n",
       "   np.float64(0.30412706807255746),\n",
       "   np.float64(0.32112737189978363),\n",
       "   np.float64(0.3356255889311433),\n",
       "   np.float64(0.30807320289313794),\n",
       "   np.float64(0.329801567979157),\n",
       "   np.float64(0.3163272082805634),\n",
       "   np.float64(0.3242562600504607),\n",
       "   np.float64(0.3196433652937412),\n",
       "   np.float64(0.3341844902932644),\n",
       "   np.float64(0.3100047282874584),\n",
       "   np.float64(0.3165042879804969),\n",
       "   np.float64(0.3030376038700342),\n",
       "   np.float64(0.3208095798641443),\n",
       "   np.float64(0.31842535860836507),\n",
       "   np.float64(0.3085369014646858),\n",
       "   np.float64(0.3043373109027743),\n",
       "   np.float64(0.34366896882653236),\n",
       "   np.float64(0.31953985650092365),\n",
       "   np.float64(0.3129427268728614),\n",
       "   np.float64(0.2998492281138897),\n",
       "   np.float64(0.3155062966048717),\n",
       "   np.float64(0.3403870861977339),\n",
       "   np.float64(0.3284127334970981),\n",
       "   np.float64(0.33897704519331456),\n",
       "   np.float64(0.32334767904132605),\n",
       "   np.float64(0.3251848728954792),\n",
       "   np.float64(0.33501946307718755),\n",
       "   np.float64(0.3564327522367239),\n",
       "   np.float64(0.33505609050393104),\n",
       "   np.float64(0.28970074735581874),\n",
       "   np.float64(0.3246170426160097),\n",
       "   np.float64(0.3302443171851337),\n",
       "   np.float64(0.29651574630290267),\n",
       "   np.float64(0.31282153443433347),\n",
       "   np.float64(0.29572703536599876),\n",
       "   np.float64(0.33932655829936265),\n",
       "   np.float64(0.3087921667844057),\n",
       "   np.float64(0.3251883740723133),\n",
       "   np.float64(0.3352731489017606),\n",
       "   np.float64(0.30861074969172475),\n",
       "   np.float64(0.311797591149807),\n",
       "   np.float64(0.3197139051184058),\n",
       "   np.float64(0.3445286624878645),\n",
       "   np.float64(0.27353427052497864),\n",
       "   np.float64(0.330535023547709),\n",
       "   np.float64(0.31993298295885325),\n",
       "   np.float64(0.3322264587134123),\n",
       "   np.float64(0.3131951290369034),\n",
       "   np.float64(0.3024655622430146),\n",
       "   np.float64(0.31908662244677544),\n",
       "   np.float64(0.33473995644599197),\n",
       "   np.float64(0.3351798417419195),\n",
       "   np.float64(0.3193180776387453),\n",
       "   np.float64(0.31347827799618244),\n",
       "   np.float64(0.32089681923389435),\n",
       "   np.float64(0.32690867777913807),\n",
       "   np.float64(0.3280411824025214),\n",
       "   np.float64(0.3194532920047641),\n",
       "   np.float64(0.2954208220168948),\n",
       "   np.float64(0.31784604988992216),\n",
       "   np.float64(0.35059889003634453),\n",
       "   np.float64(0.3166015463322401),\n",
       "   np.float64(0.2911970567703247),\n",
       "   np.float64(0.3263662923499942),\n",
       "   np.float64(0.30932789778336883),\n",
       "   np.float64(0.3116943645849824),\n",
       "   np.float64(0.33263209108263253),\n",
       "   np.float64(0.31702363535761835),\n",
       "   np.float64(0.34385102689266206),\n",
       "   np.float64(0.3181167643517256),\n",
       "   np.float64(0.30390290424227717),\n",
       "   np.float64(0.34121380142867563),\n",
       "   np.float64(0.32504878625273703),\n",
       "   np.float64(0.29275714591145513)],\n",
       "  'train_affi_coeffs_std': [np.float64(0.14566918491938458),\n",
       "   np.float64(0.13298746236158773),\n",
       "   np.float64(0.1363052583915486),\n",
       "   np.float64(0.1416268270635953),\n",
       "   np.float64(0.1373977150507784),\n",
       "   np.float64(0.14120667963270866),\n",
       "   np.float64(0.14189688664020142),\n",
       "   np.float64(0.13628723740340126),\n",
       "   np.float64(0.14234538134168728),\n",
       "   np.float64(0.16047724633963975),\n",
       "   np.float64(0.15572311034191913),\n",
       "   np.float64(0.1454602148471136),\n",
       "   np.float64(0.17558042841126906),\n",
       "   np.float64(0.14734793635006835),\n",
       "   np.float64(0.12870217073951057),\n",
       "   np.float64(0.14633517343696156),\n",
       "   np.float64(0.16053453905038464),\n",
       "   np.float64(0.1310894771822026),\n",
       "   np.float64(0.1429316797379713),\n",
       "   np.float64(0.1523114467345232),\n",
       "   np.float64(0.1493645588351387),\n",
       "   np.float64(0.1267790760390508),\n",
       "   np.float64(0.15424802770295504),\n",
       "   np.float64(0.12629105748495073),\n",
       "   np.float64(0.1341836597691119),\n",
       "   np.float64(0.14456011042297184),\n",
       "   np.float64(0.13253672999268287),\n",
       "   np.float64(0.13605790386515096),\n",
       "   np.float64(0.14110303628265292),\n",
       "   np.float64(0.13339766672906636),\n",
       "   np.float64(0.14235301219030774),\n",
       "   np.float64(0.15898057130400084),\n",
       "   np.float64(0.15740792085196587),\n",
       "   np.float64(0.1411398954665351),\n",
       "   np.float64(0.13387222075342406),\n",
       "   np.float64(0.14426431983069013),\n",
       "   np.float64(0.1403358257469767),\n",
       "   np.float64(0.1362812497564451),\n",
       "   np.float64(0.13442770398819842),\n",
       "   np.float64(0.1204523646710594),\n",
       "   np.float64(0.14062333397684854),\n",
       "   np.float64(0.14578079258098348),\n",
       "   np.float64(0.141252907780809),\n",
       "   np.float64(0.14861753932826455),\n",
       "   np.float64(0.13903391456400982),\n",
       "   np.float64(0.15028905638711273),\n",
       "   np.float64(0.14753100135966182),\n",
       "   np.float64(0.14870054850820108),\n",
       "   np.float64(0.14294238893128394),\n",
       "   np.float64(0.14079668671925905),\n",
       "   np.float64(0.13205244126849958),\n",
       "   np.float64(0.16063909414426403),\n",
       "   np.float64(0.13533151047330963),\n",
       "   np.float64(0.13159453493827733),\n",
       "   np.float64(0.14491241422660095),\n",
       "   np.float64(0.14152832120822334),\n",
       "   np.float64(0.13519713159409846),\n",
       "   np.float64(0.11926841307975575),\n",
       "   np.float64(0.14126102645622557),\n",
       "   np.float64(0.14464625668483605),\n",
       "   np.float64(0.13311679664985518),\n",
       "   np.float64(0.13531487088416466),\n",
       "   np.float64(0.1545672775984521),\n",
       "   np.float64(0.1343090105620708),\n",
       "   np.float64(0.15507347794456428),\n",
       "   np.float64(0.1496705924306509),\n",
       "   np.float64(0.15479216429409332),\n",
       "   np.float64(0.16249464864399815),\n",
       "   np.float64(0.1394581062285129),\n",
       "   np.float64(0.1487238534987865),\n",
       "   np.float64(0.1264148102930178),\n",
       "   np.float64(0.14551737554058639),\n",
       "   np.float64(0.14931652455545533),\n",
       "   np.float64(0.13297775903659687),\n",
       "   np.float64(0.14630922862077922),\n",
       "   np.float64(0.13987653040386597),\n",
       "   np.float64(0.16016936106578913),\n",
       "   np.float64(0.11841340142063413),\n",
       "   np.float64(0.15702856814139646),\n",
       "   np.float64(0.14887547079613347),\n",
       "   np.float64(0.15390092889453827),\n",
       "   np.float64(0.1432701514862227),\n",
       "   np.float64(0.136923918143443),\n",
       "   np.float64(0.14430693794076246),\n",
       "   np.float64(0.13354825423596672),\n",
       "   np.float64(0.13656293107873105),\n",
       "   np.float64(0.14785269092442116),\n",
       "   np.float64(0.1642903860561323),\n",
       "   np.float64(0.1410145027965733),\n",
       "   np.float64(0.15408911537409256),\n",
       "   np.float64(0.13524143798580807),\n",
       "   np.float64(0.1405920495342538),\n",
       "   np.float64(0.15566054751149894),\n",
       "   np.float64(0.14911574419744983),\n",
       "   np.float64(0.1323980953791063),\n",
       "   np.float64(0.14641551411384204),\n",
       "   np.float64(0.13139701887285538),\n",
       "   np.float64(0.14043838321250846),\n",
       "   np.float64(0.14944905602336495),\n",
       "   np.float64(0.1441175803291453),\n",
       "   np.float64(0.1467146489034922),\n",
       "   np.float64(0.14285563615609537),\n",
       "   np.float64(0.14784471750710507),\n",
       "   np.float64(0.1471880129779755),\n",
       "   np.float64(0.13789692835080034),\n",
       "   np.float64(0.14907981409547105),\n",
       "   np.float64(0.14843897830897765),\n",
       "   np.float64(0.1452246155596357),\n",
       "   np.float64(0.14202893390587068),\n",
       "   np.float64(0.13960479681606167),\n",
       "   np.float64(0.1363837927625926),\n",
       "   np.float64(0.1411012087370498),\n",
       "   np.float64(0.15895184947210883),\n",
       "   np.float64(0.14945971107279468),\n",
       "   np.float64(0.15681514129571986),\n",
       "   np.float64(0.15357801254459835),\n",
       "   np.float64(0.14098099465825692),\n",
       "   np.float64(0.1465871506777647),\n",
       "   np.float64(0.1309188237364809),\n",
       "   np.float64(0.1427826154018817),\n",
       "   np.float64(0.12423238850006496),\n",
       "   np.float64(0.1289325222596377),\n",
       "   np.float64(0.14475423773760668),\n",
       "   np.float64(0.1256343791855148),\n",
       "   np.float64(0.1428374921215593),\n",
       "   np.float64(0.15577090816311753),\n",
       "   np.float64(0.1485146084269858),\n",
       "   np.float64(0.16029837807997055),\n",
       "   np.float64(0.1485916496026933),\n",
       "   np.float64(0.15351439665488756),\n",
       "   np.float64(0.15086973239145887),\n",
       "   np.float64(0.17217585591933948),\n",
       "   np.float64(0.1256075500508062),\n",
       "   np.float64(0.13982506465625646),\n",
       "   np.float64(0.12785721962181854),\n",
       "   np.float64(0.14444290920634686),\n",
       "   np.float64(0.15552372101031392),\n",
       "   np.float64(0.1531335060095989),\n",
       "   np.float64(0.12701885171928137),\n",
       "   np.float64(0.15027846904226697),\n",
       "   np.float64(0.15985096804303017),\n",
       "   np.float64(0.1578707709982272),\n",
       "   np.float64(0.1321113686436905),\n",
       "   np.float64(0.1488940667277557),\n",
       "   np.float64(0.14536833938727295),\n",
       "   np.float64(0.1590187554090961),\n",
       "   np.float64(0.15205675472328645),\n",
       "   np.float64(0.13765511998495886),\n",
       "   np.float64(0.13281546646872064),\n",
       "   np.float64(0.15731886347988608),\n",
       "   np.float64(0.15721363005284458),\n",
       "   np.float64(0.12634479450614622),\n",
       "   np.float64(0.14010441287186715),\n",
       "   np.float64(0.1469973004805121),\n",
       "   np.float64(0.13513213846476918),\n",
       "   np.float64(0.14671370722151678),\n",
       "   np.float64(0.14162155776640153),\n",
       "   np.float64(0.13495719623196611),\n",
       "   np.float64(0.1589688290120822),\n",
       "   np.float64(0.1375746533326523),\n",
       "   np.float64(0.1314203230309933),\n",
       "   np.float64(0.1534294949580286),\n",
       "   np.float64(0.1463580870421192),\n",
       "   np.float64(0.1403800419595191),\n",
       "   np.float64(0.13461316062334572),\n",
       "   np.float64(0.14869943686212533),\n",
       "   np.float64(0.14159828304133262),\n",
       "   np.float64(0.1348105727463738),\n",
       "   np.float64(0.14929567543932964),\n",
       "   np.float64(0.13610708654851503),\n",
       "   np.float64(0.1577447093837917),\n",
       "   np.float64(0.16436204641389454),\n",
       "   np.float64(0.14514255231993528),\n",
       "   np.float64(0.1548381289099651),\n",
       "   np.float64(0.14643306962474753),\n",
       "   np.float64(0.1523958304063393),\n",
       "   np.float64(0.1443545483114476),\n",
       "   np.float64(0.1547747924466753),\n",
       "   np.float64(0.14265290482307985),\n",
       "   np.float64(0.15275497234490856),\n",
       "   np.float64(0.1510372196437514),\n",
       "   np.float64(0.1367352539519147),\n",
       "   np.float64(0.13883626379129985),\n",
       "   np.float64(0.1379573760848497),\n",
       "   np.float64(0.15447098574726467),\n",
       "   np.float64(0.1352883661693032),\n",
       "   np.float64(0.13969634179203663),\n",
       "   np.float64(0.1595819847531457),\n",
       "   np.float64(0.12647264341159767),\n",
       "   np.float64(0.1349051653504877),\n",
       "   np.float64(0.15001659949259893),\n",
       "   np.float64(0.16010447888484344),\n",
       "   np.float64(0.14917357325268257),\n",
       "   np.float64(0.1662656220211114),\n",
       "   np.float64(0.12674628660495493),\n",
       "   np.float64(0.13720607929464865),\n",
       "   np.float64(0.15121360464611044),\n",
       "   np.float64(0.14410133455829674),\n",
       "   np.float64(0.138570843160049),\n",
       "   np.float64(0.14734057704699743),\n",
       "   np.float64(0.14429374666609202),\n",
       "   np.float64(0.15868307236447998),\n",
       "   np.float64(0.1397403327959579),\n",
       "   np.float64(0.14503908557263562),\n",
       "   np.float64(0.16820953002029637),\n",
       "   np.float64(0.16091253656316037),\n",
       "   np.float64(0.14248525981111237),\n",
       "   np.float64(0.154511022139475),\n",
       "   np.float64(0.13107125144665932),\n",
       "   np.float64(0.15437865765017036),\n",
       "   np.float64(0.14163524869880467),\n",
       "   np.float64(0.14015225158629577),\n",
       "   np.float64(0.16620536366780853),\n",
       "   np.float64(0.14467511065639738),\n",
       "   np.float64(0.13395140096662572),\n",
       "   np.float64(0.15399561363965364),\n",
       "   np.float64(0.15481391700041922),\n",
       "   np.float64(0.15060797896832684),\n",
       "   np.float64(0.15115992509986614),\n",
       "   np.float64(0.13606369295911927),\n",
       "   np.float64(0.13620505508569605),\n",
       "   np.float64(0.14465284683231344),\n",
       "   np.float64(0.15476521027270537),\n",
       "   np.float64(0.15455622430559152),\n",
       "   np.float64(0.14491824233418832),\n",
       "   np.float64(0.1425450448249715),\n",
       "   np.float64(0.14733836224807312),\n",
       "   np.float64(0.1432953869258262),\n",
       "   np.float64(0.15110237088193582),\n",
       "   np.float64(0.14630145031119582),\n",
       "   np.float64(0.13485362318718885),\n",
       "   np.float64(0.1501682495248544),\n",
       "   np.float64(0.15957525473349493),\n",
       "   np.float64(0.13755692457326352),\n",
       "   np.float64(0.13485590892119917),\n",
       "   np.float64(0.1508763213044004),\n",
       "   np.float64(0.1506492576937531),\n",
       "   np.float64(0.1480323173003963),\n",
       "   np.float64(0.15899175093494602),\n",
       "   np.float64(0.14923516920705257),\n",
       "   np.float64(0.16046984523678234),\n",
       "   np.float64(0.12691672441636312),\n",
       "   np.float64(0.14668197521172363),\n",
       "   np.float64(0.15868343220128453),\n",
       "   np.float64(0.12052101125548105),\n",
       "   np.float64(0.1464697120221737),\n",
       "   np.float64(0.14732055066371247),\n",
       "   np.float64(0.1509185990197195),\n",
       "   np.float64(0.1294584321593353),\n",
       "   np.float64(0.15790884188342197),\n",
       "   np.float64(0.15293249455985952),\n",
       "   np.float64(0.13359336206105277),\n",
       "   np.float64(0.1545282291653158),\n",
       "   np.float64(0.14269699192686283),\n",
       "   np.float64(0.1639267101589978),\n",
       "   np.float64(0.15319083420564325),\n",
       "   np.float64(0.1303181880494693),\n",
       "   np.float64(0.15113176589590413),\n",
       "   np.float64(0.13891531235580276),\n",
       "   np.float64(0.15620079925985647),\n",
       "   np.float64(0.14479150329386023),\n",
       "   np.float64(0.14680944702934495),\n",
       "   np.float64(0.1330084364414885),\n",
       "   np.float64(0.15266835724825686),\n",
       "   np.float64(0.14855156907363418),\n",
       "   np.float64(0.13816253366717898),\n",
       "   np.float64(0.1534143418843031),\n",
       "   np.float64(0.1416923176076843),\n",
       "   np.float64(0.13251888713212934),\n",
       "   np.float64(0.15344109338280598),\n",
       "   np.float64(0.16549355488136702),\n",
       "   np.float64(0.15261515222053512),\n",
       "   np.float64(0.14764584289437901),\n",
       "   np.float64(0.13458551159307633),\n",
       "   np.float64(0.14657431505108098),\n",
       "   np.float64(0.14407841105062946),\n",
       "   np.float64(0.150016459160915),\n",
       "   np.float64(0.15535014824402493),\n",
       "   np.float64(0.14719742330362806),\n",
       "   np.float64(0.14598558237429662),\n",
       "   np.float64(0.1476129042384637),\n",
       "   np.float64(0.13788420545582714),\n",
       "   np.float64(0.16491954921225252),\n",
       "   np.float64(0.12272351353661791),\n",
       "   np.float64(0.1462583886609262),\n",
       "   np.float64(0.1383791605599649),\n",
       "   np.float64(0.15933815812903107),\n",
       "   np.float64(0.1355624502610459),\n",
       "   np.float64(0.1540479467871179),\n",
       "   np.float64(0.1463045998067725),\n",
       "   np.float64(0.16282555145096808),\n",
       "   np.float64(0.15177416098142826),\n",
       "   np.float64(0.16517553951398636),\n",
       "   np.float64(0.1413718809555964),\n",
       "   np.float64(0.14497860738747664),\n",
       "   np.float64(0.15314909248173905),\n",
       "   np.float64(0.1320890780751874),\n",
       "   np.float64(0.1444305415205748),\n",
       "   np.float64(0.15465923279611554),\n",
       "   np.float64(0.1468583006114024),\n",
       "   np.float64(0.1652320415900902),\n",
       "   np.float64(0.1599228353517077),\n",
       "   np.float64(0.15561713423701168),\n",
       "   np.float64(0.1508197384734566),\n",
       "   np.float64(0.13385074927360702),\n",
       "   np.float64(0.1313241677290331),\n",
       "   np.float64(0.1276250424852932),\n",
       "   np.float64(0.14450248998563872),\n",
       "   np.float64(0.15738870910097721),\n",
       "   np.float64(0.13255248977266992),\n",
       "   np.float64(0.15396056214108986),\n",
       "   np.float64(0.1412318453994195),\n",
       "   np.float64(0.1567130726419727),\n",
       "   np.float64(0.15927780780162057),\n",
       "   np.float64(0.1543001066755638),\n",
       "   np.float64(0.1369543518209596),\n",
       "   np.float64(0.13752976819536),\n",
       "   np.float64(0.14981301425338203),\n",
       "   np.float64(0.17058797861522162),\n",
       "   np.float64(0.14197241288022097),\n",
       "   np.float64(0.15862372708736774),\n",
       "   np.float64(0.15630496695761653),\n",
       "   np.float64(0.13105996012533475),\n",
       "   np.float64(0.16369619795724402),\n",
       "   np.float64(0.15234235449514355),\n",
       "   np.float64(0.14777090378121932),\n",
       "   np.float64(0.1517996683640635),\n",
       "   np.float64(0.14288689209328934),\n",
       "   np.float64(0.16153633018331584),\n",
       "   np.float64(0.1652216535446822),\n",
       "   np.float64(0.146031219460193),\n",
       "   np.float64(0.13841235152654843),\n",
       "   np.float64(0.16010075516668695),\n",
       "   np.float64(0.15031782019385884),\n",
       "   np.float64(0.13469106172260092),\n",
       "   np.float64(0.15382047545125518),\n",
       "   np.float64(0.15881355753682463),\n",
       "   np.float64(0.15429414817441314),\n",
       "   np.float64(0.13676263336650732),\n",
       "   np.float64(0.15700273504729936),\n",
       "   np.float64(0.16726298781631962),\n",
       "   np.float64(0.133376341793288),\n",
       "   np.float64(0.16782780594856864),\n",
       "   np.float64(0.14402176707197434),\n",
       "   np.float64(0.13915259833614385),\n",
       "   np.float64(0.15069171131899298),\n",
       "   np.float64(0.13562607306282576),\n",
       "   np.float64(0.15476563823425676),\n",
       "   np.float64(0.17205081017777818),\n",
       "   np.float64(0.14788075904485132),\n",
       "   np.float64(0.14318695334488635),\n",
       "   np.float64(0.14858663879605802),\n",
       "   np.float64(0.1489694836940914),\n",
       "   np.float64(0.16335423842583655),\n",
       "   np.float64(0.16258276714530434),\n",
       "   np.float64(0.1434851119636627),\n",
       "   np.float64(0.14796431580050443),\n",
       "   np.float64(0.15369286812313998),\n",
       "   np.float64(0.16086535942586352),\n",
       "   np.float64(0.12815371821279448),\n",
       "   np.float64(0.15243498751885629),\n",
       "   np.float64(0.15800897093866573),\n",
       "   np.float64(0.17786437108263412),\n",
       "   np.float64(0.15286310866684738),\n",
       "   np.float64(0.1365647330537411),\n",
       "   np.float64(0.13217312959708388),\n",
       "   np.float64(0.16126578968943278),\n",
       "   np.float64(0.1585620206887078),\n",
       "   np.float64(0.15701975971333504),\n",
       "   np.float64(0.15825064897050364),\n",
       "   np.float64(0.14038875551614302),\n",
       "   np.float64(0.15375859580585718),\n",
       "   np.float64(0.16627743002233208),\n",
       "   np.float64(0.1346103625268901),\n",
       "   np.float64(0.14994784002771033),\n",
       "   np.float64(0.1531112427768854),\n",
       "   np.float64(0.1387148338193754),\n",
       "   np.float64(0.14587950174496805),\n",
       "   np.float64(0.14586230071147405),\n",
       "   np.float64(0.1545480873477127),\n",
       "   np.float64(0.14095800347840448),\n",
       "   np.float64(0.15578632056711222),\n",
       "   np.float64(0.1570616667725665),\n",
       "   np.float64(0.14609285411711967),\n",
       "   np.float64(0.14725549923671422),\n",
       "   np.float64(0.1451843477353456),\n",
       "   np.float64(0.12634553546191413),\n",
       "   np.float64(0.1459631995964282),\n",
       "   np.float64(0.14053533346287045),\n",
       "   np.float64(0.1587326995109298),\n",
       "   np.float64(0.14565747456420608),\n",
       "   np.float64(0.13900849490242123),\n",
       "   np.float64(0.16351018676595777),\n",
       "   np.float64(0.1327033401436205),\n",
       "   np.float64(0.13080206189981086),\n",
       "   np.float64(0.1566430791227548),\n",
       "   np.float64(0.15094708101820128),\n",
       "   np.float64(0.14207760051433507),\n",
       "   np.float64(0.14752159546960936),\n",
       "   np.float64(0.16429874172403053),\n",
       "   np.float64(0.14643658846282007),\n",
       "   np.float64(0.1441429505470329),\n",
       "   np.float64(0.13867576729310913),\n",
       "   np.float64(0.14044599972302674),\n",
       "   np.float64(0.13741499741800556),\n",
       "   np.float64(0.13350354072294204),\n",
       "   np.float64(0.13304589844084438),\n",
       "   np.float64(0.1469621577562289),\n",
       "   np.float64(0.13037137564794843),\n",
       "   np.float64(0.13895072820271723),\n",
       "   np.float64(0.14759085013824694),\n",
       "   np.float64(0.15631959660301317),\n",
       "   np.float64(0.14819650771159637),\n",
       "   np.float64(0.1486009104366215),\n",
       "   np.float64(0.1479804268265076),\n",
       "   np.float64(0.1575061682060122),\n",
       "   np.float64(0.14555983529770422),\n",
       "   np.float64(0.1386643847783341),\n",
       "   np.float64(0.14208856741173212),\n",
       "   np.float64(0.1367522066909525),\n",
       "   np.float64(0.1522318024105341),\n",
       "   np.float64(0.13635402306724745),\n",
       "   np.float64(0.1441635839489211),\n",
       "   np.float64(0.14556371464672258),\n",
       "   np.float64(0.14366208218295617),\n",
       "   np.float64(0.14918509924976536),\n",
       "   np.float64(0.1418774876495989),\n",
       "   np.float64(0.14869022672722662),\n",
       "   np.float64(0.1650871201887775),\n",
       "   np.float64(0.15381544927886126),\n",
       "   np.float64(0.155150737652895),\n",
       "   np.float64(0.1259714044753241),\n",
       "   np.float64(0.16706606182824588),\n",
       "   np.float64(0.1440997085091617),\n",
       "   np.float64(0.14037905200213874),\n",
       "   np.float64(0.1604110291787567),\n",
       "   np.float64(0.15526191433931116)],\n",
       "  'train_atom_loss_mean': [np.float64(1.0781596592068672),\n",
       "   np.float64(0.438122421503067),\n",
       "   np.float64(0.3189658045768738),\n",
       "   np.float64(0.2343675036728382),\n",
       "   np.float64(0.1751262255758047),\n",
       "   np.float64(0.1428707018494606),\n",
       "   np.float64(0.1161973948776722),\n",
       "   np.float64(0.0917862068861723),\n",
       "   np.float64(0.08796060055494309),\n",
       "   np.float64(0.0789157198369503),\n",
       "   np.float64(0.06860238119959831),\n",
       "   np.float64(0.06242821894586086),\n",
       "   np.float64(0.05428277548402548),\n",
       "   np.float64(0.050361298173666),\n",
       "   np.float64(0.048498961608856916),\n",
       "   np.float64(0.04439408745616674),\n",
       "   np.float64(0.044291712455451486),\n",
       "   np.float64(0.046094488762319086),\n",
       "   np.float64(0.04460377240553498),\n",
       "   np.float64(0.044896475337445736),\n",
       "   np.float64(0.03860710389912128),\n",
       "   np.float64(0.041554536167532206),\n",
       "   np.float64(0.03630057038739323),\n",
       "   np.float64(0.03801223494112491),\n",
       "   np.float64(0.03538603600114584),\n",
       "   np.float64(0.03586080653592944),\n",
       "   np.float64(0.039413715228438374),\n",
       "   np.float64(0.037497859168797734),\n",
       "   np.float64(0.03634083500131965),\n",
       "   np.float64(0.03567729447036982),\n",
       "   np.float64(0.0350748342089355),\n",
       "   np.float64(0.03513626653701067),\n",
       "   np.float64(0.03318785954266787),\n",
       "   np.float64(0.033921408131718636),\n",
       "   np.float64(0.033037750031799075),\n",
       "   np.float64(0.03714274244382978),\n",
       "   np.float64(0.03346869546920061),\n",
       "   np.float64(0.03352253830060363),\n",
       "   np.float64(0.03262181533500552),\n",
       "   np.float64(0.03443652426823974),\n",
       "   np.float64(0.03529390424489975),\n",
       "   np.float64(0.03548557827249169),\n",
       "   np.float64(0.03509089665487409),\n",
       "   np.float64(0.03568584056571126),\n",
       "   np.float64(0.036571138463914396),\n",
       "   np.float64(0.03710502514615655),\n",
       "   np.float64(0.03467722645029426),\n",
       "   np.float64(0.03370349741540849),\n",
       "   np.float64(0.036493478398770096),\n",
       "   np.float64(0.03439876055344939),\n",
       "   np.float64(0.032536005526781084),\n",
       "   np.float64(0.03147328764200211),\n",
       "   np.float64(0.03233826650306582),\n",
       "   np.float64(0.03342096915468574),\n",
       "   np.float64(0.030569574069231747),\n",
       "   np.float64(0.03034881329163909),\n",
       "   np.float64(0.028544068224728106),\n",
       "   np.float64(0.031174661442637443),\n",
       "   np.float64(0.029985354170203208),\n",
       "   np.float64(0.03087849833071232),\n",
       "   np.float64(0.030515559129416944),\n",
       "   np.float64(0.029820567928254604),\n",
       "   np.float64(0.02811706069856882),\n",
       "   np.float64(0.027979361824691296),\n",
       "   np.float64(0.027194128381088378),\n",
       "   np.float64(0.02751742954365909),\n",
       "   np.float64(0.026054570684209466),\n",
       "   np.float64(0.028338776789605617),\n",
       "   np.float64(0.0282631266862154),\n",
       "   np.float64(0.0256020927708596),\n",
       "   np.float64(0.028043544199317694),\n",
       "   np.float64(0.026821904070675372),\n",
       "   np.float64(0.02467379548586905),\n",
       "   np.float64(0.02595764539204538),\n",
       "   np.float64(0.026788770873099565),\n",
       "   np.float64(0.024039886482059954),\n",
       "   np.float64(0.02366963411681354),\n",
       "   np.float64(0.02395710801705718),\n",
       "   np.float64(0.02353551342152059),\n",
       "   np.float64(0.02222222522832453),\n",
       "   np.float64(0.024093478498980404),\n",
       "   np.float64(0.022401096820831298),\n",
       "   np.float64(0.021711463192477822),\n",
       "   np.float64(0.019852329259738326),\n",
       "   np.float64(0.02128236452117562),\n",
       "   np.float64(0.02150404524989426),\n",
       "   np.float64(0.0206445265840739),\n",
       "   np.float64(0.020300435051321983),\n",
       "   np.float64(0.019915994172915816),\n",
       "   np.float64(0.01959927223622799),\n",
       "   np.float64(0.02034421998076141),\n",
       "   np.float64(0.020498392200097443),\n",
       "   np.float64(0.020529673835262656),\n",
       "   np.float64(0.0191391733661294),\n",
       "   np.float64(0.018420578762888908),\n",
       "   np.float64(0.01831944303587079),\n",
       "   np.float64(0.02061420832760632),\n",
       "   np.float64(0.0183354112226516),\n",
       "   np.float64(0.01830600298009813),\n",
       "   np.float64(0.020560271134600044),\n",
       "   np.float64(0.018489151084795593),\n",
       "   np.float64(0.019462811974808574),\n",
       "   np.float64(0.01883750583510846),\n",
       "   np.float64(0.0184609339479357),\n",
       "   np.float64(0.01863080566748977),\n",
       "   np.float64(0.018612490659579636),\n",
       "   np.float64(0.01800857687368989),\n",
       "   np.float64(0.017343907267786562),\n",
       "   np.float64(0.016610056767240165),\n",
       "   np.float64(0.01794698647223413),\n",
       "   np.float64(0.017392924064770342),\n",
       "   np.float64(0.016608591247349976),\n",
       "   np.float64(0.017274871310219167),\n",
       "   np.float64(0.01654884135350585),\n",
       "   np.float64(0.016841170135885476),\n",
       "   np.float64(0.016040286556817593),\n",
       "   np.float64(0.01606137676630169),\n",
       "   np.float64(0.01722960655577481),\n",
       "   np.float64(0.01646542178466916),\n",
       "   np.float64(0.01632328607607633),\n",
       "   np.float64(0.01589123918209225),\n",
       "   np.float64(0.014353043362498284),\n",
       "   np.float64(0.015374948596581816),\n",
       "   np.float64(0.0160513724014163),\n",
       "   np.float64(0.016015557618811727),\n",
       "   np.float64(0.014719553245231509),\n",
       "   np.float64(0.016096800034865737),\n",
       "   np.float64(0.015828599352389575),\n",
       "   np.float64(0.014268910558894277),\n",
       "   np.float64(0.014922319566830993),\n",
       "   np.float64(0.015754775665700434),\n",
       "   np.float64(0.015867666937410832),\n",
       "   np.float64(0.015117676029913128),\n",
       "   np.float64(0.01416471735574305),\n",
       "   np.float64(0.013434818112291396),\n",
       "   np.float64(0.01312985546886921),\n",
       "   np.float64(0.01304174328688532),\n",
       "   np.float64(0.012428495730273426),\n",
       "   np.float64(0.012835834864526987),\n",
       "   np.float64(0.013662048899568617),\n",
       "   np.float64(0.01338015106972307),\n",
       "   np.float64(0.013606921494938433),\n",
       "   np.float64(0.013657113737426698),\n",
       "   np.float64(0.014068488739430904),\n",
       "   np.float64(0.012926002778112889),\n",
       "   np.float64(0.012544924248941243),\n",
       "   np.float64(0.013290824280120432),\n",
       "   np.float64(0.012829714012332261),\n",
       "   np.float64(0.012436626306734979),\n",
       "   np.float64(0.013998246029950678),\n",
       "   np.float64(0.013146726754494012),\n",
       "   np.float64(0.012800792092457413),\n",
       "   np.float64(0.013834793763235211),\n",
       "   np.float64(0.01352273992728442),\n",
       "   np.float64(0.012663552011363209),\n",
       "   np.float64(0.012856825198978185),\n",
       "   np.float64(0.012645625732839108),\n",
       "   np.float64(0.012922826032154262),\n",
       "   np.float64(0.013128437674604356),\n",
       "   np.float64(0.013061674861237407),\n",
       "   np.float64(0.01258681815583259),\n",
       "   np.float64(0.013570689987391233),\n",
       "   np.float64(0.0130272701010108),\n",
       "   np.float64(0.012906206725165247),\n",
       "   np.float64(0.01174255513586104),\n",
       "   np.float64(0.011475859531201423),\n",
       "   np.float64(0.012279604319483042),\n",
       "   np.float64(0.012906628577038646),\n",
       "   np.float64(0.012465668297372758),\n",
       "   np.float64(0.011886188294738531),\n",
       "   np.float64(0.012506841360591351),\n",
       "   np.float64(0.012220655959099531),\n",
       "   np.float64(0.011709257112815976),\n",
       "   np.float64(0.012959006293676793),\n",
       "   np.float64(0.012319375239312648),\n",
       "   np.float64(0.011774204103276134),\n",
       "   np.float64(0.012272644769400359),\n",
       "   np.float64(0.012041047280654311),\n",
       "   np.float64(0.011497773537412286),\n",
       "   np.float64(0.012157052676193415),\n",
       "   np.float64(0.011811349848285317),\n",
       "   np.float64(0.01262975116726011),\n",
       "   np.float64(0.012000960442237555),\n",
       "   np.float64(0.010973170180805028),\n",
       "   np.float64(0.010760966981761157),\n",
       "   np.float64(0.009973771781660616),\n",
       "   np.float64(0.010913798012770712),\n",
       "   np.float64(0.010189169410150498),\n",
       "   np.float64(0.009568180646747351),\n",
       "   np.float64(0.011114465910941363),\n",
       "   np.float64(0.011205728272907436),\n",
       "   np.float64(0.011007568170316517),\n",
       "   np.float64(0.010740165528841317),\n",
       "   np.float64(0.010932774457614869),\n",
       "   np.float64(0.010454376665875316),\n",
       "   np.float64(0.009683905832935125),\n",
       "   np.float64(0.010306265482213348),\n",
       "   np.float64(0.010227228002622723),\n",
       "   np.float64(0.009751516003161669),\n",
       "   np.float64(0.009939473734702916),\n",
       "   np.float64(0.008985613947734237),\n",
       "   np.float64(0.010432033038232475),\n",
       "   np.float64(0.009849506819155068),\n",
       "   np.float64(0.010102412025444209),\n",
       "   np.float64(0.010300758192315698),\n",
       "   np.float64(0.009918964323587716),\n",
       "   np.float64(0.009807506089564412),\n",
       "   np.float64(0.010085857359226793),\n",
       "   np.float64(0.00997972826473415),\n",
       "   np.float64(0.010011318670585752),\n",
       "   np.float64(0.0100056530861184),\n",
       "   np.float64(0.009837381863035262),\n",
       "   np.float64(0.00983318880200386),\n",
       "   np.float64(0.009258901618886739),\n",
       "   np.float64(0.009138927024323493),\n",
       "   np.float64(0.008777848260942847),\n",
       "   np.float64(0.009199661840684712),\n",
       "   np.float64(0.0099871278507635),\n",
       "   np.float64(0.008540477689821272),\n",
       "   np.float64(0.009451353717595338),\n",
       "   np.float64(0.009311369340866803),\n",
       "   np.float64(0.009143456362653523),\n",
       "   np.float64(0.009123511263169348),\n",
       "   np.float64(0.009615447870455682),\n",
       "   np.float64(0.009899880427401513),\n",
       "   np.float64(0.008806145458947867),\n",
       "   np.float64(0.009506619044113904),\n",
       "   np.float64(0.009416235780809074),\n",
       "   np.float64(0.009754386588465423),\n",
       "   np.float64(0.00921646353090182),\n",
       "   np.float64(0.00944960066350177),\n",
       "   np.float64(0.009751815693452954),\n",
       "   np.float64(0.008761116629466414),\n",
       "   np.float64(0.009834104070905595),\n",
       "   np.float64(0.009724441647995264),\n",
       "   np.float64(0.008909909662324935),\n",
       "   np.float64(0.008676537240389734),\n",
       "   np.float64(0.00916772047523409),\n",
       "   np.float64(0.009194687891285866),\n",
       "   np.float64(0.009612215885426849),\n",
       "   np.float64(0.009337964598089457),\n",
       "   np.float64(0.010350277503021062),\n",
       "   np.float64(0.009212957443669439),\n",
       "   np.float64(0.0088335813395679),\n",
       "   np.float64(0.009777526850812138),\n",
       "   np.float64(0.008426029484253377),\n",
       "   np.float64(0.008603039365261793),\n",
       "   np.float64(0.009630545747932047),\n",
       "   np.float64(0.009323326358571648),\n",
       "   np.float64(0.008700597598217428),\n",
       "   np.float64(0.008524032563436777),\n",
       "   np.float64(0.007919616040308029),\n",
       "   np.float64(0.008054632875137031),\n",
       "   np.float64(0.008076394328381866),\n",
       "   np.float64(0.007671094180550426),\n",
       "   np.float64(0.007838660557754338),\n",
       "   np.float64(0.008080908281262964),\n",
       "   np.float64(0.007794056113343686),\n",
       "   np.float64(0.00833306675311178),\n",
       "   np.float64(0.007966850642114878),\n",
       "   np.float64(0.008139999157283456),\n",
       "   np.float64(0.00818991539417766),\n",
       "   np.float64(0.008066601646132767),\n",
       "   np.float64(0.008352837399579585),\n",
       "   np.float64(0.00765959546668455),\n",
       "   np.float64(0.007540032127872109),\n",
       "   np.float64(0.008010994992218912),\n",
       "   np.float64(0.008173736531753094),\n",
       "   np.float64(0.008052363521419465),\n",
       "   np.float64(0.007551930912304669),\n",
       "   np.float64(0.007860347313107923),\n",
       "   np.float64(0.007913349063601345),\n",
       "   np.float64(0.007186696422286332),\n",
       "   np.float64(0.007304717537481338),\n",
       "   np.float64(0.007656290256418288),\n",
       "   np.float64(0.00768653694074601),\n",
       "   np.float64(0.008261712351813912),\n",
       "   np.float64(0.007238710287492722),\n",
       "   np.float64(0.007784920134581625),\n",
       "   np.float64(0.00781036343658343),\n",
       "   np.float64(0.007491285868454724),\n",
       "   np.float64(0.0072668722318485375),\n",
       "   np.float64(0.007124818859156221),\n",
       "   np.float64(0.00733932543778792),\n",
       "   np.float64(0.007346905444283038),\n",
       "   np.float64(0.007651548460125923),\n",
       "   np.float64(0.007590118458028883),\n",
       "   np.float64(0.007418465360533446),\n",
       "   np.float64(0.00718766925856471),\n",
       "   np.float64(0.006213130860123784),\n",
       "   np.float64(0.006543339632917196),\n",
       "   np.float64(0.006622064046096057),\n",
       "   np.float64(0.007191023894120008),\n",
       "   np.float64(0.007118943661917001),\n",
       "   np.float64(0.006853606177028269),\n",
       "   np.float64(0.006929581989534199),\n",
       "   np.float64(0.0073977718339301645),\n",
       "   np.float64(0.007709821939934045),\n",
       "   np.float64(0.007880096648586915),\n",
       "   np.float64(0.006801322305109352),\n",
       "   np.float64(0.006646275923121721),\n",
       "   np.float64(0.006684322824003175),\n",
       "   np.float64(0.007371300968807191),\n",
       "   np.float64(0.007012150102527812),\n",
       "   np.float64(0.006950910175219178),\n",
       "   np.float64(0.007250691805966198),\n",
       "   np.float64(0.006970736160874367),\n",
       "   np.float64(0.0068388967053033415),\n",
       "   np.float64(0.00616406012326479),\n",
       "   np.float64(0.006336980329360813),\n",
       "   np.float64(0.006664597082417459),\n",
       "   np.float64(0.006004043663851917),\n",
       "   np.float64(0.006371549721807241),\n",
       "   np.float64(0.006835011974908411),\n",
       "   np.float64(0.006668822185602039),\n",
       "   np.float64(0.00644630663562566),\n",
       "   np.float64(0.006887744633713737),\n",
       "   np.float64(0.00663027458358556),\n",
       "   np.float64(0.007281366331735626),\n",
       "   np.float64(0.006434372870717198),\n",
       "   np.float64(0.006363074426772073),\n",
       "   np.float64(0.006683426285162568),\n",
       "   np.float64(0.006267158481059596),\n",
       "   np.float64(0.0063033310044556855),\n",
       "   np.float64(0.0067318597657140345),\n",
       "   np.float64(0.0059935386211145665),\n",
       "   np.float64(0.006085079000331462),\n",
       "   np.float64(0.006515608920017258),\n",
       "   np.float64(0.006320067027118057),\n",
       "   np.float64(0.005834377470891923),\n",
       "   np.float64(0.005755606819875538),\n",
       "   np.float64(0.005675715043907985),\n",
       "   np.float64(0.006235433750553057),\n",
       "   np.float64(0.0066943540214560926),\n",
       "   np.float64(0.006054923550691455),\n",
       "   np.float64(0.006049401081399992),\n",
       "   np.float64(0.005566973054083064),\n",
       "   np.float64(0.005994121625553817),\n",
       "   np.float64(0.005905872194562107),\n",
       "   np.float64(0.005876612802967429),\n",
       "   np.float64(0.005658183323685079),\n",
       "   np.float64(0.005852868257788941),\n",
       "   np.float64(0.005829973928630352),\n",
       "   np.float64(0.0052683042257558555),\n",
       "   np.float64(0.00520752070704475),\n",
       "   np.float64(0.005070167718222365),\n",
       "   np.float64(0.004956013009650633),\n",
       "   np.float64(0.0049391920084599404),\n",
       "   np.float64(0.005139468190027401),\n",
       "   np.float64(0.005271566414739937),\n",
       "   np.float64(0.004943458728957921),\n",
       "   np.float64(0.004851039686473086),\n",
       "   np.float64(0.005390993977198377),\n",
       "   np.float64(0.005466574393212795),\n",
       "   np.float64(0.00593864715192467),\n",
       "   np.float64(0.0059984983375761655),\n",
       "   np.float64(0.005466060601174831),\n",
       "   np.float64(0.00598922811448574),\n",
       "   np.float64(0.0050044143362902105),\n",
       "   np.float64(0.004807976154843345),\n",
       "   np.float64(0.005727552904281765),\n",
       "   np.float64(0.00509676365298219),\n",
       "   np.float64(0.005376073482912034),\n",
       "   np.float64(0.005429795528762042),\n",
       "   np.float64(0.005328380096470937),\n",
       "   np.float64(0.0046700575610157105),\n",
       "   np.float64(0.005038918387726881),\n",
       "   np.float64(0.004701839403714984),\n",
       "   np.float64(0.004913560224231332),\n",
       "   np.float64(0.004782358150696382),\n",
       "   np.float64(0.005052450806833804),\n",
       "   np.float64(0.004696620853501372),\n",
       "   np.float64(0.00484502540086396),\n",
       "   np.float64(0.004538689266191795),\n",
       "   np.float64(0.004535961130168289),\n",
       "   np.float64(0.005224278952227906),\n",
       "   np.float64(0.004094172611949034),\n",
       "   np.float64(0.004527893915656023),\n",
       "   np.float64(0.004592289312276989),\n",
       "   np.float64(0.004505934177432209),\n",
       "   np.float64(0.004926128424704075),\n",
       "   np.float64(0.004534965394996106),\n",
       "   np.float64(0.004552607990335673),\n",
       "   np.float64(0.004223929258296266),\n",
       "   np.float64(0.004343168075429275),\n",
       "   np.float64(0.004832224091514945),\n",
       "   np.float64(0.0048632263694889846),\n",
       "   np.float64(0.004481531195342541),\n",
       "   np.float64(0.00485581303248182),\n",
       "   np.float64(0.004550201807869598),\n",
       "   np.float64(0.004024418767075985),\n",
       "   np.float64(0.004498270003823563),\n",
       "   np.float64(0.0039692643377929925),\n",
       "   np.float64(0.004316837848746218),\n",
       "   np.float64(0.004540195594308898),\n",
       "   np.float64(0.004343637364218012),\n",
       "   np.float64(0.00442495335242711),\n",
       "   np.float64(0.0042062932928092775),\n",
       "   np.float64(0.004646675236290321),\n",
       "   np.float64(0.004284708969062194),\n",
       "   np.float64(0.0045667893410427495),\n",
       "   np.float64(0.003935938916401938),\n",
       "   np.float64(0.003693915511248633),\n",
       "   np.float64(0.00420788832125254),\n",
       "   np.float64(0.00475088415434584),\n",
       "   np.float64(0.004188197087496519),\n",
       "   np.float64(0.0038844692433485763),\n",
       "   np.float64(0.004341946702916175),\n",
       "   np.float64(0.004335084421327338),\n",
       "   np.float64(0.004099617344327271),\n",
       "   np.float64(0.004403294342919253),\n",
       "   np.float64(0.004558886900776997),\n",
       "   np.float64(0.003943581391940825),\n",
       "   np.float64(0.003782175402739085),\n",
       "   np.float64(0.003961019704584032),\n",
       "   np.float64(0.00417911714175716),\n",
       "   np.float64(0.0037711772142210976),\n",
       "   np.float64(0.00419138758443296),\n",
       "   np.float64(0.004005790445953608),\n",
       "   np.float64(0.004071528568165377),\n",
       "   np.float64(0.003831518368679099),\n",
       "   np.float64(0.004151819710386917),\n",
       "   np.float64(0.004445432088105008),\n",
       "   np.float64(0.0036767362942919136),\n",
       "   np.float64(0.004305950006237253),\n",
       "   np.float64(0.004336531207663938),\n",
       "   np.float64(0.004205927026341669),\n",
       "   np.float64(0.0039292310521705075),\n",
       "   np.float64(0.0040763496677391235),\n",
       "   np.float64(0.00444512780639343),\n",
       "   np.float64(0.004292557097505778),\n",
       "   np.float64(0.004249378545791842),\n",
       "   np.float64(0.0038818852650001645),\n",
       "   np.float64(0.004191531492397189),\n",
       "   np.float64(0.00383103471074719),\n",
       "   np.float64(0.003742966704303399),\n",
       "   np.float64(0.00392066206492018)],\n",
       "  'train_atom_loss_std': [np.float64(0.9181827805879847),\n",
       "   np.float64(0.05441801878373792),\n",
       "   np.float64(0.03484712514220751),\n",
       "   np.float64(0.030333067011351442),\n",
       "   np.float64(0.029151162321180993),\n",
       "   np.float64(0.023224402690755645),\n",
       "   np.float64(0.02001693698489009),\n",
       "   np.float64(0.017394056608985787),\n",
       "   np.float64(0.015214223138432442),\n",
       "   np.float64(0.01636922776953104),\n",
       "   np.float64(0.012252249697207602),\n",
       "   np.float64(0.012356936861982162),\n",
       "   np.float64(0.011076577551261962),\n",
       "   np.float64(0.01112710380203608),\n",
       "   np.float64(0.009794468644025974),\n",
       "   np.float64(0.009953824858303595),\n",
       "   np.float64(0.00850016867659152),\n",
       "   np.float64(0.00857748617728066),\n",
       "   np.float64(0.009177171096851388),\n",
       "   np.float64(0.00845882534896924),\n",
       "   np.float64(0.00786873610631484),\n",
       "   np.float64(0.008269980381444898),\n",
       "   np.float64(0.007349818690270873),\n",
       "   np.float64(0.008136404821936985),\n",
       "   np.float64(0.007667125122350849),\n",
       "   np.float64(0.00833247672291177),\n",
       "   np.float64(0.008769615873512356),\n",
       "   np.float64(0.008600494519106891),\n",
       "   np.float64(0.008133110010751359),\n",
       "   np.float64(0.0077952232516538735),\n",
       "   np.float64(0.008432610738624358),\n",
       "   np.float64(0.008186016482703734),\n",
       "   np.float64(0.007057313118170016),\n",
       "   np.float64(0.0071196696528529595),\n",
       "   np.float64(0.008044088667037584),\n",
       "   np.float64(0.007846443401518471),\n",
       "   np.float64(0.008248971598996074),\n",
       "   np.float64(0.007939046694305361),\n",
       "   np.float64(0.007587786438390035),\n",
       "   np.float64(0.007647102401865029),\n",
       "   np.float64(0.00782983572716207),\n",
       "   np.float64(0.007626786748459182),\n",
       "   np.float64(0.008272843701428117),\n",
       "   np.float64(0.0071612328969759085),\n",
       "   np.float64(0.007444825142311052),\n",
       "   np.float64(0.009390417189390377),\n",
       "   np.float64(0.008209388365905602),\n",
       "   np.float64(0.008873424646196709),\n",
       "   np.float64(0.008069422795872942),\n",
       "   np.float64(0.008649823914392563),\n",
       "   np.float64(0.00724906437541961),\n",
       "   np.float64(0.0076517375017566185),\n",
       "   np.float64(0.006703637246333567),\n",
       "   np.float64(0.007555990911379518),\n",
       "   np.float64(0.0077971024917085595),\n",
       "   np.float64(0.00721254078493843),\n",
       "   np.float64(0.0067770422999415365),\n",
       "   np.float64(0.006273617587792267),\n",
       "   np.float64(0.006406255313685521),\n",
       "   np.float64(0.006585791451927884),\n",
       "   np.float64(0.007377061997800623),\n",
       "   np.float64(0.006901169486386614),\n",
       "   np.float64(0.007480817359493951),\n",
       "   np.float64(0.006418904223892126),\n",
       "   np.float64(0.005136531319636137),\n",
       "   np.float64(0.006388382007800809),\n",
       "   np.float64(0.006712217661113952),\n",
       "   np.float64(0.006100679306756572),\n",
       "   np.float64(0.006180913425144451),\n",
       "   np.float64(0.00570754343189959),\n",
       "   np.float64(0.007281661434644596),\n",
       "   np.float64(0.0068948401942569805),\n",
       "   np.float64(0.006181947743209355),\n",
       "   np.float64(0.005757589213360467),\n",
       "   np.float64(0.006547545128753631),\n",
       "   np.float64(0.006406593818202307),\n",
       "   np.float64(0.006342516902547228),\n",
       "   np.float64(0.0059490225312716005),\n",
       "   np.float64(0.0065200571032468235),\n",
       "   np.float64(0.005764093695376467),\n",
       "   np.float64(0.005378059827455676),\n",
       "   np.float64(0.00563851678124725),\n",
       "   np.float64(0.005403539827240978),\n",
       "   np.float64(0.0058144316194659925),\n",
       "   np.float64(0.00622292714422579),\n",
       "   np.float64(0.006143599015397833),\n",
       "   np.float64(0.005522001354306328),\n",
       "   np.float64(0.005203368919015825),\n",
       "   np.float64(0.0060380857123073),\n",
       "   np.float64(0.005219197301723155),\n",
       "   np.float64(0.005506043743238647),\n",
       "   np.float64(0.005746302689744594),\n",
       "   np.float64(0.005662741943643107),\n",
       "   np.float64(0.005342111880313279),\n",
       "   np.float64(0.005441313152912749),\n",
       "   np.float64(0.005201645368234769),\n",
       "   np.float64(0.005716720344800964),\n",
       "   np.float64(0.0047639106916535535),\n",
       "   np.float64(0.0049924361826380135),\n",
       "   np.float64(0.006258318580669456),\n",
       "   np.float64(0.005616380206857268),\n",
       "   np.float64(0.005523276964247759),\n",
       "   np.float64(0.005433935565951326),\n",
       "   np.float64(0.0059129603176738665),\n",
       "   np.float64(0.006346821599710993),\n",
       "   np.float64(0.005416786497338137),\n",
       "   np.float64(0.005296599374233921),\n",
       "   np.float64(0.005065307623233387),\n",
       "   np.float64(0.00545609813603907),\n",
       "   np.float64(0.0050497233059676894),\n",
       "   np.float64(0.00425149157732212),\n",
       "   np.float64(0.004964044068347824),\n",
       "   np.float64(0.005635609546759943),\n",
       "   np.float64(0.004722507008732866),\n",
       "   np.float64(0.00524340826578282),\n",
       "   np.float64(0.004961125975211854),\n",
       "   np.float64(0.0049324400680886395),\n",
       "   np.float64(0.005313151971808021),\n",
       "   np.float64(0.004916830631911883),\n",
       "   np.float64(0.0047025796421579),\n",
       "   np.float64(0.004631001804467744),\n",
       "   np.float64(0.00399382111655201),\n",
       "   np.float64(0.004592544849509788),\n",
       "   np.float64(0.004828869983709625),\n",
       "   np.float64(0.0049666767449803185),\n",
       "   np.float64(0.004289696517188731),\n",
       "   np.float64(0.004643687719164788),\n",
       "   np.float64(0.004758080926106395),\n",
       "   np.float64(0.00474375321038557),\n",
       "   np.float64(0.005112955169954711),\n",
       "   np.float64(0.004909675590475777),\n",
       "   np.float64(0.0048788145039217684),\n",
       "   np.float64(0.004867534166581277),\n",
       "   np.float64(0.003953076702367507),\n",
       "   np.float64(0.004283605720133089),\n",
       "   np.float64(0.004543911191412435),\n",
       "   np.float64(0.004062703787159308),\n",
       "   np.float64(0.004598510662020506),\n",
       "   np.float64(0.004186727091839037),\n",
       "   np.float64(0.004313938175524395),\n",
       "   np.float64(0.0035305757809813566),\n",
       "   np.float64(0.004208680221601373),\n",
       "   np.float64(0.004341869683817802),\n",
       "   np.float64(0.004760080873126678),\n",
       "   np.float64(0.00392275019769559),\n",
       "   np.float64(0.004118718555312653),\n",
       "   np.float64(0.00387132822851553),\n",
       "   np.float64(0.004078996865616847),\n",
       "   np.float64(0.004053334639191387),\n",
       "   np.float64(0.004247026477426781),\n",
       "   np.float64(0.004353506886458407),\n",
       "   np.float64(0.004732835040984658),\n",
       "   np.float64(0.004609195755297537),\n",
       "   np.float64(0.004315933009206384),\n",
       "   np.float64(0.004151501252636039),\n",
       "   np.float64(0.004252976382396769),\n",
       "   np.float64(0.0042157653782411),\n",
       "   np.float64(0.004431077784290123),\n",
       "   np.float64(0.003868754886619416),\n",
       "   np.float64(0.004856052864025488),\n",
       "   np.float64(0.003902765115208446),\n",
       "   np.float64(0.004803895711710084),\n",
       "   np.float64(0.004341142267847781),\n",
       "   np.float64(0.004428063148041369),\n",
       "   np.float64(0.004182225514992403),\n",
       "   np.float64(0.0038624808155046826),\n",
       "   np.float64(0.004095604387534608),\n",
       "   np.float64(0.003667883400399277),\n",
       "   np.float64(0.004153205327039927),\n",
       "   np.float64(0.0035791129044324965),\n",
       "   np.float64(0.0042422571881429744),\n",
       "   np.float64(0.00419051937046793),\n",
       "   np.float64(0.0037671282964515606),\n",
       "   np.float64(0.004372885334145924),\n",
       "   np.float64(0.003900155802815162),\n",
       "   np.float64(0.004082036744836743),\n",
       "   np.float64(0.0043627633881394845),\n",
       "   np.float64(0.004046451448609896),\n",
       "   np.float64(0.004012263475762953),\n",
       "   np.float64(0.004391284909486924),\n",
       "   np.float64(0.0037030050792127753),\n",
       "   np.float64(0.004110230972228758),\n",
       "   np.float64(0.004597792517117269),\n",
       "   np.float64(0.004067576224040981),\n",
       "   np.float64(0.003832402242626673),\n",
       "   np.float64(0.0038494399571503304),\n",
       "   np.float64(0.004128238613444458),\n",
       "   np.float64(0.003687883764237886),\n",
       "   np.float64(0.0033657580415299474),\n",
       "   np.float64(0.004267186748847698),\n",
       "   np.float64(0.003739575435004302),\n",
       "   np.float64(0.0037632042084973063),\n",
       "   np.float64(0.0041034271515559915),\n",
       "   np.float64(0.00338813015245854),\n",
       "   np.float64(0.003450736064525143),\n",
       "   np.float64(0.003714473561853526),\n",
       "   np.float64(0.0040395177786749226),\n",
       "   np.float64(0.004332035120582878),\n",
       "   np.float64(0.0029400751178181917),\n",
       "   np.float64(0.0035110924486869838),\n",
       "   np.float64(0.003635842888638257),\n",
       "   np.float64(0.00398594305532771),\n",
       "   np.float64(0.003753531396272179),\n",
       "   np.float64(0.003633513853124916),\n",
       "   np.float64(0.00439774428289823),\n",
       "   np.float64(0.003301135862919911),\n",
       "   np.float64(0.0034315341956938964),\n",
       "   np.float64(0.003987724681175455),\n",
       "   np.float64(0.0038733164684042905),\n",
       "   np.float64(0.0036529865345815405),\n",
       "   np.float64(0.00393949380780561),\n",
       "   np.float64(0.0038923026335213233),\n",
       "   np.float64(0.0035785537610041637),\n",
       "   np.float64(0.0035075631275902214),\n",
       "   np.float64(0.0032086771096245356),\n",
       "   np.float64(0.003422047932296194),\n",
       "   np.float64(0.0034931678263833065),\n",
       "   np.float64(0.003791784455711345),\n",
       "   np.float64(0.0033734956329712082),\n",
       "   np.float64(0.0036020250019417315),\n",
       "   np.float64(0.0032459085333629628),\n",
       "   np.float64(0.00329455404136645),\n",
       "   np.float64(0.0036074672157935935),\n",
       "   np.float64(0.003528028417539669),\n",
       "   np.float64(0.003972702965069624),\n",
       "   np.float64(0.0033364380256068068),\n",
       "   np.float64(0.003832867549543304),\n",
       "   np.float64(0.003679539978904066),\n",
       "   np.float64(0.003959788222848129),\n",
       "   np.float64(0.00380410309099634),\n",
       "   np.float64(0.003575423236117535),\n",
       "   np.float64(0.003765293958655946),\n",
       "   np.float64(0.003032700433942553),\n",
       "   np.float64(0.003850389783985789),\n",
       "   np.float64(0.003363402656502849),\n",
       "   np.float64(0.0035114330363971045),\n",
       "   np.float64(0.0029426753814878995),\n",
       "   np.float64(0.0038791246041891612),\n",
       "   np.float64(0.003487997541188858),\n",
       "   np.float64(0.0037018972024097183),\n",
       "   np.float64(0.0032321704097720903),\n",
       "   np.float64(0.004761879372368452),\n",
       "   np.float64(0.0038659077891506732),\n",
       "   np.float64(0.003340743709574337),\n",
       "   np.float64(0.003495180646951631),\n",
       "   np.float64(0.0030733440859182083),\n",
       "   np.float64(0.003516756820945375),\n",
       "   np.float64(0.004238481731874884),\n",
       "   np.float64(0.0040447397379266645),\n",
       "   np.float64(0.0035099529381023785),\n",
       "   np.float64(0.0038180885134685588),\n",
       "   np.float64(0.003326157713371646),\n",
       "   np.float64(0.0029826475529427985),\n",
       "   np.float64(0.0032667437357879027),\n",
       "   np.float64(0.003166639605756216),\n",
       "   np.float64(0.0028659466533357725),\n",
       "   np.float64(0.0033761353524900273),\n",
       "   np.float64(0.002759873583309786),\n",
       "   np.float64(0.003100137801528701),\n",
       "   np.float64(0.003088435027439898),\n",
       "   np.float64(0.0027813530161578176),\n",
       "   np.float64(0.0034034495829120214),\n",
       "   np.float64(0.003042377680978896),\n",
       "   np.float64(0.0033230242679765983),\n",
       "   np.float64(0.00323389740036521),\n",
       "   np.float64(0.002716385889293856),\n",
       "   np.float64(0.003218266006530431),\n",
       "   np.float64(0.003175103021048626),\n",
       "   np.float64(0.0035383187390829523),\n",
       "   np.float64(0.0030344332190132095),\n",
       "   np.float64(0.0032689333642969202),\n",
       "   np.float64(0.0030429329607249817),\n",
       "   np.float64(0.0029854205711501553),\n",
       "   np.float64(0.0033664407218107067),\n",
       "   np.float64(0.0028709567289166766),\n",
       "   np.float64(0.0028429153900049213),\n",
       "   np.float64(0.003101598380220344),\n",
       "   np.float64(0.0028829416301331995),\n",
       "   np.float64(0.003115606317413528),\n",
       "   np.float64(0.0034366543595512),\n",
       "   np.float64(0.0027861451447747988),\n",
       "   np.float64(0.003447440439099605),\n",
       "   np.float64(0.002900488671817674),\n",
       "   np.float64(0.00299832800334309),\n",
       "   np.float64(0.0029799011596747305),\n",
       "   np.float64(0.0033392038813745565),\n",
       "   np.float64(0.003524069209769264),\n",
       "   np.float64(0.0034453443634219895),\n",
       "   np.float64(0.0031210794114149972),\n",
       "   np.float64(0.0026578296515303726),\n",
       "   np.float64(0.0028034887623519873),\n",
       "   np.float64(0.002628188829583055),\n",
       "   np.float64(0.003556972413579188),\n",
       "   np.float64(0.003371074107569295),\n",
       "   np.float64(0.002962769875169727),\n",
       "   np.float64(0.0034051222155753727),\n",
       "   np.float64(0.0031130733296303166),\n",
       "   np.float64(0.003653059784346947),\n",
       "   np.float64(0.0033208574859521175),\n",
       "   np.float64(0.002955156254669328),\n",
       "   np.float64(0.0028932396054576668),\n",
       "   np.float64(0.0029295279832555563),\n",
       "   np.float64(0.0031792513294282214),\n",
       "   np.float64(0.0028804748771307817),\n",
       "   np.float64(0.0028810839379766998),\n",
       "   np.float64(0.003217596974042149),\n",
       "   np.float64(0.0031885524315632026),\n",
       "   np.float64(0.0031509928661749564),\n",
       "   np.float64(0.002764117382548527),\n",
       "   np.float64(0.0032151712692982874),\n",
       "   np.float64(0.0025716153550078124),\n",
       "   np.float64(0.0024480767009770585),\n",
       "   np.float64(0.0030165645873443353),\n",
       "   np.float64(0.0026446440352812404),\n",
       "   np.float64(0.0029152718049942237),\n",
       "   np.float64(0.0027977144208640374),\n",
       "   np.float64(0.0032290479361297233),\n",
       "   np.float64(0.0032753899857123807),\n",
       "   np.float64(0.0035448917825272722),\n",
       "   np.float64(0.0028464396942271502),\n",
       "   np.float64(0.0029235879438780396),\n",
       "   np.float64(0.00248294185227036),\n",
       "   np.float64(0.0023514590375140134),\n",
       "   np.float64(0.00265346959306525),\n",
       "   np.float64(0.0034297623205158784),\n",
       "   np.float64(0.002691722748676169),\n",
       "   np.float64(0.0033372935514033118),\n",
       "   np.float64(0.002798012360924264),\n",
       "   np.float64(0.0027642321395157585),\n",
       "   np.float64(0.0029160233789142375),\n",
       "   np.float64(0.0028004254420034027),\n",
       "   np.float64(0.002300361973675992),\n",
       "   np.float64(0.0028997121716443914),\n",
       "   np.float64(0.002905778010182652),\n",
       "   np.float64(0.0025572935940734796),\n",
       "   np.float64(0.0028871108118120035),\n",
       "   np.float64(0.0023879150492369427),\n",
       "   np.float64(0.00265627498824827),\n",
       "   np.float64(0.0029048538855424755),\n",
       "   np.float64(0.002556810053557474),\n",
       "   np.float64(0.0025704230655493525),\n",
       "   np.float64(0.002918376403521592),\n",
       "   np.float64(0.002829429217110117),\n",
       "   np.float64(0.002449156377256492),\n",
       "   np.float64(0.0022217838507674775),\n",
       "   np.float64(0.002365751646837143),\n",
       "   np.float64(0.002406978990204873),\n",
       "   np.float64(0.002406525680183572),\n",
       "   np.float64(0.0020954762986053364),\n",
       "   np.float64(0.0024613145296982723),\n",
       "   np.float64(0.0023652677811635773),\n",
       "   np.float64(0.0026202235384021194),\n",
       "   np.float64(0.002559788147363517),\n",
       "   np.float64(0.002406844720825667),\n",
       "   np.float64(0.002773922812263663),\n",
       "   np.float64(0.002970116900805439),\n",
       "   np.float64(0.0023583295589949048),\n",
       "   np.float64(0.002801209751403003),\n",
       "   np.float64(0.002364815222256503),\n",
       "   np.float64(0.0024806404130364333),\n",
       "   np.float64(0.0030884148410409754),\n",
       "   np.float64(0.0024700691055412594),\n",
       "   np.float64(0.0024816837793210297),\n",
       "   np.float64(0.0024341376297479164),\n",
       "   np.float64(0.0025240209604527864),\n",
       "   np.float64(0.0024308805681530527),\n",
       "   np.float64(0.002645393627794152),\n",
       "   np.float64(0.002115427614433799),\n",
       "   np.float64(0.0023121885986306903),\n",
       "   np.float64(0.0027902028569702034),\n",
       "   np.float64(0.0028864562797738433),\n",
       "   np.float64(0.002115147802951507),\n",
       "   np.float64(0.0023970240825223824),\n",
       "   np.float64(0.0019389591679083802),\n",
       "   np.float64(0.002592479903523942),\n",
       "   np.float64(0.002755255455050726),\n",
       "   np.float64(0.0020079301809494588),\n",
       "   np.float64(0.0024819386051793484),\n",
       "   np.float64(0.0021439385776138124),\n",
       "   np.float64(0.0022641119071242725),\n",
       "   np.float64(0.002687910517437756),\n",
       "   np.float64(0.002405297001301591),\n",
       "   np.float64(0.0022077618283595697),\n",
       "   np.float64(0.002237138657619737),\n",
       "   np.float64(0.0024700383674906116),\n",
       "   np.float64(0.002159260532091011),\n",
       "   np.float64(0.002194919280921511),\n",
       "   np.float64(0.0020653586890530306),\n",
       "   np.float64(0.0023717521161352255),\n",
       "   np.float64(0.0026386887582183173),\n",
       "   np.float64(0.001998254534382251),\n",
       "   np.float64(0.0023542229992234696),\n",
       "   np.float64(0.0019361391136485024),\n",
       "   np.float64(0.002587583903382362),\n",
       "   np.float64(0.0024795834062323824),\n",
       "   np.float64(0.0018476609291930377),\n",
       "   np.float64(0.0023981009678617606),\n",
       "   np.float64(0.0020027500405455775),\n",
       "   np.float64(0.0023558908349074086),\n",
       "   np.float64(0.00240034752242049),\n",
       "   np.float64(0.002799515890502749),\n",
       "   np.float64(0.0016967776841194206),\n",
       "   np.float64(0.001728469961969665),\n",
       "   np.float64(0.002190501478665186),\n",
       "   np.float64(0.002290481823119279),\n",
       "   np.float64(0.0020016891767161682),\n",
       "   np.float64(0.0019248127652437862),\n",
       "   np.float64(0.0023215311226607404),\n",
       "   np.float64(0.0023426606831746465),\n",
       "   np.float64(0.0026069301954918895),\n",
       "   np.float64(0.0022095044903541945),\n",
       "   np.float64(0.0025409572477364285),\n",
       "   np.float64(0.0017096475603428374),\n",
       "   np.float64(0.0019638408915384604),\n",
       "   np.float64(0.0019120172229619378),\n",
       "   np.float64(0.0018896807477844766),\n",
       "   np.float64(0.0018244020549958752),\n",
       "   np.float64(0.00245273452591859),\n",
       "   np.float64(0.002054579129508779),\n",
       "   np.float64(0.0025876711844553387),\n",
       "   np.float64(0.00208456625017928),\n",
       "   np.float64(0.0020715303136356585),\n",
       "   np.float64(0.0025131580834619827),\n",
       "   np.float64(0.00217080677964183),\n",
       "   np.float64(0.0018250012480103997),\n",
       "   np.float64(0.0022498206892010092),\n",
       "   np.float64(0.0020750423450637758),\n",
       "   np.float64(0.0021198260457272627),\n",
       "   np.float64(0.0022790625421966423),\n",
       "   np.float64(0.0026635072757354426),\n",
       "   np.float64(0.0023538134021598442),\n",
       "   np.float64(0.0024413959528110426),\n",
       "   np.float64(0.0019540706859844472),\n",
       "   np.float64(0.0023818521994624256),\n",
       "   np.float64(0.0020737446659017497),\n",
       "   np.float64(0.0021618724072795213),\n",
       "   np.float64(0.0022759083396658455)],\n",
       "  'train_bond_loss_mean': [np.float64(0.46409444987773896),\n",
       "   np.float64(0.2795542229712009),\n",
       "   np.float64(0.23157594442367554),\n",
       "   np.float64(0.22037783086299897),\n",
       "   np.float64(0.20971422657370567),\n",
       "   np.float64(0.1981101657450199),\n",
       "   np.float64(0.17676234737038612),\n",
       "   np.float64(0.16331200398504733),\n",
       "   np.float64(0.1568001451343298),\n",
       "   np.float64(0.144739980250597),\n",
       "   np.float64(0.13590070649981498),\n",
       "   np.float64(0.13906186230480672),\n",
       "   np.float64(0.1257877292484045),\n",
       "   np.float64(0.1261602321267128),\n",
       "   np.float64(0.1240954790264368),\n",
       "   np.float64(0.1114952426403761),\n",
       "   np.float64(0.11194956891238689),\n",
       "   np.float64(0.11734597660601138),\n",
       "   np.float64(0.10698777876794338),\n",
       "   np.float64(0.10750141628086567),\n",
       "   np.float64(0.09692587904632091),\n",
       "   np.float64(0.091450206451118),\n",
       "   np.float64(0.0974824583530426),\n",
       "   np.float64(0.09238063596189022),\n",
       "   np.float64(0.09934376992285252),\n",
       "   np.float64(0.09861101664602756),\n",
       "   np.float64(0.09907797671854496),\n",
       "   np.float64(0.09147569011896849),\n",
       "   np.float64(0.09887223742902279),\n",
       "   np.float64(0.0948149748891592),\n",
       "   np.float64(0.09057105869054795),\n",
       "   np.float64(0.08794986020773649),\n",
       "   np.float64(0.09268895264714956),\n",
       "   np.float64(0.08989350941032172),\n",
       "   np.float64(0.09015707850456238),\n",
       "   np.float64(0.0991150788217783),\n",
       "   np.float64(0.10016923941671849),\n",
       "   np.float64(0.09538949079811573),\n",
       "   np.float64(0.09337936490774154),\n",
       "   np.float64(0.0926308274269104),\n",
       "   np.float64(0.0972698774561286),\n",
       "   np.float64(0.09472767900675536),\n",
       "   np.float64(0.09680697865784169),\n",
       "   np.float64(0.09584963217377662),\n",
       "   np.float64(0.09778880346566439),\n",
       "   np.float64(0.0948914136737585),\n",
       "   np.float64(0.09969882048666477),\n",
       "   np.float64(0.10065668180584908),\n",
       "   np.float64(0.09904073305428028),\n",
       "   np.float64(0.09951499119400978),\n",
       "   np.float64(0.10265280477702618),\n",
       "   np.float64(0.10148429550230503),\n",
       "   np.float64(0.09406523127108812),\n",
       "   np.float64(0.09867788709700108),\n",
       "   np.float64(0.09185866393148899),\n",
       "   np.float64(0.09861297272145748),\n",
       "   np.float64(0.102069675847888),\n",
       "   np.float64(0.10276400111615658),\n",
       "   np.float64(0.10084018871188163),\n",
       "   np.float64(0.10282727248966694),\n",
       "   np.float64(0.10024560406804085),\n",
       "   np.float64(0.09661460377275943),\n",
       "   np.float64(0.09588354118168355),\n",
       "   np.float64(0.09420427318662405),\n",
       "   np.float64(0.09418471492826938),\n",
       "   np.float64(0.09809347577393054),\n",
       "   np.float64(0.0965479912981391),\n",
       "   np.float64(0.10176238693296909),\n",
       "   np.float64(0.10236669562757016),\n",
       "   np.float64(0.09983744367957115),\n",
       "   np.float64(0.10062240108847618),\n",
       "   np.float64(0.10008739545941353),\n",
       "   np.float64(0.09773541688919067),\n",
       "   np.float64(0.09740690805017949),\n",
       "   np.float64(0.09739815458655357),\n",
       "   np.float64(0.09302800506353379),\n",
       "   np.float64(0.09589984402060509),\n",
       "   np.float64(0.10126280508935452),\n",
       "   np.float64(0.10024967685341835),\n",
       "   np.float64(0.0983273245766759),\n",
       "   np.float64(0.0980129138380289),\n",
       "   np.float64(0.09702040247619152),\n",
       "   np.float64(0.09746210400015115),\n",
       "   np.float64(0.09160462073981762),\n",
       "   np.float64(0.09550035495311021),\n",
       "   np.float64(0.09481499165296554),\n",
       "   np.float64(0.09577994726598263),\n",
       "   np.float64(0.0927858915179968),\n",
       "   np.float64(0.09005338974297046),\n",
       "   np.float64(0.09006494086235761),\n",
       "   np.float64(0.09391270022839308),\n",
       "   np.float64(0.09336832255125045),\n",
       "   np.float64(0.09427161697298288),\n",
       "   np.float64(0.09693354308605194),\n",
       "   np.float64(0.09416043184697628),\n",
       "   np.float64(0.09412014115601779),\n",
       "   np.float64(0.09849141411483288),\n",
       "   np.float64(0.09782786399126053),\n",
       "   np.float64(0.0997896683216095),\n",
       "   np.float64(0.1078747709095478),\n",
       "   np.float64(0.10641636237502099),\n",
       "   np.float64(0.10835053727030754),\n",
       "   np.float64(0.10687897734344005),\n",
       "   np.float64(0.10661071550101042),\n",
       "   np.float64(0.10701067499816418),\n",
       "   np.float64(0.10477223753929138),\n",
       "   np.float64(0.10892710156738758),\n",
       "   np.float64(0.10836262002587319),\n",
       "   np.float64(0.1048846446722746),\n",
       "   np.float64(0.11009183384478093),\n",
       "   np.float64(0.10605910651385785),\n",
       "   np.float64(0.09866138190031051),\n",
       "   np.float64(0.10518676530569791),\n",
       "   np.float64(0.10558775648474693),\n",
       "   np.float64(0.10369782947003842),\n",
       "   np.float64(0.10250035744160414),\n",
       "   np.float64(0.10640248589217663),\n",
       "   np.float64(0.10481769405305386),\n",
       "   np.float64(0.11026007521897555),\n",
       "   np.float64(0.11121188364923),\n",
       "   np.float64(0.10439518928527831),\n",
       "   np.float64(0.10389406375586986),\n",
       "   np.float64(0.10793648436665534),\n",
       "   np.float64(0.10736455373466015),\n",
       "   np.float64(0.11014028549194337),\n",
       "   np.float64(0.10628079943358898),\n",
       "   np.float64(0.11087058514356612),\n",
       "   np.float64(0.11944573983550072),\n",
       "   np.float64(0.10733048550784588),\n",
       "   np.float64(0.11129413619637489),\n",
       "   np.float64(0.10840351171791554),\n",
       "   np.float64(0.11221744030714036),\n",
       "   np.float64(0.10677137792110443),\n",
       "   np.float64(0.10552497901022434),\n",
       "   np.float64(0.1086140788346529),\n",
       "   np.float64(0.10777907192707062),\n",
       "   np.float64(0.11531577937304974),\n",
       "   np.float64(0.1128487703204155),\n",
       "   np.float64(0.10963356152176856),\n",
       "   np.float64(0.11052205815911292),\n",
       "   np.float64(0.11045099586248398),\n",
       "   np.float64(0.11465733714401721),\n",
       "   np.float64(0.11768527805805207),\n",
       "   np.float64(0.11381878808140755),\n",
       "   np.float64(0.11387475855648517),\n",
       "   np.float64(0.11320624172687531),\n",
       "   np.float64(0.11706890404224396),\n",
       "   np.float64(0.11032250002026558),\n",
       "   np.float64(0.10807470068335533),\n",
       "   np.float64(0.10984945841133595),\n",
       "   np.float64(0.1085616347938776),\n",
       "   np.float64(0.1077586855739355),\n",
       "   np.float64(0.11132775485515595),\n",
       "   np.float64(0.10637656703591347),\n",
       "   np.float64(0.11068088598549367),\n",
       "   np.float64(0.10862383842468262),\n",
       "   np.float64(0.1122600432485342),\n",
       "   np.float64(0.11176681369543076),\n",
       "   np.float64(0.11100022442638874),\n",
       "   np.float64(0.11106265127658844),\n",
       "   np.float64(0.10843252807855606),\n",
       "   np.float64(0.10668043851852417),\n",
       "   np.float64(0.10505498014390469),\n",
       "   np.float64(0.1014655713737011),\n",
       "   np.float64(0.10425926730036736),\n",
       "   np.float64(0.10421748787164688),\n",
       "   np.float64(0.10651891440153122),\n",
       "   np.float64(0.11366963513195515),\n",
       "   np.float64(0.11049676522612571),\n",
       "   np.float64(0.11447817817330361),\n",
       "   np.float64(0.11525827825069428),\n",
       "   np.float64(0.11489006645977497),\n",
       "   np.float64(0.11302043303847313),\n",
       "   np.float64(0.116605977229774),\n",
       "   np.float64(0.11257219299674034),\n",
       "   np.float64(0.1126313168555498),\n",
       "   np.float64(0.11230389527976513),\n",
       "   np.float64(0.11357809595763683),\n",
       "   np.float64(0.10790709875524045),\n",
       "   np.float64(0.10968631319701672),\n",
       "   np.float64(0.11000978782773017),\n",
       "   np.float64(0.1112060458213091),\n",
       "   np.float64(0.11243359342217446),\n",
       "   np.float64(0.11031164802610874),\n",
       "   np.float64(0.11364154078066349),\n",
       "   np.float64(0.11122057691216469),\n",
       "   np.float64(0.10914112843573093),\n",
       "   np.float64(0.10923115994781256),\n",
       "   np.float64(0.11009122617542744),\n",
       "   np.float64(0.11379792511463166),\n",
       "   np.float64(0.11454492576420307),\n",
       "   np.float64(0.11323711566627026),\n",
       "   np.float64(0.10828738875687122),\n",
       "   np.float64(0.11062947962433099),\n",
       "   np.float64(0.11286416284739971),\n",
       "   np.float64(0.11164546430110932),\n",
       "   np.float64(0.10476793006062507),\n",
       "   np.float64(0.10647830650210381),\n",
       "   np.float64(0.10504463784396649),\n",
       "   np.float64(0.1108758419752121),\n",
       "   np.float64(0.10921870127320289),\n",
       "   np.float64(0.1127033694088459),\n",
       "   np.float64(0.11363844595849514),\n",
       "   np.float64(0.11157013773918152),\n",
       "   np.float64(0.11406059376895428),\n",
       "   np.float64(0.11168595530092716),\n",
       "   np.float64(0.1138280026614666),\n",
       "   np.float64(0.11345086380839348),\n",
       "   np.float64(0.11197070948779583),\n",
       "   np.float64(0.10628604225814342),\n",
       "   np.float64(0.11184582404792309),\n",
       "   np.float64(0.1088584826886654),\n",
       "   np.float64(0.11197070669382811),\n",
       "   np.float64(0.1101007306203246),\n",
       "   np.float64(0.11385196432471276),\n",
       "   np.float64(0.11573074907064437),\n",
       "   np.float64(0.11417931221425533),\n",
       "   np.float64(0.11426101908087731),\n",
       "   np.float64(0.1078647830337286),\n",
       "   np.float64(0.11170064434409141),\n",
       "   np.float64(0.11339353278279304),\n",
       "   np.float64(0.11286507934331894),\n",
       "   np.float64(0.11276594310998916),\n",
       "   np.float64(0.10803482234477997),\n",
       "   np.float64(0.10908093184232712),\n",
       "   np.float64(0.11062023855745792),\n",
       "   np.float64(0.10784943044185638),\n",
       "   np.float64(0.11389093160629273),\n",
       "   np.float64(0.11437701813876629),\n",
       "   np.float64(0.11155453465878963),\n",
       "   np.float64(0.1068254604935646),\n",
       "   np.float64(0.11197976425290107),\n",
       "   np.float64(0.11352646075189114),\n",
       "   np.float64(0.11068908765912056),\n",
       "   np.float64(0.11249487727880478),\n",
       "   np.float64(0.1138523892313242),\n",
       "   np.float64(0.11022629648447037),\n",
       "   np.float64(0.10971591293811798),\n",
       "   np.float64(0.11130606524646282),\n",
       "   np.float64(0.11251046553254128),\n",
       "   np.float64(0.10818627499043941),\n",
       "   np.float64(0.11559862524271011),\n",
       "   np.float64(0.11077051132917404),\n",
       "   np.float64(0.11064001802355052),\n",
       "   np.float64(0.11124910041689873),\n",
       "   np.float64(0.10790658369660378),\n",
       "   np.float64(0.10858452245593071),\n",
       "   np.float64(0.11268034286797046),\n",
       "   np.float64(0.10831190846860408),\n",
       "   np.float64(0.11008653692901134),\n",
       "   np.float64(0.11184132635593415),\n",
       "   np.float64(0.11204148195683956),\n",
       "   np.float64(0.1123305706679821),\n",
       "   np.float64(0.11486770749092103),\n",
       "   np.float64(0.1155099879950285),\n",
       "   np.float64(0.11283390581607819),\n",
       "   np.float64(0.10988905243575572),\n",
       "   np.float64(0.10933424949645996),\n",
       "   np.float64(0.10869590252637863),\n",
       "   np.float64(0.11206527404487134),\n",
       "   np.float64(0.11184555612504482),\n",
       "   np.float64(0.1108042261004448),\n",
       "   np.float64(0.11187802352011204),\n",
       "   np.float64(0.10908934567123652),\n",
       "   np.float64(0.10597904086112976),\n",
       "   np.float64(0.10931923858821392),\n",
       "   np.float64(0.11007958866655826),\n",
       "   np.float64(0.10755968160927296),\n",
       "   np.float64(0.10913075886666775),\n",
       "   np.float64(0.10757188610732556),\n",
       "   np.float64(0.11346423089504241),\n",
       "   np.float64(0.10587304785847663),\n",
       "   np.float64(0.10992302566766739),\n",
       "   np.float64(0.10936341598629952),\n",
       "   np.float64(0.11481157630681992),\n",
       "   np.float64(0.11242485031485558),\n",
       "   np.float64(0.10561195798218251),\n",
       "   np.float64(0.1065811163932085),\n",
       "   np.float64(0.1091409458965063),\n",
       "   np.float64(0.11032050684094429),\n",
       "   np.float64(0.10834180973470212),\n",
       "   np.float64(0.11002942129969596),\n",
       "   np.float64(0.10630909405648709),\n",
       "   np.float64(0.11096068911254406),\n",
       "   np.float64(0.11133189901709556),\n",
       "   np.float64(0.11103803597390652),\n",
       "   np.float64(0.10889843098819256),\n",
       "   np.float64(0.10677909277379513),\n",
       "   np.float64(0.1028658850491047),\n",
       "   np.float64(0.10286274533718824),\n",
       "   np.float64(0.10655730798840522),\n",
       "   np.float64(0.1071167055517435),\n",
       "   np.float64(0.10613523617386818),\n",
       "   np.float64(0.1113461446017027),\n",
       "   np.float64(0.10757020696997642),\n",
       "   np.float64(0.11006206914782524),\n",
       "   np.float64(0.10859018601477147),\n",
       "   np.float64(0.10663992460817098),\n",
       "   np.float64(0.10704143635928631),\n",
       "   np.float64(0.10938603606075048),\n",
       "   np.float64(0.109632678180933),\n",
       "   np.float64(0.10238770797848701),\n",
       "   np.float64(0.10971446759998799),\n",
       "   np.float64(0.10958679735660554),\n",
       "   np.float64(0.1061097764223814),\n",
       "   np.float64(0.11126422770321369),\n",
       "   np.float64(0.1075404229015112),\n",
       "   np.float64(0.10741548620164394),\n",
       "   np.float64(0.10977760337293148),\n",
       "   np.float64(0.10546534940600395),\n",
       "   np.float64(0.10623410940170289),\n",
       "   np.float64(0.10466975845396519),\n",
       "   np.float64(0.10814039848744869),\n",
       "   np.float64(0.1086385576426983),\n",
       "   np.float64(0.10570865765213966),\n",
       "   np.float64(0.10928294375538826),\n",
       "   np.float64(0.10835304029285908),\n",
       "   np.float64(0.10314749266952276),\n",
       "   np.float64(0.1078351829200983),\n",
       "   np.float64(0.10339358355849981),\n",
       "   np.float64(0.10400723721832036),\n",
       "   np.float64(0.10423335801810026),\n",
       "   np.float64(0.10580504074692726),\n",
       "   np.float64(0.1067162998020649),\n",
       "   np.float64(0.10174006536602974),\n",
       "   np.float64(0.10397793777287007),\n",
       "   np.float64(0.10560058563947677),\n",
       "   np.float64(0.10096068270504474),\n",
       "   np.float64(0.0999836178869009),\n",
       "   np.float64(0.10445353448390961),\n",
       "   np.float64(0.10521214336156845),\n",
       "   np.float64(0.10757676847279071),\n",
       "   np.float64(0.10589819200336933),\n",
       "   np.float64(0.10661982871592045),\n",
       "   np.float64(0.10665754452347756),\n",
       "   np.float64(0.10387899234890938),\n",
       "   np.float64(0.1039218882471323),\n",
       "   np.float64(0.10730547528713942),\n",
       "   np.float64(0.1065102256089449),\n",
       "   np.float64(0.10673119954764843),\n",
       "   np.float64(0.10593441396951675),\n",
       "   np.float64(0.10288290780037641),\n",
       "   np.float64(0.1063725133985281),\n",
       "   np.float64(0.10016134910285474),\n",
       "   np.float64(0.10339670367538929),\n",
       "   np.float64(0.10266125690191984),\n",
       "   np.float64(0.10430214621126652),\n",
       "   np.float64(0.10595614895224571),\n",
       "   np.float64(0.1055045160651207),\n",
       "   np.float64(0.10051811620593071),\n",
       "   np.float64(0.10521793931722641),\n",
       "   np.float64(0.10433061748743057),\n",
       "   np.float64(0.10165385339409112),\n",
       "   np.float64(0.10588145352900029),\n",
       "   np.float64(0.10206042230129242),\n",
       "   np.float64(0.10380997493863106),\n",
       "   np.float64(0.10110232923179865),\n",
       "   np.float64(0.10066361121833324),\n",
       "   np.float64(0.1011062752455473),\n",
       "   np.float64(0.09647698797285557),\n",
       "   np.float64(0.10326401226222515),\n",
       "   np.float64(0.10466069404035806),\n",
       "   np.float64(0.10472937270998955),\n",
       "   np.float64(0.10325056549161672),\n",
       "   np.float64(0.10072905115783215),\n",
       "   np.float64(0.10200286373496055),\n",
       "   np.float64(0.09834323331713676),\n",
       "   np.float64(0.09937142703682184),\n",
       "   np.float64(0.09965089831501245),\n",
       "   np.float64(0.10196668662130833),\n",
       "   np.float64(0.10184750240296125),\n",
       "   np.float64(0.09868708290159703),\n",
       "   np.float64(0.10370022520422935),\n",
       "   np.float64(0.09564414788037538),\n",
       "   np.float64(0.09872537188231945),\n",
       "   np.float64(0.1054344367235899),\n",
       "   np.float64(0.10246661841869355),\n",
       "   np.float64(0.10438046842813492),\n",
       "   np.float64(0.10218341894447804),\n",
       "   np.float64(0.10064297176897526),\n",
       "   np.float64(0.10326927464455365),\n",
       "   np.float64(0.10264097291976214),\n",
       "   np.float64(0.10459795624017715),\n",
       "   np.float64(0.09997632779181004),\n",
       "   np.float64(0.10289652943611145),\n",
       "   np.float64(0.10500535022467375),\n",
       "   np.float64(0.10293964825570584),\n",
       "   np.float64(0.09960814975202084),\n",
       "   np.float64(0.1100233244150877),\n",
       "   np.float64(0.10600019529461861),\n",
       "   np.float64(0.10435639269649982),\n",
       "   np.float64(0.10276128794997931),\n",
       "   np.float64(0.10336109839379787),\n",
       "   np.float64(0.09983132813125849),\n",
       "   np.float64(0.10219811223447323),\n",
       "   np.float64(0.1041546532139182),\n",
       "   np.float64(0.10206644151359796),\n",
       "   np.float64(0.10078456684947014),\n",
       "   np.float64(0.101105981990695),\n",
       "   np.float64(0.09702079437673092),\n",
       "   np.float64(0.10317502401769162),\n",
       "   np.float64(0.10307214420288802),\n",
       "   np.float64(0.10401383575052023),\n",
       "   np.float64(0.10165602184832095),\n",
       "   np.float64(0.10081122640520335),\n",
       "   np.float64(0.1024827316403389),\n",
       "   np.float64(0.09613106161355972),\n",
       "   np.float64(0.09783917903900147),\n",
       "   np.float64(0.10314308613538742),\n",
       "   np.float64(0.10497380245476962),\n",
       "   np.float64(0.09889050915837289),\n",
       "   np.float64(0.09723678447306155),\n",
       "   np.float64(0.09863027557730675),\n",
       "   np.float64(0.1023453214019537),\n",
       "   np.float64(0.09842486418783665),\n",
       "   np.float64(0.09624132812023163),\n",
       "   np.float64(0.10020036987960339),\n",
       "   np.float64(0.09959123130887747),\n",
       "   np.float64(0.09682333201169968),\n",
       "   np.float64(0.10049232967197895),\n",
       "   np.float64(0.09889338292181492),\n",
       "   np.float64(0.09955165818333626),\n",
       "   np.float64(0.10110750518739224),\n",
       "   np.float64(0.0951431806385517),\n",
       "   np.float64(0.09804465591907502),\n",
       "   np.float64(0.09354239501059056),\n",
       "   np.float64(0.09677485432475805),\n",
       "   np.float64(0.10253144416958093),\n",
       "   np.float64(0.09951060947030782),\n",
       "   np.float64(0.10061314232647418),\n",
       "   np.float64(0.09787325669080019),\n",
       "   np.float64(0.10438071362674237),\n",
       "   np.float64(0.09923638306558132),\n",
       "   np.float64(0.10243369534611702),\n",
       "   np.float64(0.10282241873443126),\n",
       "   np.float64(0.10189093288034201),\n",
       "   np.float64(0.09905909944325686)],\n",
       "  'train_bond_loss_std': [np.float64(0.25958683584743697),\n",
       "   np.float64(0.028084910903252814),\n",
       "   np.float64(0.028025834986327887),\n",
       "   np.float64(0.024864574732039103),\n",
       "   np.float64(0.026678365320206166),\n",
       "   np.float64(0.028054586106712024),\n",
       "   np.float64(0.02222333894015583),\n",
       "   np.float64(0.021106290995830487),\n",
       "   np.float64(0.01783268141742026),\n",
       "   np.float64(0.02147122111957206),\n",
       "   np.float64(0.022450525331926593),\n",
       "   np.float64(0.02287406661364253),\n",
       "   np.float64(0.01912277370118677),\n",
       "   np.float64(0.018332833949038767),\n",
       "   np.float64(0.01859338472223732),\n",
       "   np.float64(0.016317850577677984),\n",
       "   np.float64(0.017213554939624773),\n",
       "   np.float64(0.017017294588333537),\n",
       "   np.float64(0.01634771999374811),\n",
       "   np.float64(0.019114847694038664),\n",
       "   np.float64(0.014938080720772774),\n",
       "   np.float64(0.01685094902913093),\n",
       "   np.float64(0.0168112683932452),\n",
       "   np.float64(0.016986881333595324),\n",
       "   np.float64(0.01826812603310393),\n",
       "   np.float64(0.017360016552955274),\n",
       "   np.float64(0.0189914237238534),\n",
       "   np.float64(0.01593166472289069),\n",
       "   np.float64(0.01787567060835043),\n",
       "   np.float64(0.013686067028107273),\n",
       "   np.float64(0.015853587302897175),\n",
       "   np.float64(0.016419784940934552),\n",
       "   np.float64(0.015931698981059266),\n",
       "   np.float64(0.01680494994030538),\n",
       "   np.float64(0.014543639954344162),\n",
       "   np.float64(0.016864458294478),\n",
       "   np.float64(0.022823339731403384),\n",
       "   np.float64(0.01938017808173715),\n",
       "   np.float64(0.016410358989152628),\n",
       "   np.float64(0.015420939840567067),\n",
       "   np.float64(0.020322785823121924),\n",
       "   np.float64(0.017472637117404297),\n",
       "   np.float64(0.017898370895762874),\n",
       "   np.float64(0.018475542420620363),\n",
       "   np.float64(0.018990184809930585),\n",
       "   np.float64(0.018937602082679722),\n",
       "   np.float64(0.015444809854644935),\n",
       "   np.float64(0.020754966898068387),\n",
       "   np.float64(0.017062486723151315),\n",
       "   np.float64(0.016660890807234666),\n",
       "   np.float64(0.0181024070856064),\n",
       "   np.float64(0.016011963961216968),\n",
       "   np.float64(0.01594577565204388),\n",
       "   np.float64(0.018264487922219547),\n",
       "   np.float64(0.01540808289435083),\n",
       "   np.float64(0.01920625848624577),\n",
       "   np.float64(0.018778156083558192),\n",
       "   np.float64(0.019535594705048193),\n",
       "   np.float64(0.01741696708278661),\n",
       "   np.float64(0.017889909898662293),\n",
       "   np.float64(0.019000933848746893),\n",
       "   np.float64(0.01737879869045012),\n",
       "   np.float64(0.016904144850279773),\n",
       "   np.float64(0.01563925657592086),\n",
       "   np.float64(0.015467733869754014),\n",
       "   np.float64(0.017807469287750115),\n",
       "   np.float64(0.01757903927178419),\n",
       "   np.float64(0.01849795317318899),\n",
       "   np.float64(0.01806981297270957),\n",
       "   np.float64(0.01844933205246632),\n",
       "   np.float64(0.018131946036478677),\n",
       "   np.float64(0.01745586540646273),\n",
       "   np.float64(0.019293590738611567),\n",
       "   np.float64(0.01795630872514182),\n",
       "   np.float64(0.017851156586099676),\n",
       "   np.float64(0.01696426288863494),\n",
       "   np.float64(0.018606869803150298),\n",
       "   np.float64(0.021787035623121804),\n",
       "   np.float64(0.01878243943519833),\n",
       "   np.float64(0.019112765092342426),\n",
       "   np.float64(0.01543295248790885),\n",
       "   np.float64(0.016024729889178753),\n",
       "   np.float64(0.018877738226045885),\n",
       "   np.float64(0.017772721301248164),\n",
       "   np.float64(0.017202525116029977),\n",
       "   np.float64(0.01774286612025713),\n",
       "   np.float64(0.018457331827520046),\n",
       "   np.float64(0.016758234318339088),\n",
       "   np.float64(0.018942660431925506),\n",
       "   np.float64(0.016897472336790946),\n",
       "   np.float64(0.018629013525760175),\n",
       "   np.float64(0.017495822317364976),\n",
       "   np.float64(0.01787966466699626),\n",
       "   np.float64(0.01717132910415549),\n",
       "   np.float64(0.016052503391482495),\n",
       "   np.float64(0.018399011019850562),\n",
       "   np.float64(0.01981167632707624),\n",
       "   np.float64(0.017615882875178682),\n",
       "   np.float64(0.02065314752725059),\n",
       "   np.float64(0.017857625218745836),\n",
       "   np.float64(0.01984765011161196),\n",
       "   np.float64(0.022009712383688554),\n",
       "   np.float64(0.024309845034024287),\n",
       "   np.float64(0.023904667282507626),\n",
       "   np.float64(0.024345766686922687),\n",
       "   np.float64(0.022259930408737777),\n",
       "   np.float64(0.02364588952047678),\n",
       "   np.float64(0.02257610706966022),\n",
       "   np.float64(0.02035136763201682),\n",
       "   np.float64(0.027998963385834047),\n",
       "   np.float64(0.0192214137754161),\n",
       "   np.float64(0.01790778702751405),\n",
       "   np.float64(0.023022862292066182),\n",
       "   np.float64(0.02242139320797251),\n",
       "   np.float64(0.018527706000687028),\n",
       "   np.float64(0.02381215180071541),\n",
       "   np.float64(0.024069514098111765),\n",
       "   np.float64(0.021722700326653854),\n",
       "   np.float64(0.02540697111797598),\n",
       "   np.float64(0.026606797752921758),\n",
       "   np.float64(0.021993724033179306),\n",
       "   np.float64(0.020439861377226014),\n",
       "   np.float64(0.022654984235607748),\n",
       "   np.float64(0.020612674618379588),\n",
       "   np.float64(0.021618145484962638),\n",
       "   np.float64(0.022956952667237122),\n",
       "   np.float64(0.025349212744711178),\n",
       "   np.float64(0.021713288361389194),\n",
       "   np.float64(0.026704463321056162),\n",
       "   np.float64(0.026153310202789472),\n",
       "   np.float64(0.0189147284389754),\n",
       "   np.float64(0.023862463356114815),\n",
       "   np.float64(0.020039925431339835),\n",
       "   np.float64(0.02371043168655262),\n",
       "   np.float64(0.02226262123019922),\n",
       "   np.float64(0.022211286713894607),\n",
       "   np.float64(0.027305351804487683),\n",
       "   np.float64(0.02350878912464406),\n",
       "   np.float64(0.022203834466266722),\n",
       "   np.float64(0.021892147282558786),\n",
       "   np.float64(0.02055898123222978),\n",
       "   np.float64(0.022535232764092965),\n",
       "   np.float64(0.02592528481343805),\n",
       "   np.float64(0.01885349228597558),\n",
       "   np.float64(0.021330630089069786),\n",
       "   np.float64(0.023497170156199728),\n",
       "   np.float64(0.021893870943478114),\n",
       "   np.float64(0.018645352986991735),\n",
       "   np.float64(0.02156345460169659),\n",
       "   np.float64(0.020416191712031125),\n",
       "   np.float64(0.021138970048306546),\n",
       "   np.float64(0.022428814571585534),\n",
       "   np.float64(0.02254340189887605),\n",
       "   np.float64(0.020616938178218837),\n",
       "   np.float64(0.022753630628347492),\n",
       "   np.float64(0.022266997733236243),\n",
       "   np.float64(0.01992955835048174),\n",
       "   np.float64(0.02069975974285044),\n",
       "   np.float64(0.02226319124365229),\n",
       "   np.float64(0.023755817359793062),\n",
       "   np.float64(0.021406207383242386),\n",
       "   np.float64(0.02058844185708671),\n",
       "   np.float64(0.02107002183292914),\n",
       "   np.float64(0.021377820844137863),\n",
       "   np.float64(0.02317805822509348),\n",
       "   np.float64(0.02237750974381028),\n",
       "   np.float64(0.021061399578088395),\n",
       "   np.float64(0.021407692564260592),\n",
       "   np.float64(0.021484649783549175),\n",
       "   np.float64(0.02507021953111647),\n",
       "   np.float64(0.024913409613990762),\n",
       "   np.float64(0.022575883779922423),\n",
       "   np.float64(0.02097370227544982),\n",
       "   np.float64(0.02190439517162264),\n",
       "   np.float64(0.023357151704379048),\n",
       "   np.float64(0.023841961700276342),\n",
       "   np.float64(0.02528313557300356),\n",
       "   np.float64(0.022370278489705565),\n",
       "   np.float64(0.017763480110579863),\n",
       "   np.float64(0.022078236325502293),\n",
       "   np.float64(0.023486722490875005),\n",
       "   np.float64(0.021251664419776526),\n",
       "   np.float64(0.023742722080684155),\n",
       "   np.float64(0.02120839748164404),\n",
       "   np.float64(0.022976562527972404),\n",
       "   np.float64(0.02223036676625734),\n",
       "   np.float64(0.021596075480958484),\n",
       "   np.float64(0.023849122646324907),\n",
       "   np.float64(0.022233982324608433),\n",
       "   np.float64(0.02261066315708959),\n",
       "   np.float64(0.02247979570413854),\n",
       "   np.float64(0.022644721112928014),\n",
       "   np.float64(0.020543916902821282),\n",
       "   np.float64(0.022979071830523716),\n",
       "   np.float64(0.02171246854606493),\n",
       "   np.float64(0.019897771719800512),\n",
       "   np.float64(0.02211729655179193),\n",
       "   np.float64(0.02359703939014408),\n",
       "   np.float64(0.018704216953759102),\n",
       "   np.float64(0.02551498127661546),\n",
       "   np.float64(0.024330689705764796),\n",
       "   np.float64(0.022642799659072465),\n",
       "   np.float64(0.02319961963002954),\n",
       "   np.float64(0.020858647572780182),\n",
       "   np.float64(0.021866136765295217),\n",
       "   np.float64(0.02099657834692793),\n",
       "   np.float64(0.022949038110837056),\n",
       "   np.float64(0.021478947804138757),\n",
       "   np.float64(0.02051622969450325),\n",
       "   np.float64(0.022257005550839534),\n",
       "   np.float64(0.01999306340479064),\n",
       "   np.float64(0.02160182330903285),\n",
       "   np.float64(0.022119129548032603),\n",
       "   np.float64(0.025048925814900057),\n",
       "   np.float64(0.02337996741777332),\n",
       "   np.float64(0.022003255112967992),\n",
       "   np.float64(0.024422733762055324),\n",
       "   np.float64(0.023029469171643512),\n",
       "   np.float64(0.018526913324788227),\n",
       "   np.float64(0.025961849684698817),\n",
       "   np.float64(0.021550373808215933),\n",
       "   np.float64(0.020282238423264043),\n",
       "   np.float64(0.022036358301832967),\n",
       "   np.float64(0.023029803781599932),\n",
       "   np.float64(0.02288878749017891),\n",
       "   np.float64(0.018769226710779775),\n",
       "   np.float64(0.023691081377154515),\n",
       "   np.float64(0.022894890303271197),\n",
       "   np.float64(0.022883125650139814),\n",
       "   np.float64(0.021572646551917596),\n",
       "   np.float64(0.020207669624422695),\n",
       "   np.float64(0.021869113177607567),\n",
       "   np.float64(0.01994282173736801),\n",
       "   np.float64(0.01939267993236035),\n",
       "   np.float64(0.02474669880509868),\n",
       "   np.float64(0.022465596401947018),\n",
       "   np.float64(0.018374279321081016),\n",
       "   np.float64(0.02074904362348918),\n",
       "   np.float64(0.021476325093051172),\n",
       "   np.float64(0.025934255117686808),\n",
       "   np.float64(0.01871713273684995),\n",
       "   np.float64(0.02169954474461388),\n",
       "   np.float64(0.020837775329824823),\n",
       "   np.float64(0.02113574627997386),\n",
       "   np.float64(0.02145438837193678),\n",
       "   np.float64(0.019474884052641795),\n",
       "   np.float64(0.020708663684294866),\n",
       "   np.float64(0.022876060006014855),\n",
       "   np.float64(0.02064355182126069),\n",
       "   np.float64(0.02213068532226736),\n",
       "   np.float64(0.022296755616789933),\n",
       "   np.float64(0.020441347098552064),\n",
       "   np.float64(0.023907020624601257),\n",
       "   np.float64(0.023836048354043207),\n",
       "   np.float64(0.02116515238853084),\n",
       "   np.float64(0.021364811181385553),\n",
       "   np.float64(0.019919824123821876),\n",
       "   np.float64(0.023501950061673813),\n",
       "   np.float64(0.020666123755653253),\n",
       "   np.float64(0.02204879951308499),\n",
       "   np.float64(0.019466674225212054),\n",
       "   np.float64(0.020713221618126512),\n",
       "   np.float64(0.02075161529556803),\n",
       "   np.float64(0.01987907399444821),\n",
       "   np.float64(0.02104095884773776),\n",
       "   np.float64(0.021977271655102017),\n",
       "   np.float64(0.02202537001607083),\n",
       "   np.float64(0.021842286683271785),\n",
       "   np.float64(0.021520469018432478),\n",
       "   np.float64(0.020455421552397598),\n",
       "   np.float64(0.021010486003188944),\n",
       "   np.float64(0.01837296316486749),\n",
       "   np.float64(0.022710099493595162),\n",
       "   np.float64(0.02079014521054847),\n",
       "   np.float64(0.02088499761113722),\n",
       "   np.float64(0.01968708585425263),\n",
       "   np.float64(0.018692174071632723),\n",
       "   np.float64(0.0180459216357005),\n",
       "   np.float64(0.022837966050698256),\n",
       "   np.float64(0.022249855049336284),\n",
       "   np.float64(0.020035646719499034),\n",
       "   np.float64(0.020062265339426722),\n",
       "   np.float64(0.017470071591011622),\n",
       "   np.float64(0.02142765814695087),\n",
       "   np.float64(0.02415935621931801),\n",
       "   np.float64(0.023204801861733872),\n",
       "   np.float64(0.018060133749242632),\n",
       "   np.float64(0.02208699311054996),\n",
       "   np.float64(0.02093770560081133),\n",
       "   np.float64(0.01754988717978153),\n",
       "   np.float64(0.023947142439759737),\n",
       "   np.float64(0.021111836711390133),\n",
       "   np.float64(0.01975275397070137),\n",
       "   np.float64(0.022562830703024755),\n",
       "   np.float64(0.018674244890489272),\n",
       "   np.float64(0.023145229714109344),\n",
       "   np.float64(0.018946011109987466),\n",
       "   np.float64(0.019730764106447772),\n",
       "   np.float64(0.020981716077852118),\n",
       "   np.float64(0.02332107963815703),\n",
       "   np.float64(0.02096240908770346),\n",
       "   np.float64(0.01926248668442207),\n",
       "   np.float64(0.021105311965814992),\n",
       "   np.float64(0.020977263834337834),\n",
       "   np.float64(0.022269098187184),\n",
       "   np.float64(0.02356755735129683),\n",
       "   np.float64(0.02229915405877008),\n",
       "   np.float64(0.02110979417041386),\n",
       "   np.float64(0.023939909774364318),\n",
       "   np.float64(0.019073008950176426),\n",
       "   np.float64(0.020063535180749714),\n",
       "   np.float64(0.022049562192692097),\n",
       "   np.float64(0.022194876611451742),\n",
       "   np.float64(0.023674393655098176),\n",
       "   np.float64(0.023767568471055562),\n",
       "   np.float64(0.022958320267806597),\n",
       "   np.float64(0.023348093394587648),\n",
       "   np.float64(0.018869571189238075),\n",
       "   np.float64(0.019871051744355713),\n",
       "   np.float64(0.020148766391342096),\n",
       "   np.float64(0.02093277110933149),\n",
       "   np.float64(0.023441658514737757),\n",
       "   np.float64(0.0207425035766448),\n",
       "   np.float64(0.021856812492209672),\n",
       "   np.float64(0.018356466855466286),\n",
       "   np.float64(0.0229812282852834),\n",
       "   np.float64(0.021762363912799107),\n",
       "   np.float64(0.020315999299987703),\n",
       "   np.float64(0.01920302096509874),\n",
       "   np.float64(0.021228604511454356),\n",
       "   np.float64(0.018675599993144183),\n",
       "   np.float64(0.01941716307696046),\n",
       "   np.float64(0.021570278201496748),\n",
       "   np.float64(0.021877769614820408),\n",
       "   np.float64(0.019764131240198192),\n",
       "   np.float64(0.019547924487042078),\n",
       "   np.float64(0.02080769383060128),\n",
       "   np.float64(0.02136794501533118),\n",
       "   np.float64(0.02381221225592669),\n",
       "   np.float64(0.01811891781337733),\n",
       "   np.float64(0.02146592280481649),\n",
       "   np.float64(0.023696548834053036),\n",
       "   np.float64(0.019772647274094068),\n",
       "   np.float64(0.022337795140966687),\n",
       "   np.float64(0.021287166307073193),\n",
       "   np.float64(0.01870663572606381),\n",
       "   np.float64(0.01848995343483662),\n",
       "   np.float64(0.022186389544227653),\n",
       "   np.float64(0.02051917432974467),\n",
       "   np.float64(0.022251778262730476),\n",
       "   np.float64(0.02106141841489097),\n",
       "   np.float64(0.02008562324822547),\n",
       "   np.float64(0.01961995409459011),\n",
       "   np.float64(0.022839190533995826),\n",
       "   np.float64(0.017030791704670045),\n",
       "   np.float64(0.02098754133157246),\n",
       "   np.float64(0.02109222074704774),\n",
       "   np.float64(0.022118080028026864),\n",
       "   np.float64(0.021248503267481247),\n",
       "   np.float64(0.018608881579668526),\n",
       "   np.float64(0.01997094633533833),\n",
       "   np.float64(0.022375298209848432),\n",
       "   np.float64(0.02077224121063956),\n",
       "   np.float64(0.02176217219879882),\n",
       "   np.float64(0.021067331755975322),\n",
       "   np.float64(0.021462880966289125),\n",
       "   np.float64(0.018342426218998297),\n",
       "   np.float64(0.01794679038785077),\n",
       "   np.float64(0.019692233231280297),\n",
       "   np.float64(0.019353856700036268),\n",
       "   np.float64(0.020726622992115713),\n",
       "   np.float64(0.019455794189239265),\n",
       "   np.float64(0.02151144094614638),\n",
       "   np.float64(0.017821885620684822),\n",
       "   np.float64(0.02137578759647405),\n",
       "   np.float64(0.02236035741213403),\n",
       "   np.float64(0.01883585903623048),\n",
       "   np.float64(0.021787848179674028),\n",
       "   np.float64(0.02106438279997137),\n",
       "   np.float64(0.02267866585519073),\n",
       "   np.float64(0.02133343904544802),\n",
       "   np.float64(0.022575673939903926),\n",
       "   np.float64(0.021841229375055275),\n",
       "   np.float64(0.020778304661381506),\n",
       "   np.float64(0.02188030729202208),\n",
       "   np.float64(0.02053559398603786),\n",
       "   np.float64(0.02030541517900155),\n",
       "   np.float64(0.01997807872454984),\n",
       "   np.float64(0.020500635149901435),\n",
       "   np.float64(0.019056224855552754),\n",
       "   np.float64(0.022056699097417944),\n",
       "   np.float64(0.021985801598988868),\n",
       "   np.float64(0.022318021441639347),\n",
       "   np.float64(0.018805201563225165),\n",
       "   np.float64(0.02215233113937173),\n",
       "   np.float64(0.022994426428356515),\n",
       "   np.float64(0.022445168584797823),\n",
       "   np.float64(0.020839039605414063),\n",
       "   np.float64(0.019637221511039762),\n",
       "   np.float64(0.020063158195571543),\n",
       "   np.float64(0.020158909479827344),\n",
       "   np.float64(0.021408827900929635),\n",
       "   np.float64(0.019671998870946197),\n",
       "   np.float64(0.01976543732402465),\n",
       "   np.float64(0.0185308843595836),\n",
       "   np.float64(0.019075550595383853),\n",
       "   np.float64(0.019795103522357776),\n",
       "   np.float64(0.018537946161812837),\n",
       "   np.float64(0.020572806152503433),\n",
       "   np.float64(0.019376411806313054),\n",
       "   np.float64(0.01845739939267097),\n",
       "   np.float64(0.019518598069775697),\n",
       "   np.float64(0.01797379058427551),\n",
       "   np.float64(0.01917760508566963),\n",
       "   np.float64(0.017823137255992015),\n",
       "   np.float64(0.017743689657227567),\n",
       "   np.float64(0.018752820387163006),\n",
       "   np.float64(0.023990674220565188),\n",
       "   np.float64(0.021198448109256186),\n",
       "   np.float64(0.021894317726072456),\n",
       "   np.float64(0.020565708290850984),\n",
       "   np.float64(0.020681894771704033),\n",
       "   np.float64(0.021274240557236614),\n",
       "   np.float64(0.02000881830552061),\n",
       "   np.float64(0.020515939752675346),\n",
       "   np.float64(0.02175894995779239),\n",
       "   np.float64(0.020652738704330206),\n",
       "   np.float64(0.021013864952053332),\n",
       "   np.float64(0.02233706874378179),\n",
       "   np.float64(0.022960854327623605),\n",
       "   np.float64(0.01932356114644975),\n",
       "   np.float64(0.019202749881066305),\n",
       "   np.float64(0.01805163016142532),\n",
       "   np.float64(0.01911900110432093),\n",
       "   np.float64(0.019311604950102843),\n",
       "   np.float64(0.02359271608409492),\n",
       "   np.float64(0.020051297943644606)],\n",
       "  'train_total_affi_coeff_mean': [np.float64(0.33324124514163744),\n",
       "   np.float64(0.3234513700274937),\n",
       "   np.float64(0.3177921353904493),\n",
       "   np.float64(0.3203517917210044),\n",
       "   np.float64(0.3118013766905107),\n",
       "   np.float64(0.3277162147665378),\n",
       "   np.float64(0.3176268839424926),\n",
       "   np.float64(0.34872571627023624),\n",
       "   np.float64(0.31026028899048175),\n",
       "   np.float64(0.326964758024914),\n",
       "   np.float64(0.3025380172235158),\n",
       "   np.float64(0.3148506888078409),\n",
       "   np.float64(0.3353759061816169),\n",
       "   np.float64(0.31044310835107825),\n",
       "   np.float64(0.3281721298603914),\n",
       "   np.float64(0.33031138141271355),\n",
       "   np.float64(0.3404975169919387),\n",
       "   np.float64(0.32240927984463746),\n",
       "   np.float64(0.344660092541591),\n",
       "   np.float64(0.3353822035719583),\n",
       "   np.float64(0.32985788794627174),\n",
       "   np.float64(0.3286420738895302),\n",
       "   np.float64(0.34271839607323534),\n",
       "   np.float64(0.35684435472979487),\n",
       "   np.float64(0.3525216669630422),\n",
       "   np.float64(0.33110001887969326),\n",
       "   np.float64(0.3214641083054283),\n",
       "   np.float64(0.337010842420131),\n",
       "   np.float64(0.338305524711039),\n",
       "   np.float64(0.36507573786253095),\n",
       "   np.float64(0.30069844441309074),\n",
       "   np.float64(0.33526310850165414),\n",
       "   np.float64(0.34630793898952794),\n",
       "   np.float64(0.3416394930156843),\n",
       "   np.float64(0.33519371465172026),\n",
       "   np.float64(0.35938581774053235),\n",
       "   np.float64(0.3574630958937284),\n",
       "   np.float64(0.3625844049441623),\n",
       "   np.float64(0.34008719258211234),\n",
       "   np.float64(0.3518995600043821),\n",
       "   np.float64(0.35429505439570896),\n",
       "   np.float64(0.32164328488244315),\n",
       "   np.float64(0.33967273920378616),\n",
       "   np.float64(0.349242462414696),\n",
       "   np.float64(0.3773986271983869),\n",
       "   np.float64(0.32676878816532834),\n",
       "   np.float64(0.3143159661180158),\n",
       "   np.float64(0.34321100477321825),\n",
       "   np.float64(0.3449088300081467),\n",
       "   np.float64(0.3410460263203163),\n",
       "   np.float64(0.34566507759417625),\n",
       "   np.float64(0.34299974044238674),\n",
       "   np.float64(0.379365085103089),\n",
       "   np.float64(0.37144719536827225),\n",
       "   np.float64(0.35105722513191284),\n",
       "   np.float64(0.3314437420530798),\n",
       "   np.float64(0.34674503330825146),\n",
       "   np.float64(0.3803419185915059),\n",
       "   np.float64(0.36682827225600284),\n",
       "   np.float64(0.35864852187072155),\n",
       "   np.float64(0.33617062726541164),\n",
       "   np.float64(0.37027370274972604),\n",
       "   np.float64(0.31086909761674647),\n",
       "   np.float64(0.38587982613186755),\n",
       "   np.float64(0.3408618700689444),\n",
       "   np.float64(0.3662086568137848),\n",
       "   np.float64(0.3341583991755481),\n",
       "   np.float64(0.36762383038693747),\n",
       "   np.float64(0.37547742964836667),\n",
       "   np.float64(0.35121183858400606),\n",
       "   np.float64(0.3624133172335917),\n",
       "   np.float64(0.3349305064854955),\n",
       "   np.float64(0.3437798346691581),\n",
       "   np.float64(0.3337364184792293),\n",
       "   np.float64(0.330193432651591),\n",
       "   np.float64(0.35318601395364657),\n",
       "   np.float64(0.3632903590243333),\n",
       "   np.float64(0.36811330291612626),\n",
       "   np.float64(0.36845345255444045),\n",
       "   np.float64(0.35671884647541674),\n",
       "   np.float64(0.3368058782527661),\n",
       "   np.float64(0.35972285491438527),\n",
       "   np.float64(0.32068862484254945),\n",
       "   np.float64(0.34693390219496095),\n",
       "   np.float64(0.35612888588583586),\n",
       "   np.float64(0.3717488422847176),\n",
       "   np.float64(0.36324699218492734),\n",
       "   np.float64(0.35719351929325394),\n",
       "   np.float64(0.3305882883117139),\n",
       "   np.float64(0.36093390455022584),\n",
       "   np.float64(0.34759489797430304),\n",
       "   np.float64(0.3372660199454153),\n",
       "   np.float64(0.3473922177141299),\n",
       "   np.float64(0.36316743252303035),\n",
       "   np.float64(0.3643472850518203),\n",
       "   np.float64(0.34961726416376865),\n",
       "   np.float64(0.35490512543400404),\n",
       "   np.float64(0.3612308097699193),\n",
       "   np.float64(0.3550107832699582),\n",
       "   np.float64(0.36074407073315734),\n",
       "   np.float64(0.35850138449477514),\n",
       "   np.float64(0.34556080553454976),\n",
       "   np.float64(0.35740972811355953),\n",
       "   np.float64(0.34682962976320797),\n",
       "   np.float64(0.35061457844365873),\n",
       "   np.float64(0.36395708669092613),\n",
       "   np.float64(0.36594235910822787),\n",
       "   np.float64(0.3320324066406741),\n",
       "   np.float64(0.3423607416273904),\n",
       "   np.float64(0.31690456807615985),\n",
       "   np.float64(0.3687080417172208),\n",
       "   np.float64(0.31959014372137745),\n",
       "   np.float64(0.33771441781890793),\n",
       "   np.float64(0.36006024218965055),\n",
       "   np.float64(0.3160783036784196),\n",
       "   np.float64(0.3588931711470417),\n",
       "   np.float64(0.3545429678207162),\n",
       "   np.float64(0.34118734531305867),\n",
       "   np.float64(0.32367358031922827),\n",
       "   np.float64(0.32967214418721064),\n",
       "   np.float64(0.33290046437643783),\n",
       "   np.float64(0.320291825225409),\n",
       "   np.float64(0.3426218352572954),\n",
       "   np.float64(0.33983124624770666),\n",
       "   np.float64(0.34087168077453234),\n",
       "   np.float64(0.3549757879614188),\n",
       "   np.float64(0.3448097218227605),\n",
       "   np.float64(0.3680825370547456),\n",
       "   np.float64(0.34688765266129906),\n",
       "   np.float64(0.3292932917730856),\n",
       "   np.float64(0.3453077673362728),\n",
       "   np.float64(0.32616694686295333),\n",
       "   np.float64(0.3386427327519243),\n",
       "   np.float64(0.33123236756991453),\n",
       "   np.float64(0.3383328794905623),\n",
       "   np.float64(0.33312693704033836),\n",
       "   np.float64(0.3197156232689945),\n",
       "   np.float64(0.32124325926341724),\n",
       "   np.float64(0.33568790907393525),\n",
       "   np.float64(0.3162264887009151),\n",
       "   np.float64(0.3339619707173369),\n",
       "   np.float64(0.3333355411834582),\n",
       "   np.float64(0.34952591309526404),\n",
       "   np.float64(0.3484628693289467),\n",
       "   np.float64(0.3194853539543899),\n",
       "   np.float64(0.3422105941664411),\n",
       "   np.float64(0.3323093792411403),\n",
       "   np.float64(0.32104126142033834),\n",
       "   np.float64(0.3233645995160331),\n",
       "   np.float64(0.34282787029165496),\n",
       "   np.float64(0.3384708075066682),\n",
       "   np.float64(0.31932800283491686),\n",
       "   np.float64(0.3520759789235862),\n",
       "   np.float64(0.31933165305582567),\n",
       "   np.float64(0.32243641245461496),\n",
       "   np.float64(0.31019607502242397),\n",
       "   np.float64(0.33834795258797945),\n",
       "   np.float64(0.35150898307843575),\n",
       "   np.float64(0.33283777865626957),\n",
       "   np.float64(0.3356520665584722),\n",
       "   np.float64(0.3442886396174629),\n",
       "   np.float64(0.34083239117681335),\n",
       "   np.float64(0.3515152721230259),\n",
       "   np.float64(0.31891127760575094),\n",
       "   np.float64(0.33764218905889953),\n",
       "   np.float64(0.333909970589245),\n",
       "   np.float64(0.3661249314773999),\n",
       "   np.float64(0.32703947388306054),\n",
       "   np.float64(0.31864541299193877),\n",
       "   np.float64(0.3620856211985802),\n",
       "   np.float64(0.3368891940498081),\n",
       "   np.float64(0.3234791665875917),\n",
       "   np.float64(0.33222292822801125),\n",
       "   np.float64(0.34049570240839605),\n",
       "   np.float64(0.3074355512959707),\n",
       "   np.float64(0.30602510244210784),\n",
       "   np.float64(0.3244568206204457),\n",
       "   np.float64(0.33601300285216296),\n",
       "   np.float64(0.3232340719514707),\n",
       "   np.float64(0.3383366307238137),\n",
       "   np.float64(0.3240543504861993),\n",
       "   np.float64(0.3211101338021024),\n",
       "   np.float64(0.32201046553522894),\n",
       "   np.float64(0.3356587146342139),\n",
       "   np.float64(0.3485606697680849),\n",
       "   np.float64(0.31544856559950885),\n",
       "   np.float64(0.3288607853155803),\n",
       "   np.float64(0.3305129045815315),\n",
       "   np.float64(0.31692545703813946),\n",
       "   np.float64(0.33589549176230316),\n",
       "   np.float64(0.35177325324281455),\n",
       "   np.float64(0.31089051397752265),\n",
       "   np.float64(0.3414219867793949),\n",
       "   np.float64(0.3434690601978143),\n",
       "   np.float64(0.35035162959050203),\n",
       "   np.float64(0.30707479066235466),\n",
       "   np.float64(0.36484425222758277),\n",
       "   np.float64(0.31929165624006045),\n",
       "   np.float64(0.32708966302154757),\n",
       "   np.float64(0.29419382333365107),\n",
       "   np.float64(0.3260063169759343),\n",
       "   np.float64(0.27510482328579355),\n",
       "   np.float64(0.3287276042517248),\n",
       "   np.float64(0.3215441933955314),\n",
       "   np.float64(0.32765892679105746),\n",
       "   np.float64(0.3264111944375492),\n",
       "   np.float64(0.3503075120283944),\n",
       "   np.float64(0.34703022016345614),\n",
       "   np.float64(0.31862560544650054),\n",
       "   np.float64(0.3241956583689977),\n",
       "   np.float64(0.34797382668245336),\n",
       "   np.float64(0.3217761518237255),\n",
       "   np.float64(0.34609052957070474),\n",
       "   np.float64(0.3299109393169976),\n",
       "   np.float64(0.3234208497871506),\n",
       "   np.float64(0.32200372925424886),\n",
       "   np.float64(0.33240571214508824),\n",
       "   np.float64(0.3416792186379841),\n",
       "   np.float64(0.3242427445505627),\n",
       "   np.float64(0.32230567094643914),\n",
       "   np.float64(0.3343911286257557),\n",
       "   np.float64(0.31432121004421576),\n",
       "   np.float64(0.31193669488764164),\n",
       "   np.float64(0.3161826384829201),\n",
       "   np.float64(0.32203881330946205),\n",
       "   np.float64(0.3215819202335453),\n",
       "   np.float64(0.2856213409568423),\n",
       "   np.float64(0.3314171831494873),\n",
       "   np.float64(0.34249175073513277),\n",
       "   np.float64(0.32940213600390783),\n",
       "   np.float64(0.3112969871097215),\n",
       "   np.float64(0.3049361470101516),\n",
       "   np.float64(0.30212054993378074),\n",
       "   np.float64(0.32669380814404597),\n",
       "   np.float64(0.32736924750211743),\n",
       "   np.float64(0.32852489298479604),\n",
       "   np.float64(0.3051232807157531),\n",
       "   np.float64(0.30311429706556126),\n",
       "   np.float64(0.3297459193146131),\n",
       "   np.float64(0.2932728980756238),\n",
       "   np.float64(0.36044514849763654),\n",
       "   np.float64(0.315681338515894),\n",
       "   np.float64(0.33376560383502807),\n",
       "   np.float64(0.2903704820324071),\n",
       "   np.float64(0.3484634278566176),\n",
       "   np.float64(0.3499342445350736),\n",
       "   np.float64(0.28367314117060166),\n",
       "   np.float64(0.3294610368303765),\n",
       "   np.float64(0.3192815436353229),\n",
       "   np.float64(0.30831787895105595),\n",
       "   np.float64(0.32068050527152164),\n",
       "   np.float64(0.30188345602732014),\n",
       "   np.float64(0.31993988813110535),\n",
       "   np.float64(0.3091360596738828),\n",
       "   np.float64(0.33203808427887155),\n",
       "   np.float64(0.31166041536573735),\n",
       "   np.float64(0.3267071538964381),\n",
       "   np.float64(0.2982337950005928),\n",
       "   np.float64(0.32078726572914945),\n",
       "   np.float64(0.31787707115666153),\n",
       "   np.float64(0.288648011470202),\n",
       "   np.float64(0.3032354055192655),\n",
       "   np.float64(0.31676468970185145),\n",
       "   np.float64(0.3061342567543203),\n",
       "   np.float64(0.33504982807044725),\n",
       "   np.float64(0.3119764217875688),\n",
       "   np.float64(0.33322980365310256),\n",
       "   np.float64(0.3311362542937826),\n",
       "   np.float64(0.3020209261046548),\n",
       "   np.float64(0.3080348073662334),\n",
       "   np.float64(0.30328815957513555),\n",
       "   np.float64(0.32593311458066715),\n",
       "   np.float64(0.3331685290982201),\n",
       "   np.float64(0.306158448703397),\n",
       "   np.float64(0.3311689319670872),\n",
       "   np.float64(0.31659349101035994),\n",
       "   np.float64(0.3223583041573729),\n",
       "   np.float64(0.3026293670775769),\n",
       "   np.float64(0.28753063179606647),\n",
       "   np.float64(0.2994479281492318),\n",
       "   np.float64(0.29575444203113715),\n",
       "   np.float64(0.2947328574218579),\n",
       "   np.float64(0.31965639539589774),\n",
       "   np.float64(0.32662700706118036),\n",
       "   np.float64(0.29098166732432634),\n",
       "   np.float64(0.34145774185000227),\n",
       "   np.float64(0.3193649301616863),\n",
       "   np.float64(0.327512180174633),\n",
       "   np.float64(0.31799635968974677),\n",
       "   np.float64(0.32774402000159425),\n",
       "   np.float64(0.2907859287749696),\n",
       "   np.float64(0.3006808862045207),\n",
       "   np.float64(0.30163542725457615),\n",
       "   np.float64(0.32959461199849927),\n",
       "   np.float64(0.3251156422052576),\n",
       "   np.float64(0.30194368699082935),\n",
       "   np.float64(0.32981114394694216),\n",
       "   np.float64(0.32386921608278774),\n",
       "   np.float64(0.31682469867242957),\n",
       "   np.float64(0.29037106354472),\n",
       "   np.float64(0.28184665303222833),\n",
       "   np.float64(0.31962868249856113),\n",
       "   np.float64(0.31467493993668255),\n",
       "   np.float64(0.2757717632974687),\n",
       "   np.float64(0.3150098764462906),\n",
       "   np.float64(0.3074688960887191),\n",
       "   np.float64(0.3391465396234315),\n",
       "   np.float64(0.32348568602363903),\n",
       "   np.float64(0.32549994865000853),\n",
       "   np.float64(0.30856777287998705),\n",
       "   np.float64(0.3143060072232582),\n",
       "   np.float64(0.3349518973762081),\n",
       "   np.float64(0.3144694582671857),\n",
       "   np.float64(0.33542999369156556),\n",
       "   np.float64(0.327115438502075),\n",
       "   np.float64(0.3181450758043068),\n",
       "   np.float64(0.3057686801704745),\n",
       "   np.float64(0.304835861321336),\n",
       "   np.float64(0.3174140491913033),\n",
       "   np.float64(0.3389726568117035),\n",
       "   np.float64(0.293220623403353),\n",
       "   np.float64(0.29037742248061366),\n",
       "   np.float64(0.3158467459636041),\n",
       "   np.float64(0.32695253891151155),\n",
       "   np.float64(0.32140195457938675),\n",
       "   np.float64(0.36083173420101966),\n",
       "   np.float64(0.3172019121759258),\n",
       "   np.float64(0.3273302000533704),\n",
       "   np.float64(0.3314776074405082),\n",
       "   np.float64(0.31097353366262803),\n",
       "   np.float64(0.2912651348314499),\n",
       "   np.float64(0.3210085393638578),\n",
       "   np.float64(0.30024389426046),\n",
       "   np.float64(0.32468083931656844),\n",
       "   np.float64(0.3381767882337469),\n",
       "   np.float64(0.33495080673276184),\n",
       "   np.float64(0.2910728618561396),\n",
       "   np.float64(0.33013335696140067),\n",
       "   np.float64(0.31994506019674296),\n",
       "   np.float64(0.29687061333027026),\n",
       "   np.float64(0.3182775305915128),\n",
       "   np.float64(0.30745619922081957),\n",
       "   np.float64(0.31822491169772577),\n",
       "   np.float64(0.33778714492965384),\n",
       "   np.float64(0.326734160130734),\n",
       "   np.float64(0.2956125978250686),\n",
       "   np.float64(0.3140482225000305),\n",
       "   np.float64(0.3300299453074495),\n",
       "   np.float64(0.28266558763839567),\n",
       "   np.float64(0.3160796449384165),\n",
       "   np.float64(0.28452510247253116),\n",
       "   np.float64(0.3093816651401169),\n",
       "   np.float64(0.3227067898106275),\n",
       "   np.float64(0.3044400650990708),\n",
       "   np.float64(0.32905208094835714),\n",
       "   np.float64(0.3118193564882495),\n",
       "   np.float64(0.2825195242900087),\n",
       "   np.float64(0.31044625270443293),\n",
       "   np.float64(0.3182008883043188),\n",
       "   np.float64(0.29377412250736473),\n",
       "   np.float64(0.3150993837308772),\n",
       "   np.float64(0.2971326242376044),\n",
       "   np.float64(0.29636021026898113),\n",
       "   np.float64(0.2955408843766202),\n",
       "   np.float64(0.3008724669838855),\n",
       "   np.float64(0.31696161887047364),\n",
       "   np.float64(0.2873411426625639),\n",
       "   np.float64(0.30764291906769403),\n",
       "   np.float64(0.30161983151197425),\n",
       "   np.float64(0.3112237139277745),\n",
       "   np.float64(0.2963240395847568),\n",
       "   np.float64(0.3214605381389285),\n",
       "   np.float64(0.29046192977336593),\n",
       "   np.float64(0.30630068575239167),\n",
       "   np.float64(0.2763979213830684),\n",
       "   np.float64(0.2960022579225215),\n",
       "   np.float64(0.30180004695549717),\n",
       "   np.float64(0.2913254226694731),\n",
       "   np.float64(0.27913420466758543),\n",
       "   np.float64(0.32345478595063654),\n",
       "   np.float64(0.2958519919495921),\n",
       "   np.float64(0.30520377500550333),\n",
       "   np.float64(0.29263015963353145),\n",
       "   np.float64(0.293886820586866),\n",
       "   np.float64(0.31784401922571137),\n",
       "   np.float64(0.320991632110772),\n",
       "   np.float64(0.32454558666044087),\n",
       "   np.float64(0.30436555183549857),\n",
       "   np.float64(0.30273627912381235),\n",
       "   np.float64(0.3217057752975231),\n",
       "   np.float64(0.33319290096048493),\n",
       "   np.float64(0.31801596167749235),\n",
       "   np.float64(0.2873167876586748),\n",
       "   np.float64(0.2985336868383712),\n",
       "   np.float64(0.3143943115392764),\n",
       "   np.float64(0.28151706062464366),\n",
       "   np.float64(0.30035228402027553),\n",
       "   np.float64(0.26971336570519494),\n",
       "   np.float64(0.3241000398624726),\n",
       "   np.float64(0.30135036604726806),\n",
       "   np.float64(0.3161679368628538),\n",
       "   np.float64(0.32516670708875905),\n",
       "   np.float64(0.27669619910349036),\n",
       "   np.float64(0.30612262911408605),\n",
       "   np.float64(0.3060250407437906),\n",
       "   np.float64(0.3237866473596296),\n",
       "   np.float64(0.2618741631127024),\n",
       "   np.float64(0.3089208449110791),\n",
       "   np.float64(0.29805167893747603),\n",
       "   np.float64(0.3106894016334108),\n",
       "   np.float64(0.2954410517697958),\n",
       "   np.float64(0.2695893214577218),\n",
       "   np.float64(0.3026064726064266),\n",
       "   np.float64(0.31871979919391635),\n",
       "   np.float64(0.31829919673589224),\n",
       "   np.float64(0.28967898728692393),\n",
       "   np.float64(0.3108669752363379),\n",
       "   np.float64(0.31323273030305204),\n",
       "   np.float64(0.2950303164153857),\n",
       "   np.float64(0.3160795962175549),\n",
       "   np.float64(0.29807781378723297),\n",
       "   np.float64(0.27568516980526864),\n",
       "   np.float64(0.30799315979363406),\n",
       "   np.float64(0.34843120287829693),\n",
       "   np.float64(0.3037736850399493),\n",
       "   np.float64(0.2789044281163828),\n",
       "   np.float64(0.31244305768650144),\n",
       "   np.float64(0.2893229653687157),\n",
       "   np.float64(0.2939985475324817),\n",
       "   np.float64(0.32116822946782053),\n",
       "   np.float64(0.2999070822262917),\n",
       "   np.float64(0.33921648450663155),\n",
       "   np.float64(0.30418713931845265),\n",
       "   np.float64(0.2722417256263675),\n",
       "   np.float64(0.33138707393928485),\n",
       "   np.float64(0.3042344776541056),\n",
       "   np.float64(0.286014826128734)],\n",
       "  'train_total_affi_coeff_std': [np.float64(0.1557701806203094),\n",
       "   np.float64(0.13041239328073578),\n",
       "   np.float64(0.1359155912441213),\n",
       "   np.float64(0.15743918273683388),\n",
       "   np.float64(0.12829155633016565),\n",
       "   np.float64(0.1549176907061195),\n",
       "   np.float64(0.14722563766401783),\n",
       "   np.float64(0.13661228936025355),\n",
       "   np.float64(0.15822974442881804),\n",
       "   np.float64(0.16838715031211984),\n",
       "   np.float64(0.1626067158262034),\n",
       "   np.float64(0.1492570947822203),\n",
       "   np.float64(0.17632439628793045),\n",
       "   np.float64(0.14403771203781376),\n",
       "   np.float64(0.13724871623922472),\n",
       "   np.float64(0.13872447982343905),\n",
       "   np.float64(0.1700024210583291),\n",
       "   np.float64(0.14553002947668936),\n",
       "   np.float64(0.14951876005105746),\n",
       "   np.float64(0.15914245917729936),\n",
       "   np.float64(0.15467976880585738),\n",
       "   np.float64(0.13180179059516797),\n",
       "   np.float64(0.15414641827996167),\n",
       "   np.float64(0.12847521652068417),\n",
       "   np.float64(0.1410854817047151),\n",
       "   np.float64(0.14011123404107148),\n",
       "   np.float64(0.1361273749827045),\n",
       "   np.float64(0.13523970570175922),\n",
       "   np.float64(0.15881970342532445),\n",
       "   np.float64(0.13993452853449956),\n",
       "   np.float64(0.15164496267710603),\n",
       "   np.float64(0.16147282047650283),\n",
       "   np.float64(0.16117133794151078),\n",
       "   np.float64(0.14564999780886914),\n",
       "   np.float64(0.13729561565606427),\n",
       "   np.float64(0.1427341393421313),\n",
       "   np.float64(0.13891116326939082),\n",
       "   np.float64(0.1386379652869051),\n",
       "   np.float64(0.14356138780955782),\n",
       "   np.float64(0.11933278840537918),\n",
       "   np.float64(0.14741447910459077),\n",
       "   np.float64(0.15428773654746863),\n",
       "   np.float64(0.15364748499941752),\n",
       "   np.float64(0.14944902236533678),\n",
       "   np.float64(0.14406830281380179),\n",
       "   np.float64(0.1436663682253612),\n",
       "   np.float64(0.15275835167538515),\n",
       "   np.float64(0.14560974096554244),\n",
       "   np.float64(0.14567067384456187),\n",
       "   np.float64(0.15051672171260486),\n",
       "   np.float64(0.14545481431393995),\n",
       "   np.float64(0.1595011778777647),\n",
       "   np.float64(0.12908588345006966),\n",
       "   np.float64(0.14034095062718743),\n",
       "   np.float64(0.15578724971159935),\n",
       "   np.float64(0.13778994985623694),\n",
       "   np.float64(0.13351633871315893),\n",
       "   np.float64(0.12872577433879437),\n",
       "   np.float64(0.15440023006999717),\n",
       "   np.float64(0.14383744652643632),\n",
       "   np.float64(0.14471863669050952),\n",
       "   np.float64(0.15177314445114662),\n",
       "   np.float64(0.1579778668652892),\n",
       "   np.float64(0.1481103588325783),\n",
       "   np.float64(0.15217286698758095),\n",
       "   np.float64(0.156148096477805),\n",
       "   np.float64(0.1622183084002613),\n",
       "   np.float64(0.16122364784143864),\n",
       "   np.float64(0.15658708873296648),\n",
       "   np.float64(0.14747566662685263),\n",
       "   np.float64(0.13688541582278682),\n",
       "   np.float64(0.16282988398721146),\n",
       "   np.float64(0.14970613527040233),\n",
       "   np.float64(0.1422600670161326),\n",
       "   np.float64(0.15460165792911654),\n",
       "   np.float64(0.13367900156089754),\n",
       "   np.float64(0.16978879890453447),\n",
       "   np.float64(0.12936231439329524),\n",
       "   np.float64(0.16082039354765915),\n",
       "   np.float64(0.14231720187575694),\n",
       "   np.float64(0.14774134077097625),\n",
       "   np.float64(0.1523927983217789),\n",
       "   np.float64(0.14205283377826083),\n",
       "   np.float64(0.16805331983021624),\n",
       "   np.float64(0.15049591475870022),\n",
       "   np.float64(0.14868014446894873),\n",
       "   np.float64(0.16438538158557203),\n",
       "   np.float64(0.16266103390534353),\n",
       "   np.float64(0.14293132317727414),\n",
       "   np.float64(0.15197508763819073),\n",
       "   np.float64(0.1447423009599651),\n",
       "   np.float64(0.150208091235928),\n",
       "   np.float64(0.162333565249593),\n",
       "   np.float64(0.1568977477294073),\n",
       "   np.float64(0.1467774153992566),\n",
       "   np.float64(0.1494229655460999),\n",
       "   np.float64(0.1383480248090788),\n",
       "   np.float64(0.14855174475393196),\n",
       "   np.float64(0.15788233043967945),\n",
       "   np.float64(0.14746944910801013),\n",
       "   np.float64(0.15406219371442278),\n",
       "   np.float64(0.15093556242123984),\n",
       "   np.float64(0.14580214548197243),\n",
       "   np.float64(0.1417145428603707),\n",
       "   np.float64(0.15306080370204025),\n",
       "   np.float64(0.15304067250988201),\n",
       "   np.float64(0.14973777039549943),\n",
       "   np.float64(0.15770749729134798),\n",
       "   np.float64(0.14748396857475832),\n",
       "   np.float64(0.15529502260998682),\n",
       "   np.float64(0.14952221633718937),\n",
       "   np.float64(0.15736100246628254),\n",
       "   np.float64(0.15758881884028073),\n",
       "   np.float64(0.15854299020728052),\n",
       "   np.float64(0.16818966400270988),\n",
       "   np.float64(0.16527848071013937),\n",
       "   np.float64(0.1340547409702133),\n",
       "   np.float64(0.1609441281944081),\n",
       "   np.float64(0.13776706107323328),\n",
       "   np.float64(0.15932714848214163),\n",
       "   np.float64(0.13103058968232834),\n",
       "   np.float64(0.1330574126366446),\n",
       "   np.float64(0.1548459391018219),\n",
       "   np.float64(0.13110048855713827),\n",
       "   np.float64(0.15166524255477967),\n",
       "   np.float64(0.15667907440360823),\n",
       "   np.float64(0.154206358075215),\n",
       "   np.float64(0.16512565965590226),\n",
       "   np.float64(0.15926611457777287),\n",
       "   np.float64(0.1637867394891646),\n",
       "   np.float64(0.1543675519372177),\n",
       "   np.float64(0.17611555358746436),\n",
       "   np.float64(0.12999911972490016),\n",
       "   np.float64(0.15492640876269817),\n",
       "   np.float64(0.13270042291565215),\n",
       "   np.float64(0.1574879964660311),\n",
       "   np.float64(0.15737963196629265),\n",
       "   np.float64(0.16391880757555752),\n",
       "   np.float64(0.14417559107630615),\n",
       "   np.float64(0.156672272363612),\n",
       "   np.float64(0.15913724379177605),\n",
       "   np.float64(0.16607357277345058),\n",
       "   np.float64(0.13263957055761388),\n",
       "   np.float64(0.14570786879775144),\n",
       "   np.float64(0.16485888335775767),\n",
       "   np.float64(0.1650463439373455),\n",
       "   np.float64(0.15117516044120732),\n",
       "   np.float64(0.13735142233354122),\n",
       "   np.float64(0.15172870009278727),\n",
       "   np.float64(0.1498798125336879),\n",
       "   np.float64(0.1825162315485629),\n",
       "   np.float64(0.1353915849077722),\n",
       "   np.float64(0.13931644223020126),\n",
       "   np.float64(0.1556197994683704),\n",
       "   np.float64(0.15898952474531933),\n",
       "   np.float64(0.15608501156605387),\n",
       "   np.float64(0.14860327915905241),\n",
       "   np.float64(0.13894550503949188),\n",
       "   np.float64(0.17185671425746296),\n",
       "   np.float64(0.15250414232639486),\n",
       "   np.float64(0.14695686449946388),\n",
       "   np.float64(0.16872180273578952),\n",
       "   np.float64(0.14945287084255368),\n",
       "   np.float64(0.14690743560752423),\n",
       "   np.float64(0.1394729438791058),\n",
       "   np.float64(0.16633916643578733),\n",
       "   np.float64(0.14568529992864115),\n",
       "   np.float64(0.1484308413656011),\n",
       "   np.float64(0.15143451106166042),\n",
       "   np.float64(0.15409501256536032),\n",
       "   np.float64(0.1752849750326169),\n",
       "   np.float64(0.17420036248245893),\n",
       "   np.float64(0.14802794348054932),\n",
       "   np.float64(0.15985924461732162),\n",
       "   np.float64(0.16100831540240554),\n",
       "   np.float64(0.15827069303487937),\n",
       "   np.float64(0.1478489586118309),\n",
       "   np.float64(0.15819353597222546),\n",
       "   np.float64(0.14453010311657977),\n",
       "   np.float64(0.14903857488187994),\n",
       "   np.float64(0.15222856132792376),\n",
       "   np.float64(0.15589100236137643),\n",
       "   np.float64(0.14324882458977997),\n",
       "   np.float64(0.15073780278561885),\n",
       "   np.float64(0.153794845594346),\n",
       "   np.float64(0.14624624366717234),\n",
       "   np.float64(0.13610242597189764),\n",
       "   np.float64(0.1674918481650249),\n",
       "   np.float64(0.13320204651858092),\n",
       "   np.float64(0.12910756025650685),\n",
       "   np.float64(0.15077770720824415),\n",
       "   np.float64(0.16817249531251313),\n",
       "   np.float64(0.15597981147526313),\n",
       "   np.float64(0.17857429340328926),\n",
       "   np.float64(0.14050215947365577),\n",
       "   np.float64(0.14413737842448457),\n",
       "   np.float64(0.15793967884621907),\n",
       "   np.float64(0.13843472055211276),\n",
       "   np.float64(0.16203111669953424),\n",
       "   np.float64(0.15192311074054726),\n",
       "   np.float64(0.14931678640315546),\n",
       "   np.float64(0.16052562574898185),\n",
       "   np.float64(0.14512905083079436),\n",
       "   np.float64(0.14288637445311173),\n",
       "   np.float64(0.17024061145480648),\n",
       "   np.float64(0.15146770938181048),\n",
       "   np.float64(0.14288700089736087),\n",
       "   np.float64(0.15817547543750707),\n",
       "   np.float64(0.138949293674124),\n",
       "   np.float64(0.14603733948441971),\n",
       "   np.float64(0.16465641778820822),\n",
       "   np.float64(0.15193499728397558),\n",
       "   np.float64(0.14686838701227237),\n",
       "   np.float64(0.1526128945144591),\n",
       "   np.float64(0.14818264873281525),\n",
       "   np.float64(0.16209568220540102),\n",
       "   np.float64(0.1559163140980209),\n",
       "   np.float64(0.16278544160942576),\n",
       "   np.float64(0.1585236951090871),\n",
       "   np.float64(0.15820723315926155),\n",
       "   np.float64(0.1386897602204473),\n",
       "   np.float64(0.16161619791784396),\n",
       "   np.float64(0.1511475469008768),\n",
       "   np.float64(0.16482596593639218),\n",
       "   np.float64(0.15573625525790952),\n",
       "   np.float64(0.15442944093733119),\n",
       "   np.float64(0.14159077459360156),\n",
       "   np.float64(0.1557878568895006),\n",
       "   np.float64(0.16712640374547016),\n",
       "   np.float64(0.1547008532966914),\n",
       "   np.float64(0.1473564184163996),\n",
       "   np.float64(0.1600329371178152),\n",
       "   np.float64(0.14883625504984926),\n",
       "   np.float64(0.1397819694281152),\n",
       "   np.float64(0.13583196872725092),\n",
       "   np.float64(0.15049323511485563),\n",
       "   np.float64(0.15642506066745065),\n",
       "   np.float64(0.16711856155583132),\n",
       "   np.float64(0.16739455293373362),\n",
       "   np.float64(0.16469942716968156),\n",
       "   np.float64(0.16086401252061344),\n",
       "   np.float64(0.1300735227903742),\n",
       "   np.float64(0.16011501369722494),\n",
       "   np.float64(0.16611469111164257),\n",
       "   np.float64(0.13171523226874302),\n",
       "   np.float64(0.15214687177866754),\n",
       "   np.float64(0.15289084111502335),\n",
       "   np.float64(0.14559130511164275),\n",
       "   np.float64(0.1456288465114308),\n",
       "   np.float64(0.15849802179890315),\n",
       "   np.float64(0.1573820554558487),\n",
       "   np.float64(0.14314334270117954),\n",
       "   np.float64(0.15262769465029136),\n",
       "   np.float64(0.15344874076312354),\n",
       "   np.float64(0.1688435577285083),\n",
       "   np.float64(0.15938617865542481),\n",
       "   np.float64(0.14271783096113738),\n",
       "   np.float64(0.15784525426221704),\n",
       "   np.float64(0.15904636730305094),\n",
       "   np.float64(0.1624363619445425),\n",
       "   np.float64(0.16184143455951047),\n",
       "   np.float64(0.1479907899529176),\n",
       "   np.float64(0.14899569233042428),\n",
       "   np.float64(0.16375279202607979),\n",
       "   np.float64(0.146614395032307),\n",
       "   np.float64(0.14517854389835833),\n",
       "   np.float64(0.15752588097719333),\n",
       "   np.float64(0.1515680875896552),\n",
       "   np.float64(0.1479831041156115),\n",
       "   np.float64(0.17937475043861226),\n",
       "   np.float64(0.16355550277279926),\n",
       "   np.float64(0.15727557503781567),\n",
       "   np.float64(0.14895803524049167),\n",
       "   np.float64(0.1463563103078038),\n",
       "   np.float64(0.15409824581896767),\n",
       "   np.float64(0.16147175048358153),\n",
       "   np.float64(0.15938490638316502),\n",
       "   np.float64(0.1749854281586291),\n",
       "   np.float64(0.1594388934748453),\n",
       "   np.float64(0.15549353844105843),\n",
       "   np.float64(0.14224002318543505),\n",
       "   np.float64(0.15799818839901264),\n",
       "   np.float64(0.17525009699547098),\n",
       "   np.float64(0.13561351560393492),\n",
       "   np.float64(0.1507278185853688),\n",
       "   np.float64(0.15203986059870622),\n",
       "   np.float64(0.16310398869037943),\n",
       "   np.float64(0.1415215004402179),\n",
       "   np.float64(0.16180852988524005),\n",
       "   np.float64(0.15682597629522174),\n",
       "   np.float64(0.1721889041573546),\n",
       "   np.float64(0.16037543357147369),\n",
       "   np.float64(0.17410357744904006),\n",
       "   np.float64(0.14846545954469337),\n",
       "   np.float64(0.1572882319796594),\n",
       "   np.float64(0.15599324735237668),\n",
       "   np.float64(0.15716161333150738),\n",
       "   np.float64(0.1512850108542386),\n",
       "   np.float64(0.16709717263084226),\n",
       "   np.float64(0.15856662020538523),\n",
       "   np.float64(0.17805948546705996),\n",
       "   np.float64(0.16394952977932326),\n",
       "   np.float64(0.15512995001062574),\n",
       "   np.float64(0.15686839089693333),\n",
       "   np.float64(0.14002457885591835),\n",
       "   np.float64(0.14184203805576176),\n",
       "   np.float64(0.14363306160162528),\n",
       "   np.float64(0.16387970693416226),\n",
       "   np.float64(0.16610631473243825),\n",
       "   np.float64(0.14740952693185314),\n",
       "   np.float64(0.16024394492979507),\n",
       "   np.float64(0.15560126275470934),\n",
       "   np.float64(0.16761542425302883),\n",
       "   np.float64(0.17497683991854052),\n",
       "   np.float64(0.16614661088557572),\n",
       "   np.float64(0.13711087262937324),\n",
       "   np.float64(0.15262658282128355),\n",
       "   np.float64(0.15703310847764873),\n",
       "   np.float64(0.1769922188260227),\n",
       "   np.float64(0.15236728608021663),\n",
       "   np.float64(0.16206661812215414),\n",
       "   np.float64(0.1705265087644898),\n",
       "   np.float64(0.143234194703916),\n",
       "   np.float64(0.17542677320059427),\n",
       "   np.float64(0.16137749154489126),\n",
       "   np.float64(0.14815255891878412),\n",
       "   np.float64(0.1649799550056731),\n",
       "   np.float64(0.16447687595382054),\n",
       "   np.float64(0.16743018809892082),\n",
       "   np.float64(0.17000627468675383),\n",
       "   np.float64(0.15326215207154884),\n",
       "   np.float64(0.1537430877573841),\n",
       "   np.float64(0.16377448196482605),\n",
       "   np.float64(0.17088702490986452),\n",
       "   np.float64(0.15240953778787314),\n",
       "   np.float64(0.1494364023659517),\n",
       "   np.float64(0.1627150853130755),\n",
       "   np.float64(0.15924012152789827),\n",
       "   np.float64(0.1468714341155808),\n",
       "   np.float64(0.17780434248278826),\n",
       "   np.float64(0.17522476904941006),\n",
       "   np.float64(0.14547367504727896),\n",
       "   np.float64(0.16968645773500532),\n",
       "   np.float64(0.16180238415743808),\n",
       "   np.float64(0.1557056401576145),\n",
       "   np.float64(0.16306736410957925),\n",
       "   np.float64(0.14603774602889505),\n",
       "   np.float64(0.16180238447396192),\n",
       "   np.float64(0.16955975497722103),\n",
       "   np.float64(0.15133491413781477),\n",
       "   np.float64(0.1416443708841291),\n",
       "   np.float64(0.15021243781502844),\n",
       "   np.float64(0.15011871647198008),\n",
       "   np.float64(0.17501835369794455),\n",
       "   np.float64(0.16220567880703307),\n",
       "   np.float64(0.16075740287725318),\n",
       "   np.float64(0.16054362494511915),\n",
       "   np.float64(0.15741748341521272),\n",
       "   np.float64(0.1651640877495177),\n",
       "   np.float64(0.14825060128382428),\n",
       "   np.float64(0.1691660411289693),\n",
       "   np.float64(0.16655834411629974),\n",
       "   np.float64(0.17810005382720642),\n",
       "   np.float64(0.16628824323531818),\n",
       "   np.float64(0.1461108847220653),\n",
       "   np.float64(0.1504759840251936),\n",
       "   np.float64(0.16936887754204355),\n",
       "   np.float64(0.16807523656763868),\n",
       "   np.float64(0.16234848710225921),\n",
       "   np.float64(0.14881978576823304),\n",
       "   np.float64(0.15179201222097324),\n",
       "   np.float64(0.16126487026988826),\n",
       "   np.float64(0.1735980260714171),\n",
       "   np.float64(0.1456609850259952),\n",
       "   np.float64(0.1629065384328812),\n",
       "   np.float64(0.17636350841869683),\n",
       "   np.float64(0.14946526701601562),\n",
       "   np.float64(0.15174565817935987),\n",
       "   np.float64(0.16034297514127896),\n",
       "   np.float64(0.16891763824414743),\n",
       "   np.float64(0.15233724962455997),\n",
       "   np.float64(0.16931805488639887),\n",
       "   np.float64(0.16763821942723964),\n",
       "   np.float64(0.13890687056621465),\n",
       "   np.float64(0.15919650189361445),\n",
       "   np.float64(0.15113674000642388),\n",
       "   np.float64(0.15177376907919127),\n",
       "   np.float64(0.15502591414895417),\n",
       "   np.float64(0.15177047687498763),\n",
       "   np.float64(0.16172252927021522),\n",
       "   np.float64(0.15249436663840085),\n",
       "   np.float64(0.15793363246651923),\n",
       "   np.float64(0.1800796127866815),\n",
       "   np.float64(0.1421391871227007),\n",
       "   np.float64(0.14338480341381057),\n",
       "   np.float64(0.1578145631597326),\n",
       "   np.float64(0.15099709767217961),\n",
       "   np.float64(0.15072378394157215),\n",
       "   np.float64(0.1481336950405653),\n",
       "   np.float64(0.17693278416986577),\n",
       "   np.float64(0.1513943421121763),\n",
       "   np.float64(0.1561705146604156),\n",
       "   np.float64(0.1631679686688697),\n",
       "   np.float64(0.15285336203180647),\n",
       "   np.float64(0.1500601888723653),\n",
       "   np.float64(0.14954629105891182),\n",
       "   np.float64(0.152409840038983),\n",
       "   np.float64(0.16237573228458863),\n",
       "   np.float64(0.14313656384865645),\n",
       "   np.float64(0.1461349947537854),\n",
       "   np.float64(0.1546054375993418),\n",
       "   np.float64(0.1691346407871832),\n",
       "   np.float64(0.15358292416937033),\n",
       "   np.float64(0.16966585315164545),\n",
       "   np.float64(0.15266973293930222),\n",
       "   np.float64(0.16245824128816957),\n",
       "   np.float64(0.15726232007464477),\n",
       "   np.float64(0.14418423636446343),\n",
       "   np.float64(0.16421236227958178),\n",
       "   np.float64(0.15210000613948227),\n",
       "   np.float64(0.15286681417037365),\n",
       "   np.float64(0.1382410267313142),\n",
       "   np.float64(0.15339146294458333),\n",
       "   np.float64(0.13525640017844456),\n",
       "   np.float64(0.15505773069821108),\n",
       "   np.float64(0.15236793656144396),\n",
       "   np.float64(0.16241658332977302),\n",
       "   np.float64(0.1634342587097872),\n",
       "   np.float64(0.17168076602264415),\n",
       "   np.float64(0.1547378767692145),\n",
       "   np.float64(0.1588283936414749),\n",
       "   np.float64(0.12617151523337797),\n",
       "   np.float64(0.17768915207927385),\n",
       "   np.float64(0.14963091500709613),\n",
       "   np.float64(0.14649804069860972),\n",
       "   np.float64(0.17113348311363208),\n",
       "   np.float64(0.17306749450936337)],\n",
       "  'train_learning_rate_mean': [np.float64(0.0010000000000000002),\n",
       "   np.float64(0.0009999923665023704),\n",
       "   np.float64(0.000999973282956946),\n",
       "   np.float64(0.0009999465664289036),\n",
       "   np.float64(0.0009999122173302431),\n",
       "   np.float64(0.0009998702361906696),\n",
       "   np.float64(0.0009998206236575837),\n",
       "   np.float64(0.0009997633804960688),\n",
       "   np.float64(0.0009996985075888844),\n",
       "   np.float64(0.0009996260059364499),\n",
       "   np.float64(0.0009995458766568301),\n",
       "   np.float64(0.0009994581209857147),\n",
       "   np.float64(0.0009993627402764041),\n",
       "   np.float64(0.0009992597359997852),\n",
       "   np.float64(0.0009991491097443106),\n",
       "   np.float64(0.000999030863215971),\n",
       "   np.float64(0.0009989049982382751),\n",
       "   np.float64(0.0009987715167522138),\n",
       "   np.float64(0.0009986304208162349),\n",
       "   np.float64(0.0009984817126062128),\n",
       "   np.float64(0.0009983253944154096),\n",
       "   np.float64(0.0009981614686544427),\n",
       "   np.float64(0.0009979899378512495),\n",
       "   np.float64(0.0009978108046510472),\n",
       "   np.float64(0.0009976240718162883),\n",
       "   np.float64(0.0009974297422266243),\n",
       "   np.float64(0.0009972278188788548),\n",
       "   np.float64(0.0009970183048868867),\n",
       "   np.float64(0.0009968012034816843),\n",
       "   np.float64(0.0009965765180112178),\n",
       "   np.float64(0.000996344251940414),\n",
       "   np.float64(0.0009961044088511017),\n",
       "   np.float64(0.0009958569924419563),\n",
       "   np.float64(0.0009956020065284442),\n",
       "   np.float64(0.000995339455042762),\n",
       "   np.float64(0.0009950693420337762),\n",
       "   np.float64(0.0009947916716669637),\n",
       "   np.float64(0.000994506448224342),\n",
       "   np.float64(0.0009942136761044097),\n",
       "   np.float64(0.0009939133598220722),\n",
       "   np.float64(0.000993605504008578),\n",
       "   np.float64(0.0009932901134114423),\n",
       "   np.float64(0.0009929671928943769),\n",
       "   np.float64(0.0009926367474372149),\n",
       "   np.float64(0.0009922987821358324),\n",
       "   np.float64(0.000991953302202071),\n",
       "   np.float64(0.0009916003129636568),\n",
       "   np.float64(0.0009912398198641194),\n",
       "   np.float64(0.0009908718284627072),\n",
       "   np.float64(0.0009904963444343025),\n",
       "   np.float64(0.0009901133735693295),\n",
       "   np.float64(0.0009897229217736732),\n",
       "   np.float64(0.0009893249950685824),\n",
       "   np.float64(0.0009889195995905762),\n",
       "   np.float64(0.0009885067415913542),\n",
       "   np.float64(0.0009880864274376942),\n",
       "   np.float64(0.0009876586636113614),\n",
       "   np.float64(0.000987223456709001),\n",
       "   np.float64(0.0009867808134420397),\n",
       "   np.float64(0.0009863307406365839),\n",
       "   np.float64(0.0009858732452333102),\n",
       "   np.float64(0.0009854083342873635),\n",
       "   np.float64(0.000984936014968244),\n",
       "   np.float64(0.0009844562945596981),\n",
       "   np.float64(0.000983969180459607),\n",
       "   np.float64(0.0009834746801798717),\n",
       "   np.float64(0.0009829728013462952),\n",
       "   np.float64(0.0009824635516984708),\n",
       "   np.float64(0.0009819469390896546),\n",
       "   np.float64(0.0009814229714866524),\n",
       "   np.float64(0.0009808916569696898),\n",
       "   np.float64(0.000980353003732294),\n",
       "   np.float64(0.0009798070200811616),\n",
       "   np.float64(0.000979253714436036),\n",
       "   np.float64(0.0009786930953295714),\n",
       "   np.float64(0.0009781251714072077),\n",
       "   np.float64(0.0009775499514270332),\n",
       "   np.float64(0.0009769674442596493),\n",
       "   np.float64(0.0009763776588880363),\n",
       "   np.float64(0.000975780604407412),\n",
       "   np.float64(0.000975176290025095),\n",
       "   np.float64(0.0009745647250603577),\n",
       "   np.float64(0.0009739459189442884),\n",
       "   np.float64(0.0009733198812196403),\n",
       "   np.float64(0.0009726866215406892),\n",
       "   np.float64(0.0009720461496730802),\n",
       "   np.float64(0.0009713984754936806),\n",
       "   np.float64(0.0009707436089904266),\n",
       "   np.float64(0.0009700815602621677),\n",
       "   np.float64(0.0009694123395185135),\n",
       "   np.float64(0.0009687359570796744),\n",
       "   np.float64(0.0009680524233763017),\n",
       "   np.float64(0.000967361748949331),\n",
       "   np.float64(0.000966663944449813),\n",
       "   np.float64(0.0009659590206387556),\n",
       "   np.float64(0.0009652469883869554),\n",
       "   np.float64(0.0009645278586748279),\n",
       "   np.float64(0.000963801642592242),\n",
       "   np.float64(0.0009630683513383464),\n",
       "   np.float64(0.0009623279962213988),\n",
       "   np.float64(0.0009615805886585877),\n",
       "   np.float64(0.0009608261401758624),\n",
       "   np.float64(0.0009600646624077501),\n",
       "   np.float64(0.0009592961670971778),\n",
       "   np.float64(0.0009585206660952922),\n",
       "   np.float64(0.0009577381713612775),\n",
       "   np.float64(0.0009569486949621688),\n",
       "   np.float64(0.0009561522490726672),\n",
       "   np.float64(0.000955348845974952),\n",
       "   np.float64(0.0009545384980584916),\n",
       "   np.float64(0.0009537212178198519),\n",
       "   np.float64(0.0009528970178625029),\n",
       "   np.float64(0.0009520659108966273),\n",
       "   np.float64(0.0009512279097389202),\n",
       "   np.float64(0.0009503830273123949),\n",
       "   np.float64(0.0009495312766461818),\n",
       "   np.float64(0.0009486726708753283),\n",
       "   np.float64(0.0009478072232405976),\n",
       "   np.float64(0.000946934947088258),\n",
       "   np.float64(0.000946055855869889),\n",
       "   np.float64(0.0009451699631421608),\n",
       "   np.float64(0.000944277282566635),\n",
       "   np.float64(0.0009433778279095473),\n",
       "   np.float64(0.000942471613041601),\n",
       "   np.float64(0.0009415586519377489),\n",
       "   np.float64(0.0009406389586769781),\n",
       "   np.float64(0.0009397125474420958),\n",
       "   np.float64(0.0009387794325195069),\n",
       "   np.float64(0.0009378396282989965),\n",
       "   np.float64(0.0009368931492735076),\n",
       "   np.float64(0.0009359400100389148),\n",
       "   np.float64(0.0009349802252938033),\n",
       "   np.float64(0.0009340138098392399),\n",
       "   np.float64(0.0009330407785785448),\n",
       "   np.float64(0.0009320611465170618),\n",
       "   np.float64(0.0009310749287619276),\n",
       "   np.float64(0.0009300821405218384),\n",
       "   np.float64(0.0009290827971068154),\n",
       "   np.float64(0.0009280769139279698),\n",
       "   np.float64(0.0009270645064972611),\n",
       "   np.float64(0.0009260455904272643),\n",
       "   np.float64(0.0009250201814309229),\n",
       "   np.float64(0.0009239882953213117),\n",
       "   np.float64(0.0009229499480113888),\n",
       "   np.float64(0.0009219051555137531),\n",
       "   np.float64(0.0009208539339403947),\n",
       "   np.float64(0.0009197962995024496),\n",
       "   np.float64(0.000918732268509948),\n",
       "   np.float64(0.0009176618573715627),\n",
       "   np.float64(0.0009165850825943549),\n",
       "   np.float64(0.0009155019607835228),\n",
       "   np.float64(0.0009144125086421427),\n",
       "   np.float64(0.0009133167429709134),\n",
       "   np.float64(0.0009122146806678947),\n",
       "   np.float64(0.0009111063387282488),\n",
       "   np.float64(0.0009099917342439791),\n",
       "   np.float64(0.0009088708844036629),\n",
       "   np.float64(0.00090774380649219),\n",
       "   np.float64(0.0009066105178904943),\n",
       "   np.float64(0.000905471036075286),\n",
       "   np.float64(0.0009043253786187832),\n",
       "   np.float64(0.0009031735631884387),\n",
       "   np.float64(0.0009020156075466713),\n",
       "   np.float64(0.0009008515295505869),\n",
       "   np.float64(0.0008996813471517051),\n",
       "   np.float64(0.0008985050783956862),\n",
       "   np.float64(0.0008973227414220473),\n",
       "   np.float64(0.0008961343544638855),\n",
       "   np.float64(0.0008949399358475958),\n",
       "   np.float64(0.0008937395039925901),\n",
       "   np.float64(0.0008925330774110096),\n",
       "   np.float64(0.0008913206747074442),\n",
       "   np.float64(0.0008901023145786408),\n",
       "   np.float64(0.0008888780158132183),\n",
       "   np.float64(0.000887647797291377),\n",
       "   np.float64(0.0008864116779846047),\n",
       "   np.float64(0.0008851696769553908),\n",
       "   np.float64(0.0008839218133569254),\n",
       "   np.float64(0.000882668106432807),\n",
       "   np.float64(0.0008814085755167443),\n",
       "   np.float64(0.0008801432400322618),\n",
       "   np.float64(0.0008788721194923961),\n",
       "   np.float64(0.0008775952334993964),\n",
       "   np.float64(0.0008763126017444223),\n",
       "   np.float64(0.0008750242440072405),\n",
       "   np.float64(0.0008737301801559196),\n",
       "   np.float64(0.0008724304301465229),\n",
       "   np.float64(0.0008711250140228016),\n",
       "   np.float64(0.0008698139519158855),\n",
       "   np.float64(0.0008684972640439732),\n",
       "   np.float64(0.0008671749707120175),\n",
       "   np.float64(0.0008658470923114153),\n",
       "   np.float64(0.0008645136493196941),\n",
       "   np.float64(0.0008631746623001917),\n",
       "   np.float64(0.000861830151901741),\n",
       "   np.float64(0.0008604801388583561),\n",
       "   np.float64(0.0008591246439889056),\n",
       "   np.float64(0.0008577636881967948),\n",
       "   np.float64(0.000856397292469644),\n",
       "   np.float64(0.0008550254778789645),\n",
       "   np.float64(0.0008536482655798313),\n",
       "   np.float64(0.0008522656768105603),\n",
       "   np.float64(0.00085087773289238),\n",
       "   np.float64(0.0008494844552290985),\n",
       "   np.float64(0.0008480858653067805),\n",
       "   np.float64(0.000846681984693411),\n",
       "   np.float64(0.0008452728350385627),\n",
       "   np.float64(0.0008438584380730657),\n",
       "   np.float64(0.0008424388156086683),\n",
       "   np.float64(0.0008410139895377042),\n",
       "   np.float64(0.0008395839818327508),\n",
       "   np.float64(0.0008381488145462945),\n",
       "   np.float64(0.0008367085098103888),\n",
       "   np.float64(0.0008352630898363113),\n",
       "   np.float64(0.0008338125769142261),\n",
       "   np.float64(0.0008323569934128324),\n",
       "   np.float64(0.0008308963617790288),\n",
       "   np.float64(0.0008294307045375588),\n",
       "   np.float64(0.0008279600442906681),\n",
       "   np.float64(0.0008264844037177555),\n",
       "   np.float64(0.000825003805575022),\n",
       "   np.float64(0.0008235182726951201),\n",
       "   np.float64(0.0008220278279868036),\n",
       "   np.float64(0.0008205324944345703),\n",
       "   np.float64(0.0008190322950983122),\n",
       "   np.float64(0.0008175272531129568),\n",
       "   np.float64(0.0008160173916881112),\n",
       "   np.float64(0.0008145027341077044),\n",
       "   np.float64(0.0008129833037296279),\n",
       "   np.float64(0.0008114591239853758),\n",
       "   np.float64(0.0008099302183796831),\n",
       "   np.float64(0.0008083966104901632),\n",
       "   np.float64(0.0008068583239669456),\n",
       "   np.float64(0.0008053153825323091),\n",
       "   np.float64(0.0008037678099803167),\n",
       "   np.float64(0.0008022156301764502),\n",
       "   np.float64(0.0008006588670572401),\n",
       "   np.float64(0.0007990975446298969),\n",
       "   np.float64(0.0007975316869719419),\n",
       "   np.float64(0.0007959613182308358),\n",
       "   np.float64(0.0007943864626236041),\n",
       "   np.float64(0.0007928071444364674),\n",
       "   np.float64(0.0007912233880244625),\n",
       "   np.float64(0.0007896352178110711),\n",
       "   np.float64(0.0007880426582878401),\n",
       "   np.float64(0.0007864457340140046),\n",
       "   np.float64(0.0007848444696161127),\n",
       "   np.float64(0.0007832388897876369),\n",
       "   np.float64(0.0007816290192886043),\n",
       "   np.float64(0.000780014882945206),\n",
       "   np.float64(0.0007783965056494199),\n",
       "   np.float64(0.0007767739123586227),\n",
       "   np.float64(0.0007751471280952089),\n",
       "   np.float64(0.0007735161779462014),\n",
       "   np.float64(0.000771881087062868),\n",
       "   np.float64(0.0007702418806603287),\n",
       "   np.float64(0.0007685985840171732),\n",
       "   np.float64(0.0007669512224750654),\n",
       "   np.float64(0.0007652998214383565),\n",
       "   np.float64(0.0007636444063736906),\n",
       "   np.float64(0.0007619850028096138),\n",
       "   np.float64(0.0007603216363361798),\n",
       "   np.float64(0.0007586543326045536),\n",
       "   np.float64(0.0007569831173266203),\n",
       "   np.float64(0.000755308016274585),\n",
       "   np.float64(0.000753629055280574),\n",
       "   np.float64(0.0007519462602362413),\n",
       "   np.float64(0.0007502596570923663),\n",
       "   np.float64(0.0007485692718584529),\n",
       "   np.float64(0.0007468751306023297),\n",
       "   np.float64(0.0007451772594497495),\n",
       "   np.float64(0.0007434756845839817),\n",
       "   np.float64(0.0007417704322454142),\n",
       "   np.float64(0.0007400615287311458),\n",
       "   np.float64(0.0007383490003945786),\n",
       "   np.float64(0.0007366328736450173),\n",
       "   np.float64(0.0007349131749472557),\n",
       "   np.float64(0.0007331899308211727),\n",
       "   np.float64(0.0007314631678413222),\n",
       "   np.float64(0.0007297329126365222),\n",
       "   np.float64(0.0007279991918894468),\n",
       "   np.float64(0.000726262032336212),\n",
       "   np.float64(0.000724521460765964),\n",
       "   np.float64(0.0007227775040204677),\n",
       "   np.float64(0.0007210301889936902),\n",
       "   np.float64(0.0007192795426313885),\n",
       "   np.float64(0.0007175255919306925),\n",
       "   np.float64(0.0007157683639396889),\n",
       "   np.float64(0.0007140078857570045),\n",
       "   np.float64(0.000712244184531389),\n",
       "   np.float64(0.0007104772874612928),\n",
       "   np.float64(0.0007087072217944526),\n",
       "   np.float64(0.0007069340148274683),\n",
       "   np.float64(0.0007051576939053815),\n",
       "   np.float64(0.0007033782864212563),\n",
       "   np.float64(0.0007015958198157544),\n",
       "   np.float64(0.0006998103215767128),\n",
       "   np.float64(0.0006980218192387199),\n",
       "   np.float64(0.000696230340382692),\n",
       "   np.float64(0.0006944359126354467),\n",
       "   np.float64(0.0006926385636692765),\n",
       "   np.float64(0.0006908383212015235),\n",
       "   np.float64(0.0006890352129941503),\n",
       "   np.float64(0.0006872292668533129),\n",
       "   np.float64(0.0006854205106289325),\n",
       "   np.float64(0.0006836089722142643),\n",
       "   np.float64(0.0006817946795454696),\n",
       "   np.float64(0.0006799776606011816),\n",
       "   np.float64(0.000678157943402078),\n",
       "   np.float64(0.000676335556010446),\n",
       "   np.float64(0.0006745105265297509),\n",
       "   np.float64(0.0006726828831042012),\n",
       "   np.float64(0.0006708526539183178),\n",
       "   np.float64(0.000669019867196494),\n",
       "   np.float64(0.0006671845512025669),\n",
       "   np.float64(0.0006653467342393755),\n",
       "   np.float64(0.0006635064446483288),\n",
       "   np.float64(0.0006616637108089645),\n",
       "   np.float64(0.0006598185611385161),\n",
       "   np.float64(0.0006579710240914702),\n",
       "   np.float64(0.0006561211281591302),\n",
       "   np.float64(0.000654268901869177),\n",
       "   np.float64(0.0006524143737852289),\n",
       "   np.float64(0.0006505575725063988),\n",
       "   np.float64(0.0006486985266668566),\n",
       "   np.float64(0.0006468372649353863),\n",
       "   np.float64(0.0006449738160149417),\n",
       "   np.float64(0.0006431082086422081),\n",
       "   np.float64(0.000641240471587156),\n",
       "   np.float64(0.000639370633652597),\n",
       "   np.float64(0.0006374987236737428),\n",
       "   np.float64(0.0006356247705177566),\n",
       "   np.float64(0.0006337488030833115),\n",
       "   np.float64(0.0006318708503001433),\n",
       "   np.float64(0.0006299909411286022),\n",
       "   np.float64(0.0006281091045592112),\n",
       "   np.float64(0.0006262253696122133),\n",
       "   np.float64(0.0006243397653371302),\n",
       "   np.float64(0.0006224523208123065),\n",
       "   np.float64(0.0006205630651444699),\n",
       "   np.float64(0.0006186720274682759),\n",
       "   np.float64(0.0006167792369458612),\n",
       "   np.float64(0.0006148847227663932),\n",
       "   np.float64(0.0006129885141456213),\n",
       "   np.float64(0.0006110906403254246),\n",
       "   np.float64(0.0006091911305733605),\n",
       "   np.float64(0.0006072900141822171),\n",
       "   np.float64(0.0006053873204695553),\n",
       "   np.float64(0.0006034830787772645),\n",
       "   np.float64(0.0006015773184711015),\n",
       "   np.float64(0.0005996700689402452),\n",
       "   np.float64(0.0005977613595968388),\n",
       "   np.float64(0.0005958512198755369),\n",
       "   np.float64(0.0005939396792330542),\n",
       "   np.float64(0.0005920267671477078),\n",
       "   np.float64(0.0005901125131189651),\n",
       "   np.float64(0.0005881969466669862),\n",
       "   np.float64(0.0005862800973321725),\n",
       "   np.float64(0.0005843619946747078),\n",
       "   np.float64(0.0005824426682741043),\n",
       "   np.float64(0.0005805221477287443),\n",
       "   np.float64(0.0005786004626554275),\n",
       "   np.float64(0.000576677642688911),\n",
       "   np.float64(0.0005747537174814519),\n",
       "   np.float64(0.0005728287167023545),\n",
       "   np.float64(0.0005709026700375065),\n",
       "   np.float64(0.0005689756071889273),\n",
       "   np.float64(0.0005670475578743043),\n",
       "   np.float64(0.0005651185518265403),\n",
       "   np.float64(0.0005631886187932892),\n",
       "   np.float64(0.0005612577885365028),\n",
       "   np.float64(0.0005593260908319672),\n",
       "   np.float64(0.0005573935554688462),\n",
       "   np.float64(0.0005554602122492216),\n",
       "   np.float64(0.0005535260909876335),\n",
       "   np.float64(0.0005515912215106195),\n",
       "   np.float64(0.0005496556336562555),\n",
       "   np.float64(0.0005477193572736974),\n",
       "   np.float64(0.0005457824222227166),\n",
       "   np.float64(0.0005438448583732438),\n",
       "   np.float64(0.0005419066956049061),\n",
       "   np.float64(0.0005399679638065655),\n",
       "   np.float64(0.0005380286928758605),\n",
       "   np.float64(0.0005360889127187435),\n",
       "   np.float64(0.0005341486532490187),\n",
       "   np.float64(0.0005322079443878827),\n",
       "   np.float64(0.0005302668160634631),\n",
       "   np.float64(0.0005283252982103537),\n",
       "   np.float64(0.0005263834207691576),\n",
       "   np.float64(0.0005244412136860222),\n",
       "   np.float64(0.000522498706912178),\n",
       "   np.float64(0.0005205559304034778),\n",
       "   np.float64(0.0005186129141199334),\n",
       "   np.float64(0.0005166696880252548),\n",
       "   np.float64(0.0005147262820863873),\n",
       "   np.float64(0.0005127827262730487),\n",
       "   np.float64(0.0005108390505572694),\n",
       "   np.float64(0.000508895284912928),\n",
       "   np.float64(0.0005069514593152903),\n",
       "   np.float64(0.0005050076037405458),\n",
       "   np.float64(0.0005030637481653472),\n",
       "   np.float64(0.0005011199225663465),\n",
       "   np.float64(0.0004991761569197342),\n",
       "   np.float64(0.0004972324812007748),\n",
       "   np.float64(0.0004952889253833479),\n",
       "   np.float64(0.000493345519439482),\n",
       "   np.float64(0.0004914022933388951),\n",
       "   np.float64(0.0004894592770485335),\n",
       "   np.float64(0.0004875165005321045),\n",
       "   np.float64(0.0004855739937496209),\n",
       "   np.float64(0.00048363178665693376),\n",
       "   np.float64(0.0004816899092052737),\n",
       "   np.float64(0.0004797483913407868),\n",
       "   np.float64(0.000477807263004075),\n",
       "   np.float64(0.00047586655412973247),\n",
       "   np.float64(0.00047392629464588414),\n",
       "   np.float64(0.00047198651447372656),\n",
       "   np.float64(0.0004700472435270626),\n",
       "   np.float64(0.00046810851171184327),\n",
       "   np.float64(0.00046617034892570573),\n",
       "   np.float64(0.0004642327850575112),\n",
       "   np.float64(0.00046229584998688463),\n",
       "   np.float64(0.00046035957358375507),\n",
       "   np.float64(0.00045842398570789347),\n",
       "   np.float64(0.0004564891162084525),\n",
       "   np.float64(0.0004545549949235076),\n",
       "   np.float64(0.0004526216516795941),\n",
       "   np.float64(0.00045068911629125055),\n",
       "   np.float64(0.00044875741856055605),\n",
       "   np.float64(0.00044682658827667303),\n",
       "   np.float64(0.0004448966552153854),\n",
       "   np.float64(0.00044296764913864204),\n",
       "   np.float64(0.0004410395997940961),\n",
       "   np.float64(0.0004391125369146463),\n",
       "   np.float64(0.0004371864902179796),\n",
       "   np.float64(0.0004352614894061106),\n",
       "   np.float64(0.00043333756416492723)],\n",
       "  'train_learning_rate_std': [np.float64(2.168404344971009e-19),\n",
       "   np.float64(0.0),\n",
       "   np.float64(0.0),\n",
       "   np.float64(4.336808689942018e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(4.336808689942018e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(0.0),\n",
       "   np.float64(0.0),\n",
       "   np.float64(0.0),\n",
       "   np.float64(0.0),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(0.0),\n",
       "   np.float64(4.336808689942018e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(4.336808689942018e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(4.336808689942018e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(0.0),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(0.0),\n",
       "   np.float64(0.0),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(4.336808689942018e-19),\n",
       "   np.float64(0.0),\n",
       "   np.float64(0.0),\n",
       "   np.float64(4.336808689942018e-19),\n",
       "   np.float64(4.336808689942018e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(4.336808689942018e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(0.0),\n",
       "   np.float64(4.336808689942018e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(0.0),\n",
       "   np.float64(0.0),\n",
       "   np.float64(4.336808689942018e-19),\n",
       "   np.float64(0.0),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(0.0),\n",
       "   np.float64(0.0),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(4.336808689942018e-19),\n",
       "   np.float64(0.0),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(0.0),\n",
       "   np.float64(4.336808689942018e-19),\n",
       "   np.float64(0.0),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(0.0),\n",
       "   np.float64(0.0),\n",
       "   np.float64(3.2526065174565133e-19),\n",
       "   np.float64(4.336808689942018e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(0.0),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(5.421010862427522e-19),\n",
       "   np.float64(3.2526065174565133e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(0.0),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(0.0),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(4.336808689942018e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(4.336808689942018e-19),\n",
       "   np.float64(3.2526065174565133e-19),\n",
       "   np.float64(3.2526065174565133e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(4.336808689942018e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(4.336808689942018e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(0.0),\n",
       "   np.float64(3.2526065174565133e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(3.2526065174565133e-19),\n",
       "   np.float64(3.2526065174565133e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(4.336808689942018e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(0.0),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(3.2526065174565133e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(4.336808689942018e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(0.0),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(3.2526065174565133e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(0.0),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(3.2526065174565133e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(0.0),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(0.0),\n",
       "   np.float64(3.2526065174565133e-19),\n",
       "   np.float64(0.0),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(3.2526065174565133e-19),\n",
       "   np.float64(3.2526065174565133e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(4.336808689942018e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(4.336808689942018e-19),\n",
       "   np.float64(0.0),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(0.0),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(0.0),\n",
       "   np.float64(3.2526065174565133e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(0.0),\n",
       "   np.float64(3.2526065174565133e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(4.336808689942018e-19),\n",
       "   np.float64(0.0),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(3.2526065174565133e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(0.0),\n",
       "   np.float64(0.0),\n",
       "   np.float64(0.0),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(0.0),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(3.2526065174565133e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(3.2526065174565133e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(3.2526065174565133e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(0.0),\n",
       "   np.float64(4.336808689942018e-19),\n",
       "   np.float64(4.336808689942018e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(4.336808689942018e-19),\n",
       "   np.float64(0.0),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(3.2526065174565133e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(3.2526065174565133e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(0.0),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(3.2526065174565133e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(4.336808689942018e-19),\n",
       "   np.float64(4.336808689942018e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(3.2526065174565133e-19),\n",
       "   np.float64(0.0),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(3.2526065174565133e-19),\n",
       "   np.float64(3.2526065174565133e-19),\n",
       "   np.float64(0.0),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(0.0),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(0.0),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(0.0),\n",
       "   np.float64(3.2526065174565133e-19),\n",
       "   np.float64(3.2526065174565133e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(3.2526065174565133e-19),\n",
       "   np.float64(3.2526065174565133e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(0.0),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(3.2526065174565133e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(0.0),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(3.2526065174565133e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(3.2526065174565133e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(3.2526065174565133e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(0.0),\n",
       "   np.float64(3.2526065174565133e-19),\n",
       "   np.float64(3.2526065174565133e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(3.2526065174565133e-19),\n",
       "   np.float64(4.336808689942018e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(0.0),\n",
       "   np.float64(0.0),\n",
       "   np.float64(0.0),\n",
       "   np.float64(0.0),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(0.0),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(3.2526065174565133e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(3.2526065174565133e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(0.0),\n",
       "   np.float64(3.2526065174565133e-19),\n",
       "   np.float64(3.2526065174565133e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(3.2526065174565133e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(0.0),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(0.0),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(0.0),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(0.0),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(0.0),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(0.0),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(0.0),\n",
       "   np.float64(0.0),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(0.0),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(0.0),\n",
       "   np.float64(0.0),\n",
       "   np.float64(0.0),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(0.0),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(0.0),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(0.0),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(0.0),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(0.0),\n",
       "   np.float64(0.0),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(0.0),\n",
       "   np.float64(0.0),\n",
       "   np.float64(0.0),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(0.0),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(0.0),\n",
       "   np.float64(0.0),\n",
       "   np.float64(0.0),\n",
       "   np.float64(0.0),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(0.0),\n",
       "   np.float64(0.0),\n",
       "   np.float64(0.0),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(0.0),\n",
       "   np.float64(0.0),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(0.0),\n",
       "   np.float64(0.0),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(0.0),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(0.0),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(5.421010862427522e-20),\n",
       "   np.float64(5.421010862427522e-20),\n",
       "   np.float64(0.0),\n",
       "   np.float64(1.6263032587282567e-19),\n",
       "   np.float64(0.0),\n",
       "   np.float64(0.0),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(5.421010862427522e-20),\n",
       "   np.float64(5.421010862427522e-20),\n",
       "   np.float64(5.421010862427522e-20),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(5.421010862427522e-20),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(0.0),\n",
       "   np.float64(5.421010862427522e-20),\n",
       "   np.float64(0.0),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(5.421010862427522e-20),\n",
       "   np.float64(0.0),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(5.421010862427522e-20),\n",
       "   np.float64(2.168404344971009e-19),\n",
       "   np.float64(0.0),\n",
       "   np.float64(1.6263032587282567e-19),\n",
       "   np.float64(1.0842021724855044e-19),\n",
       "   np.float64(5.421010862427522e-20),\n",
       "   np.float64(5.421010862427522e-20),\n",
       "   np.float64(5.421010862427522e-20),\n",
       "   np.float64(2.168404344971009e-19)],\n",
       "  'val_total_loss_mean': [np.float64(1.4093983345025143),\n",
       "   np.float64(1.4159231703166757),\n",
       "   np.float64(1.3781803474583572),\n",
       "   np.float64(1.2945195091556716),\n",
       "   np.float64(1.2670156633357554),\n",
       "   np.float64(1.2045736078420166),\n",
       "   np.float64(1.1912130860556487),\n",
       "   np.float64(1.182758123116932),\n",
       "   np.float64(1.192409549759895),\n",
       "   np.float64(1.1790704856623906),\n",
       "   np.float64(1.2263265903029568),\n",
       "   np.float64(1.2314631866989298),\n",
       "   np.float64(1.1658186575877183),\n",
       "   np.float64(1.2089655550852714),\n",
       "   np.float64(1.229461540102395),\n",
       "   np.float64(1.2097153535631457),\n",
       "   np.float64(1.1849835600248435),\n",
       "   np.float64(1.2125429539732606),\n",
       "   np.float64(1.2256886791889843),\n",
       "   np.float64(1.2114015996918095),\n",
       "   np.float64(1.1992246729364433),\n",
       "   np.float64(1.1892329095221101),\n",
       "   np.float64(1.1975703101868755),\n",
       "   np.float64(1.174748525819172),\n",
       "   np.float64(1.2569957615815484),\n",
       "   np.float64(1.2351203452264963),\n",
       "   np.float64(1.2813541511439637),\n",
       "   np.float64(1.2307642900882558),\n",
       "   np.float64(1.1869431206429288),\n",
       "   np.float64(1.158526509076712),\n",
       "   np.float64(1.2277590786545893),\n",
       "   np.float64(1.2015490503378414),\n",
       "   np.float64(1.190725686253757),\n",
       "   np.float64(1.168886856697909),\n",
       "   np.float64(1.179830112713812),\n",
       "   np.float64(1.218027517786656),\n",
       "   np.float64(1.1821899301699532),\n",
       "   np.float64(1.190106740033988),\n",
       "   np.float64(1.2332102750441956),\n",
       "   np.float64(1.2029011777771852),\n",
       "   np.float64(1.2458817229608687),\n",
       "   np.float64(1.2269121933880496),\n",
       "   np.float64(1.2037155236216637),\n",
       "   np.float64(1.2403475324387312),\n",
       "   np.float64(1.1748387036297538),\n",
       "   np.float64(1.1812632851642044),\n",
       "   np.float64(1.1765046874262441),\n",
       "   np.float64(1.2115961469535919),\n",
       "   np.float64(1.1523769892546365),\n",
       "   np.float64(1.188200656974915),\n",
       "   np.float64(1.1603156184543186),\n",
       "   np.float64(1.1715640610637292),\n",
       "   np.float64(1.1799557896527177),\n",
       "   np.float64(1.1692906562525347),\n",
       "   np.float64(1.187061081228562),\n",
       "   np.float64(1.209722836177497),\n",
       "   np.float64(1.2038742195537409),\n",
       "   np.float64(1.2225073512819096),\n",
       "   np.float64(1.2270229818595468),\n",
       "   np.float64(1.1850548599723367),\n",
       "   np.float64(1.1772799869374018),\n",
       "   np.float64(1.1806607641962743),\n",
       "   np.float64(1.2225302588111253),\n",
       "   np.float64(1.152709220662209),\n",
       "   np.float64(1.2058199579394364),\n",
       "   np.float64(1.1992484315295986),\n",
       "   np.float64(1.2328264017638562),\n",
       "   np.float64(1.206104585696752),\n",
       "   np.float64(1.1793471927150239),\n",
       "   np.float64(1.2315936923994821),\n",
       "   np.float64(1.1863723918479345),\n",
       "   np.float64(1.2822035493916473),\n",
       "   np.float64(1.226063626332265),\n",
       "   np.float64(1.2058384608453363),\n",
       "   np.float64(1.239639324814001),\n",
       "   np.float64(1.1918452486761213),\n",
       "   np.float64(1.1908556416018876),\n",
       "   np.float64(1.1758644108612912),\n",
       "   np.float64(1.1869886840509618),\n",
       "   np.float64(1.1693073276102135),\n",
       "   np.float64(1.1922891159160947),\n",
       "   np.float64(1.2088088121955898),\n",
       "   np.float64(1.2303018318451704),\n",
       "   np.float64(1.2092558387861307),\n",
       "   np.float64(1.2426150484558645),\n",
       "   np.float64(1.1871405495632585),\n",
       "   np.float64(1.1843344021139524),\n",
       "   np.float64(1.1975943124816049),\n",
       "   np.float64(1.1796409799495304),\n",
       "   np.float64(1.2311345069270012),\n",
       "   np.float64(1.222471989645854),\n",
       "   np.float64(1.1806922786398963),\n",
       "   np.float64(1.2044166263149583),\n",
       "   np.float64(1.2111933466465015),\n",
       "   np.float64(1.1795740852910823),\n",
       "   np.float64(1.1595977065903464),\n",
       "   np.float64(1.215655418171254),\n",
       "   np.float64(1.2375936491282462),\n",
       "   np.float64(1.2203311504201142),\n",
       "   np.float64(1.2271115816703828),\n",
       "   np.float64(1.181490666460306),\n",
       "   np.float64(1.2348664046762252),\n",
       "   np.float64(1.1349260734742552),\n",
       "   np.float64(1.203686494647668),\n",
       "   np.float64(1.1748904831704459),\n",
       "   np.float64(1.1318558590346677),\n",
       "   np.float64(1.164649262895709),\n",
       "   np.float64(1.1622347289351842),\n",
       "   np.float64(1.148092700817188),\n",
       "   np.float64(1.1593571750587968),\n",
       "   np.float64(1.1751159677577208),\n",
       "   np.float64(1.1655672489939644),\n",
       "   np.float64(1.1713072842438863),\n",
       "   np.float64(1.1639624914568545),\n",
       "   np.float64(1.1404591196231628),\n",
       "   np.float64(1.1600689072566115),\n",
       "   np.float64(1.1357669441169458),\n",
       "   np.float64(1.124501493680683),\n",
       "   np.float64(1.151665682788777),\n",
       "   np.float64(1.1304184171596665),\n",
       "   np.float64(1.1378986942342797),\n",
       "   np.float64(1.1499844590899717),\n",
       "   np.float64(1.1340103310747431),\n",
       "   np.float64(1.1497341124579439),\n",
       "   np.float64(1.1463258755478662),\n",
       "   np.float64(1.1356660333782889),\n",
       "   np.float64(1.1447851653283219),\n",
       "   np.float64(1.1426831745512684),\n",
       "   np.float64(1.1338315877950331),\n",
       "   np.float64(1.1462362790520544),\n",
       "   np.float64(1.1400240645581408),\n",
       "   np.float64(1.1517068083285629),\n",
       "   np.float64(1.1264147591744864),\n",
       "   np.float64(1.1320395719925804),\n",
       "   np.float64(1.1453915924793983),\n",
       "   np.float64(1.1452445110811036),\n",
       "   np.float64(1.147617812400107),\n",
       "   np.float64(1.153358768728424),\n",
       "   np.float64(1.1132130721763926),\n",
       "   np.float64(1.1457438614564064),\n",
       "   np.float64(1.1425361042394764),\n",
       "   np.float64(1.11094384177191),\n",
       "   np.float64(1.1165125900227153),\n",
       "   np.float64(1.1339750339132668),\n",
       "   np.float64(1.1206373745460758),\n",
       "   np.float64(1.1091067652575692),\n",
       "   np.float64(1.116863532160076),\n",
       "   np.float64(1.1285921565652783),\n",
       "   np.float64(1.1197883318654802),\n",
       "   np.float64(1.1275510525736507),\n",
       "   np.float64(1.119929667689752),\n",
       "   np.float64(1.1282011845901936),\n",
       "   np.float64(1.114223309686509),\n",
       "   np.float64(1.1131867779639104),\n",
       "   np.float64(1.1229294382911754),\n",
       "   np.float64(1.1411713482687043),\n",
       "   np.float64(1.116233138226379),\n",
       "   np.float64(1.1167482474408645),\n",
       "   np.float64(1.1382488079314403),\n",
       "   np.float64(1.1101715103070884),\n",
       "   np.float64(1.111683653879279),\n",
       "   np.float64(1.1081032159647477),\n",
       "   np.float64(1.1067539661452095),\n",
       "   np.float64(1.1285476786811581),\n",
       "   np.float64(1.129516299596769),\n",
       "   np.float64(1.1117954978272815),\n",
       "   np.float64(1.1257243128306147),\n",
       "   np.float64(1.126492249538921),\n",
       "   np.float64(1.0992482462529183),\n",
       "   np.float64(1.0987847000315587),\n",
       "   np.float64(1.1206422472131268),\n",
       "   np.float64(1.112058394685463),\n",
       "   np.float64(1.0939800050252386),\n",
       "   np.float64(1.1023947058839445),\n",
       "   np.float64(1.0991540397673416),\n",
       "   np.float64(1.0947070611471341),\n",
       "   np.float64(1.0909390543516113),\n",
       "   np.float64(1.09754427704754),\n",
       "   np.float64(1.087841948599906),\n",
       "   np.float64(1.1148510059079164),\n",
       "   np.float64(1.116731416305026),\n",
       "   np.float64(1.0964693023589471),\n",
       "   np.float64(1.098827256447895),\n",
       "   np.float64(1.0779146641194193),\n",
       "   np.float64(1.0999739148235397),\n",
       "   np.float64(1.080467956931252),\n",
       "   np.float64(1.0715188033478298),\n",
       "   np.float64(1.0868146446705387),\n",
       "   np.float64(1.0755315983324525),\n",
       "   np.float64(1.086716279509135),\n",
       "   np.float64(1.0892477604582727),\n",
       "   np.float64(1.0805855464010985),\n",
       "   np.float64(1.0984809549699572),\n",
       "   np.float64(1.0712122340283754),\n",
       "   np.float64(1.0535174206033806),\n",
       "   np.float64(1.0941165064949043),\n",
       "   np.float64(1.1011415299935496),\n",
       "   np.float64(1.0770226926311073),\n",
       "   np.float64(1.0893187581731603),\n",
       "   np.float64(1.086263438652501),\n",
       "   np.float64(1.0957884409381686),\n",
       "   np.float64(1.0808683901735217),\n",
       "   np.float64(1.0773625092174284),\n",
       "   np.float64(1.0704774723956858),\n",
       "   np.float64(1.0631391496026996),\n",
       "   np.float64(1.0512124078552343),\n",
       "   np.float64(1.072612198585575),\n",
       "   np.float64(1.0548451827204035),\n",
       "   np.float64(1.0795875359993556),\n",
       "   np.float64(1.081622043150578),\n",
       "   np.float64(1.0773088390685441),\n",
       "   np.float64(1.0609597413789997),\n",
       "   np.float64(1.0745259007015726),\n",
       "   np.float64(1.0636102886550782),\n",
       "   np.float64(1.0628532726190305),\n",
       "   np.float64(1.0757221278585787),\n",
       "   np.float64(1.0575507382624223),\n",
       "   np.float64(1.061611232226801),\n",
       "   np.float64(1.0585456650303113),\n",
       "   np.float64(1.0243631766000456),\n",
       "   np.float64(1.0724130638792482),\n",
       "   np.float64(1.0746097238463985),\n",
       "   np.float64(1.0511341714201112),\n",
       "   np.float64(1.0510130140793597),\n",
       "   np.float64(1.0391007040605889),\n",
       "   np.float64(1.0603878453123583),\n",
       "   np.float64(1.0516037606232582),\n",
       "   np.float64(1.0529654395524841),\n",
       "   np.float64(1.0500909972677137),\n",
       "   np.float64(1.0576602626776301),\n",
       "   np.float64(1.0294519553488066),\n",
       "   np.float64(1.0416928867335007),\n",
       "   np.float64(1.0595294744012114),\n",
       "   np.float64(1.0536392518153217),\n",
       "   np.float64(1.0521251332991506),\n",
       "   np.float64(1.050186478966923),\n",
       "   np.float64(1.0573595847023673),\n",
       "   np.float64(1.0344106694996256),\n",
       "   np.float64(1.0473766334397008),\n",
       "   np.float64(1.0711211587742437),\n",
       "   np.float64(1.0514628592295447),\n",
       "   np.float64(1.0566302995783232),\n",
       "   np.float64(1.0394738417223963),\n",
       "   np.float64(1.0462905941240872),\n",
       "   np.float64(1.0420662630330169),\n",
       "   np.float64(1.0363841441744257),\n",
       "   np.float64(1.0376879728726576),\n",
       "   np.float64(1.034733712576185),\n",
       "   np.float64(1.0427670126334834),\n",
       "   np.float64(1.0474312091160876),\n",
       "   np.float64(1.03329169736958),\n",
       "   np.float64(1.041093185329807),\n",
       "   np.float64(1.049779898054806),\n",
       "   np.float64(1.0462347674119377),\n",
       "   np.float64(1.0456717923351633),\n",
       "   np.float64(1.052027073785354),\n",
       "   np.float64(1.0524773817305784),\n",
       "   np.float64(1.0391816368312325),\n",
       "   np.float64(1.0569233518787007),\n",
       "   np.float64(1.0452185094200657),\n",
       "   np.float64(1.0517399074334666),\n",
       "   np.float64(1.0489515289572857),\n",
       "   np.float64(1.0557226727157585),\n",
       "   np.float64(1.0369183047135935),\n",
       "   np.float64(1.0446378190408607),\n",
       "   np.float64(1.0455803665747796),\n",
       "   np.float64(1.0175036262247041),\n",
       "   np.float64(1.0372505842839295),\n",
       "   np.float64(1.0538944007060342),\n",
       "   np.float64(1.0511188778327867),\n",
       "   np.float64(1.0293048041720056),\n",
       "   np.float64(1.04647033764014),\n",
       "   np.float64(1.031111121381949),\n",
       "   np.float64(1.0398652514236082),\n",
       "   np.float64(1.0469128646289736),\n",
       "   np.float64(1.0331555322722836),\n",
       "   np.float64(1.0477965678893368),\n",
       "   np.float64(1.0559665846876527),\n",
       "   np.float64(1.03804022482642),\n",
       "   np.float64(1.0440089798807195),\n",
       "   np.float64(1.035539366927885),\n",
       "   np.float64(1.045399878889239),\n",
       "   np.float64(1.031904056329548),\n",
       "   np.float64(1.0368651918210483),\n",
       "   np.float64(1.0438042406486703),\n",
       "   np.float64(1.0480616542549197),\n",
       "   np.float64(1.0316711064561304),\n",
       "   np.float64(1.036603776746433),\n",
       "   np.float64(1.05124440309946),\n",
       "   np.float64(1.0454931810937578),\n",
       "   np.float64(1.0485977276361478),\n",
       "   np.float64(1.0517927187470868),\n",
       "   np.float64(1.0463593573317094),\n",
       "   np.float64(1.0365529395382767),\n",
       "   np.float64(1.0361690230504341),\n",
       "   np.float64(1.0449046902094015),\n",
       "   np.float64(1.029877919330235),\n",
       "   np.float64(1.0490837595433786),\n",
       "   np.float64(1.0360225758802233),\n",
       "   np.float64(1.0458155258486725),\n",
       "   np.float64(1.044981105340209),\n",
       "   np.float64(1.026180458192619),\n",
       "   np.float64(1.0451445573556977),\n",
       "   np.float64(1.0514811829022004),\n",
       "   np.float64(1.0424939667983821),\n",
       "   np.float64(1.048934970505628),\n",
       "   np.float64(1.037296059859535),\n",
       "   np.float64(1.053191022229008),\n",
       "   np.float64(1.042859049095211),\n",
       "   np.float64(1.0423446459141172),\n",
       "   np.float64(1.0431183015958754),\n",
       "   np.float64(1.038400623323851),\n",
       "   np.float64(1.063309321691308),\n",
       "   np.float64(1.0356811285158483),\n",
       "   np.float64(1.056632343760245),\n",
       "   np.float64(1.0316895318042847),\n",
       "   np.float64(1.0315143290477167),\n",
       "   np.float64(1.0366807754461795),\n",
       "   np.float64(1.047645149223157),\n",
       "   np.float64(1.0492816373345106),\n",
       "   np.float64(1.0331850561945508),\n",
       "   np.float64(1.030659368022551),\n",
       "   np.float64(1.0247123959881987),\n",
       "   np.float64(1.0203554095354888),\n",
       "   np.float64(1.016645509567127),\n",
       "   np.float64(1.019149718365996),\n",
       "   np.float64(1.010309501958272),\n",
       "   np.float64(1.0010473172059924),\n",
       "   np.float64(0.9967184634060993),\n",
       "   np.float64(0.9972276870707782),\n",
       "   np.float64(0.9839120130109299),\n",
       "   np.float64(0.9865203538988498),\n",
       "   np.float64(0.9906003107484197),\n",
       "   np.float64(0.9964342610603453),\n",
       "   np.float64(0.9873643519699878),\n",
       "   np.float64(1.001181210838292),\n",
       "   np.float64(0.9905573304191975),\n",
       "   np.float64(0.9884680197602198),\n",
       "   np.float64(1.0041673761089134),\n",
       "   np.float64(0.9888462184289555),\n",
       "   np.float64(1.003626332908338),\n",
       "   np.float64(1.0011901234993248),\n",
       "   np.float64(1.0111083334626039),\n",
       "   np.float64(0.9890965416520343),\n",
       "   np.float64(0.9946416003044117),\n",
       "   np.float64(0.995136822033267),\n",
       "   np.float64(0.9799211258081564),\n",
       "   np.float64(0.9794390753778157),\n",
       "   np.float64(0.9952972385450646),\n",
       "   np.float64(0.9876469580193525),\n",
       "   np.float64(0.9886439160364422),\n",
       "   np.float64(0.982265254869666),\n",
       "   np.float64(1.0038308253833303),\n",
       "   np.float64(0.9992417902918874),\n",
       "   np.float64(0.9987105959232803),\n",
       "   np.float64(0.9908847248409784),\n",
       "   np.float64(0.9856065587023227),\n",
       "   np.float64(0.977929880407831),\n",
       "   np.float64(0.9943572603068879),\n",
       "   np.float64(0.990243382584747),\n",
       "   np.float64(0.9928715921285671),\n",
       "   np.float64(0.9950959460527102),\n",
       "   np.float64(0.9768974895639208),\n",
       "   np.float64(0.9915812143512653),\n",
       "   np.float64(0.9904110164916198),\n",
       "   np.float64(0.9871668268777785),\n",
       "   np.float64(0.9848113195651662),\n",
       "   np.float64(0.9952327341545995),\n",
       "   np.float64(0.999177908495996),\n",
       "   np.float64(0.985213299748962),\n",
       "   np.float64(0.9846591574831817),\n",
       "   np.float64(0.9922515038223578),\n",
       "   np.float64(0.9837744672768928),\n",
       "   np.float64(0.9841276313596921),\n",
       "   np.float64(0.9937940721186562),\n",
       "   np.float64(0.9881126068681936),\n",
       "   np.float64(0.9886446310676634),\n",
       "   np.float64(1.0036510853409566),\n",
       "   np.float64(0.9955569132272146),\n",
       "   np.float64(0.9898273258597159),\n",
       "   np.float64(0.9961254061930078),\n",
       "   np.float64(0.9817641228907225),\n",
       "   np.float64(0.9899853798270668),\n",
       "   np.float64(0.9964715441591844),\n",
       "   np.float64(0.9915060503081677),\n",
       "   np.float64(0.9913684745403724),\n",
       "   np.float64(0.9865623471727467),\n",
       "   np.float64(0.9880442783593715),\n",
       "   np.float64(0.9872734156001928),\n",
       "   np.float64(0.9933720276804912),\n",
       "   np.float64(0.9849954213394397),\n",
       "   np.float64(0.992418096222185),\n",
       "   np.float64(0.9933997939325543),\n",
       "   np.float64(0.9846953006236265),\n",
       "   np.float64(0.9913980666927156),\n",
       "   np.float64(0.9973215560191426),\n",
       "   np.float64(1.000237546834171),\n",
       "   np.float64(1.0136814027147305),\n",
       "   np.float64(0.9982243690657148),\n",
       "   np.float64(0.9848892911790621),\n",
       "   np.float64(0.9740297684206123),\n",
       "   np.float64(0.9831140142641144),\n",
       "   np.float64(0.9875951662394562),\n",
       "   np.float64(0.978232943439449),\n",
       "   np.float64(0.9826273685143991),\n",
       "   np.float64(0.9921183175855899),\n",
       "   np.float64(1.0131332133681723),\n",
       "   np.float64(0.9971861150605409),\n",
       "   np.float64(0.9812452146190787),\n",
       "   np.float64(0.9847725266228938),\n",
       "   np.float64(0.9986300624860123),\n",
       "   np.float64(0.9895060028261249),\n",
       "   np.float64(0.9825446862344163),\n",
       "   np.float64(0.9841456303254037),\n",
       "   np.float64(0.9846911121868684),\n",
       "   np.float64(0.9873858109391596),\n",
       "   np.float64(0.9865605476511581),\n",
       "   np.float64(1.005467590041837),\n",
       "   np.float64(1.0006110356521973),\n",
       "   np.float64(1.0081285686479675),\n",
       "   np.float64(0.9808764724796701),\n",
       "   np.float64(0.9952759333935465),\n",
       "   np.float64(0.9910975327025169),\n",
       "   np.float64(1.0015357843993078),\n",
       "   np.float64(0.9964243744534933),\n",
       "   np.float64(1.0046757317752142),\n",
       "   np.float64(0.9944958887415815),\n",
       "   np.float64(1.0042510246018679),\n",
       "   np.float64(0.9916326746797536),\n",
       "   np.float64(1.0165865205038807),\n",
       "   np.float64(0.9965169584504994),\n",
       "   np.float64(1.015062861111157),\n",
       "   np.float64(1.0056933068435823),\n",
       "   np.float64(0.9966505809953146),\n",
       "   np.float64(0.993240758435397),\n",
       "   np.float64(0.9958028428840302),\n",
       "   np.float64(1.003043471001681)],\n",
       "  'val_total_loss_std': [np.float64(0.006481673895212329),\n",
       "   np.float64(0.010622774111272045),\n",
       "   np.float64(0.013830268960001075),\n",
       "   np.float64(0.016784358487425568),\n",
       "   np.float64(0.010104799534410444),\n",
       "   np.float64(0.011606822866749156),\n",
       "   np.float64(0.00992976654074016),\n",
       "   np.float64(0.010421847756733982),\n",
       "   np.float64(0.010963578554396692),\n",
       "   np.float64(0.010485784640986632),\n",
       "   np.float64(0.013267331665372493),\n",
       "   np.float64(0.015132087016579514),\n",
       "   np.float64(0.009954584422094915),\n",
       "   np.float64(0.01229037055059912),\n",
       "   np.float64(0.014207529534402574),\n",
       "   np.float64(0.012093833509473413),\n",
       "   np.float64(0.012140933021382335),\n",
       "   np.float64(0.011325223062276128),\n",
       "   np.float64(0.012726692857238882),\n",
       "   np.float64(0.012257957763608338),\n",
       "   np.float64(0.01167039664606188),\n",
       "   np.float64(0.010957412979522713),\n",
       "   np.float64(0.011390785108725623),\n",
       "   np.float64(0.011747905695950083),\n",
       "   np.float64(0.012291273735308849),\n",
       "   np.float64(0.014509758540499134),\n",
       "   np.float64(0.017309148383077833),\n",
       "   np.float64(0.012710041084658822),\n",
       "   np.float64(0.012105070867156694),\n",
       "   np.float64(0.011556299807586589),\n",
       "   np.float64(0.011128712790113385),\n",
       "   np.float64(0.011271907053305696),\n",
       "   np.float64(0.013235020678977921),\n",
       "   np.float64(0.01119068292010306),\n",
       "   np.float64(0.010441760753506413),\n",
       "   np.float64(0.011619994935844244),\n",
       "   np.float64(0.011285022646617264),\n",
       "   np.float64(0.010886794040467848),\n",
       "   np.float64(0.0117514267573207),\n",
       "   np.float64(0.010377494804313792),\n",
       "   np.float64(0.012836216224484134),\n",
       "   np.float64(0.013453580761328887),\n",
       "   np.float64(0.010760625769595479),\n",
       "   np.float64(0.010860570140433621),\n",
       "   np.float64(0.01195954969461804),\n",
       "   np.float64(0.010977570233678787),\n",
       "   np.float64(0.010197742784051328),\n",
       "   np.float64(0.011109165737353814),\n",
       "   np.float64(0.012091835704513311),\n",
       "   np.float64(0.012643617780123859),\n",
       "   np.float64(0.010937449439601515),\n",
       "   np.float64(0.009920668010422339),\n",
       "   np.float64(0.011140096902047316),\n",
       "   np.float64(0.012080998607162882),\n",
       "   np.float64(0.013222020360575817),\n",
       "   np.float64(0.011889764220664442),\n",
       "   np.float64(0.011150481961011462),\n",
       "   np.float64(0.010633436203367411),\n",
       "   np.float64(0.013387388620760814),\n",
       "   np.float64(0.009118187956303123),\n",
       "   np.float64(0.010069128869802109),\n",
       "   np.float64(0.010190539562612396),\n",
       "   np.float64(0.011142561973558673),\n",
       "   np.float64(0.010236921554040181),\n",
       "   np.float64(0.01217860980505157),\n",
       "   np.float64(0.011844407352905078),\n",
       "   np.float64(0.011265689219910004),\n",
       "   np.float64(0.011728227193391297),\n",
       "   np.float64(0.012156316458748168),\n",
       "   np.float64(0.01163493692063513),\n",
       "   np.float64(0.013428687558426054),\n",
       "   np.float64(0.01361917259405008),\n",
       "   np.float64(0.011999901198115137),\n",
       "   np.float64(0.011118783846446243),\n",
       "   np.float64(0.01207515601393623),\n",
       "   np.float64(0.011605980536408557),\n",
       "   np.float64(0.011367964385258179),\n",
       "   np.float64(0.010291385019119303),\n",
       "   np.float64(0.010954216755258444),\n",
       "   np.float64(0.010931441865685022),\n",
       "   np.float64(0.014040266081553383),\n",
       "   np.float64(0.013448122426533378),\n",
       "   np.float64(0.01688045336222238),\n",
       "   np.float64(0.010632851145674492),\n",
       "   np.float64(0.014459064809080475),\n",
       "   np.float64(0.010406827155705007),\n",
       "   np.float64(0.010747817446885096),\n",
       "   np.float64(0.014678422281420255),\n",
       "   np.float64(0.012433271055967216),\n",
       "   np.float64(0.012703544729107518),\n",
       "   np.float64(0.015342809258215434),\n",
       "   np.float64(0.010517467516650275),\n",
       "   np.float64(0.011905801194525478),\n",
       "   np.float64(0.010357917159260648),\n",
       "   np.float64(0.01194700985512437),\n",
       "   np.float64(0.011781928387341009),\n",
       "   np.float64(0.009905967717245287),\n",
       "   np.float64(0.01498886460732179),\n",
       "   np.float64(0.016559350159418236),\n",
       "   np.float64(0.01221397236202197),\n",
       "   np.float64(0.0120805475429281),\n",
       "   np.float64(0.012852129393951714),\n",
       "   np.float64(0.011664168865152306),\n",
       "   np.float64(0.01117860893804906),\n",
       "   np.float64(0.0095809265765015),\n",
       "   np.float64(0.009595905604983494),\n",
       "   np.float64(0.012442235767821047),\n",
       "   np.float64(0.013699955373424815),\n",
       "   np.float64(0.012211229400274816),\n",
       "   np.float64(0.01059922999111138),\n",
       "   np.float64(0.01315125700038344),\n",
       "   np.float64(0.011929026754876157),\n",
       "   np.float64(0.010041229700820537),\n",
       "   np.float64(0.011744336165948019),\n",
       "   np.float64(0.014218966481934067),\n",
       "   np.float64(0.010964937375602161),\n",
       "   np.float64(0.010166339141339083),\n",
       "   np.float64(0.012313725893296096),\n",
       "   np.float64(0.008655309682798535),\n",
       "   np.float64(0.00894494132059156),\n",
       "   np.float64(0.010633381565959032),\n",
       "   np.float64(0.008917955584715337),\n",
       "   np.float64(0.009904814175669527),\n",
       "   np.float64(0.010929004629207095),\n",
       "   np.float64(0.010305036226698299),\n",
       "   np.float64(0.008448474508515772),\n",
       "   np.float64(0.010343191838270586),\n",
       "   np.float64(0.008821204259950116),\n",
       "   np.float64(0.011725445912792709),\n",
       "   np.float64(0.01165498842369156),\n",
       "   np.float64(0.011466245844797173),\n",
       "   np.float64(0.008926524138338623),\n",
       "   np.float64(0.01272748671278146),\n",
       "   np.float64(0.010837821502574708),\n",
       "   np.float64(0.011084963812199287),\n",
       "   np.float64(0.010422541380137837),\n",
       "   np.float64(0.013794240374477693),\n",
       "   np.float64(0.011613693171287326),\n",
       "   np.float64(0.013207553250843853),\n",
       "   np.float64(0.01332673613186884),\n",
       "   np.float64(0.012236851086344324),\n",
       "   np.float64(0.01250708817895977),\n",
       "   np.float64(0.011975437614811001),\n",
       "   np.float64(0.01158174736991463),\n",
       "   np.float64(0.010430579243584294),\n",
       "   np.float64(0.012258650243108508),\n",
       "   np.float64(0.011592384567998243),\n",
       "   np.float64(0.009798705960924095),\n",
       "   np.float64(0.011795384988530801),\n",
       "   np.float64(0.011103765879202416),\n",
       "   np.float64(0.011093596443532664),\n",
       "   np.float64(0.010270269421812408),\n",
       "   np.float64(0.010164092551507853),\n",
       "   np.float64(0.01227564661598969),\n",
       "   np.float64(0.008543184856026731),\n",
       "   np.float64(0.01146970374440029),\n",
       "   np.float64(0.012598759185164347),\n",
       "   np.float64(0.010046015098628272),\n",
       "   np.float64(0.010138027215165603),\n",
       "   np.float64(0.009183916175105352),\n",
       "   np.float64(0.009482085408290417),\n",
       "   np.float64(0.009851648068398838),\n",
       "   np.float64(0.010498591596181264),\n",
       "   np.float64(0.011154578150246888),\n",
       "   np.float64(0.009994419777202806),\n",
       "   np.float64(0.010685152115046838),\n",
       "   np.float64(0.009365783437438359),\n",
       "   np.float64(0.009965036386515688),\n",
       "   np.float64(0.011768635639540862),\n",
       "   np.float64(0.01039248359818153),\n",
       "   np.float64(0.010681524288950384),\n",
       "   np.float64(0.012942407107131806),\n",
       "   np.float64(0.011197169544424089),\n",
       "   np.float64(0.012233885419666683),\n",
       "   np.float64(0.009335898856633062),\n",
       "   np.float64(0.010772772461766554),\n",
       "   np.float64(0.009849854908462807),\n",
       "   np.float64(0.01057741441208963),\n",
       "   np.float64(0.012413526417699975),\n",
       "   np.float64(0.01010785180809241),\n",
       "   np.float64(0.01052519091701092),\n",
       "   np.float64(0.011418667637807448),\n",
       "   np.float64(0.011932860274130793),\n",
       "   np.float64(0.010082664037682192),\n",
       "   np.float64(0.010868542959842659),\n",
       "   np.float64(0.012102345213349054),\n",
       "   np.float64(0.011299676306562903),\n",
       "   np.float64(0.011095409175824005),\n",
       "   np.float64(0.012016736077516245),\n",
       "   np.float64(0.010992373884972768),\n",
       "   np.float64(0.011369646590964504),\n",
       "   np.float64(0.011209620151653528),\n",
       "   np.float64(0.010568271725112897),\n",
       "   np.float64(0.010541392580888424),\n",
       "   np.float64(0.01085039235196221),\n",
       "   np.float64(0.010054247907045314),\n",
       "   np.float64(0.010448629009624828),\n",
       "   np.float64(0.00871101776907825),\n",
       "   np.float64(0.009126655441163844),\n",
       "   np.float64(0.009989960182145327),\n",
       "   np.float64(0.011922151040735586),\n",
       "   np.float64(0.010082443194638092),\n",
       "   np.float64(0.009167181471563469),\n",
       "   np.float64(0.00990165334663159),\n",
       "   np.float64(0.010849327517851004),\n",
       "   np.float64(0.00936102862669156),\n",
       "   np.float64(0.012161833356919834),\n",
       "   np.float64(0.009621745303608214),\n",
       "   np.float64(0.01151898450411678),\n",
       "   np.float64(0.012335957168279585),\n",
       "   np.float64(0.010478962079566145),\n",
       "   np.float64(0.010556165431369394),\n",
       "   np.float64(0.010536952683400073),\n",
       "   np.float64(0.010468769412177318),\n",
       "   np.float64(0.01020935832422543),\n",
       "   np.float64(0.012668310927168306),\n",
       "   np.float64(0.01134189665504981),\n",
       "   np.float64(0.010452920881285903),\n",
       "   np.float64(0.012086075956440978),\n",
       "   np.float64(0.012486793773365317),\n",
       "   np.float64(0.009828944507785885),\n",
       "   np.float64(0.011236759889677218),\n",
       "   np.float64(0.01095876525275718),\n",
       "   np.float64(0.011033434349041643),\n",
       "   np.float64(0.011563561260060955),\n",
       "   np.float64(0.013016742248937728),\n",
       "   np.float64(0.011142472408502037),\n",
       "   np.float64(0.011797260811378996),\n",
       "   np.float64(0.010734116851376298),\n",
       "   np.float64(0.010864827961445483),\n",
       "   np.float64(0.01018520106226909),\n",
       "   np.float64(0.011474987666451643),\n",
       "   np.float64(0.009838535039151292),\n",
       "   np.float64(0.008671731146651632),\n",
       "   np.float64(0.012568760400372532),\n",
       "   np.float64(0.012044812621983553),\n",
       "   np.float64(0.01231927342340014),\n",
       "   np.float64(0.009302735321299752),\n",
       "   np.float64(0.010629739661086244),\n",
       "   np.float64(0.011951705166883406),\n",
       "   np.float64(0.011964588823118983),\n",
       "   np.float64(0.012125354272607499),\n",
       "   np.float64(0.01045347097478727),\n",
       "   np.float64(0.011774050382397344),\n",
       "   np.float64(0.014177582622217128),\n",
       "   np.float64(0.009614010423538794),\n",
       "   np.float64(0.012140116349037125),\n",
       "   np.float64(0.01311496004465862),\n",
       "   np.float64(0.012490741212997704),\n",
       "   np.float64(0.013051827654522705),\n",
       "   np.float64(0.010949314458875425),\n",
       "   np.float64(0.011946527745445497),\n",
       "   np.float64(0.012301438264096194),\n",
       "   np.float64(0.009454082940803963),\n",
       "   np.float64(0.0120262731152102),\n",
       "   np.float64(0.013258824879740753),\n",
       "   np.float64(0.011412125252603026),\n",
       "   np.float64(0.014418278240514058),\n",
       "   np.float64(0.014404945589043042),\n",
       "   np.float64(0.011345689006111744),\n",
       "   np.float64(0.010881958195093806),\n",
       "   np.float64(0.012538875926828277),\n",
       "   np.float64(0.012368809054428262),\n",
       "   np.float64(0.014195586969129974),\n",
       "   np.float64(0.010688374770615534),\n",
       "   np.float64(0.014660069919013176),\n",
       "   np.float64(0.012236044677070816),\n",
       "   np.float64(0.01323119860699611),\n",
       "   np.float64(0.012814558403104807),\n",
       "   np.float64(0.010436176016152455),\n",
       "   np.float64(0.01191773313305093),\n",
       "   np.float64(0.012797712355621034),\n",
       "   np.float64(0.011755624971544861),\n",
       "   np.float64(0.012597188054888053),\n",
       "   np.float64(0.014222483070441177),\n",
       "   np.float64(0.013109819595297913),\n",
       "   np.float64(0.012583129647860975),\n",
       "   np.float64(0.010347843709898472),\n",
       "   np.float64(0.013570459546983243),\n",
       "   np.float64(0.011089963470073173),\n",
       "   np.float64(0.01635851870054689),\n",
       "   np.float64(0.009810686738144375),\n",
       "   np.float64(0.012182764680276058),\n",
       "   np.float64(0.01330104239628402),\n",
       "   np.float64(0.012021236582845969),\n",
       "   np.float64(0.014456722687701252),\n",
       "   np.float64(0.013937789565726085),\n",
       "   np.float64(0.013564975000929598),\n",
       "   np.float64(0.011817824559664264),\n",
       "   np.float64(0.011849421666298597),\n",
       "   np.float64(0.012400263359720822),\n",
       "   np.float64(0.013931553145971522),\n",
       "   np.float64(0.013264111571963591),\n",
       "   np.float64(0.01486268717403734),\n",
       "   np.float64(0.0140940417702144),\n",
       "   np.float64(0.012712525387868566),\n",
       "   np.float64(0.013341183651018761),\n",
       "   np.float64(0.01545812246347009),\n",
       "   np.float64(0.013135947002960719),\n",
       "   np.float64(0.010667399267291506),\n",
       "   np.float64(0.012379578871053386),\n",
       "   np.float64(0.01340203362708281),\n",
       "   np.float64(0.015573190606154011),\n",
       "   np.float64(0.012628857827133707),\n",
       "   np.float64(0.013695065112004783),\n",
       "   np.float64(0.015393342738225192),\n",
       "   np.float64(0.014088118997701944),\n",
       "   np.float64(0.012394754657184011),\n",
       "   np.float64(0.014385951082048652),\n",
       "   np.float64(0.011766993215362245),\n",
       "   np.float64(0.013759574009609313),\n",
       "   np.float64(0.012861504537208501),\n",
       "   np.float64(0.012757168465763837),\n",
       "   np.float64(0.013022817147258134),\n",
       "   np.float64(0.013120199819588722),\n",
       "   np.float64(0.0144862804816708),\n",
       "   np.float64(0.012297242352682857),\n",
       "   np.float64(0.015569683530584107),\n",
       "   np.float64(0.01276787461540631),\n",
       "   np.float64(0.014587499518753563),\n",
       "   np.float64(0.012792266208797653),\n",
       "   np.float64(0.011526420219191856),\n",
       "   np.float64(0.014418302326098918),\n",
       "   np.float64(0.011942184604273771),\n",
       "   np.float64(0.014035823338444057),\n",
       "   np.float64(0.021427113339350372),\n",
       "   np.float64(0.017269718003981065),\n",
       "   np.float64(0.017908263231835798),\n",
       "   np.float64(0.013743747954023248),\n",
       "   np.float64(0.014490277328100019),\n",
       "   np.float64(0.014895800402348097),\n",
       "   np.float64(0.01464025894641643),\n",
       "   np.float64(0.013842623852539792),\n",
       "   np.float64(0.018024999735053216),\n",
       "   np.float64(0.015618180133164792),\n",
       "   np.float64(0.01673306165825066),\n",
       "   np.float64(0.012134054793984874),\n",
       "   np.float64(0.015151002809678928),\n",
       "   np.float64(0.013993062999042072),\n",
       "   np.float64(0.017110691673969846),\n",
       "   np.float64(0.013265234422944533),\n",
       "   np.float64(0.01537512126117221),\n",
       "   np.float64(0.014947221958964032),\n",
       "   np.float64(0.014553206817448146),\n",
       "   np.float64(0.014446692265752927),\n",
       "   np.float64(0.015137738089696094),\n",
       "   np.float64(0.013579041154632428),\n",
       "   np.float64(0.01527277767438156),\n",
       "   np.float64(0.01604615704248874),\n",
       "   np.float64(0.01731378325633736),\n",
       "   np.float64(0.018474673171007136),\n",
       "   np.float64(0.015981387603251743),\n",
       "   np.float64(0.014757668495099765),\n",
       "   np.float64(0.01455416304795016),\n",
       "   np.float64(0.016386461696498343),\n",
       "   np.float64(0.014091649061311315),\n",
       "   np.float64(0.01620894930546336),\n",
       "   np.float64(0.021286589720031834),\n",
       "   np.float64(0.015882463907329),\n",
       "   np.float64(0.013086748660593546),\n",
       "   np.float64(0.015287505741590313),\n",
       "   np.float64(0.01567582202051651),\n",
       "   np.float64(0.019422571329806026),\n",
       "   np.float64(0.013904261669735482),\n",
       "   np.float64(0.0158460693365995),\n",
       "   np.float64(0.01947216128049399),\n",
       "   np.float64(0.013904274024622887),\n",
       "   np.float64(0.01545304032931992),\n",
       "   np.float64(0.017010297941551195),\n",
       "   np.float64(0.01130592683165595),\n",
       "   np.float64(0.015365506057583859),\n",
       "   np.float64(0.01502008730966768),\n",
       "   np.float64(0.016942488022263613),\n",
       "   np.float64(0.017041252950939462),\n",
       "   np.float64(0.01771347430541311),\n",
       "   np.float64(0.01606589226261272),\n",
       "   np.float64(0.017423070786592474),\n",
       "   np.float64(0.01748611785684471),\n",
       "   np.float64(0.01689187087163831),\n",
       "   np.float64(0.015077486724917398),\n",
       "   np.float64(0.01753496302040381),\n",
       "   np.float64(0.014553318668697695),\n",
       "   np.float64(0.016503649190618874),\n",
       "   np.float64(0.019445652212203388),\n",
       "   np.float64(0.012569121457636171),\n",
       "   np.float64(0.015651171579197907),\n",
       "   np.float64(0.017444055924422806),\n",
       "   np.float64(0.0151750557126751),\n",
       "   np.float64(0.016813402610061964),\n",
       "   np.float64(0.016378009848568718),\n",
       "   np.float64(0.015941337508095565),\n",
       "   np.float64(0.018374804444087612),\n",
       "   np.float64(0.01660250814902336),\n",
       "   np.float64(0.016058878098316947),\n",
       "   np.float64(0.017837696383793584),\n",
       "   np.float64(0.012405338874324531),\n",
       "   np.float64(0.01688814479572546),\n",
       "   np.float64(0.018221139300811266),\n",
       "   np.float64(0.02029576530656265),\n",
       "   np.float64(0.017746905558438823),\n",
       "   np.float64(0.016988438259439152),\n",
       "   np.float64(0.016020782235489833),\n",
       "   np.float64(0.016572258243652944),\n",
       "   np.float64(0.018380588067624656),\n",
       "   np.float64(0.014223203808416345),\n",
       "   np.float64(0.014220810656814456),\n",
       "   np.float64(0.020971047478630548),\n",
       "   np.float64(0.017311829114743688),\n",
       "   np.float64(0.018520104333269997),\n",
       "   np.float64(0.015097226905078923),\n",
       "   np.float64(0.01787238747723119),\n",
       "   np.float64(0.015532476856579292),\n",
       "   np.float64(0.016716780677662708),\n",
       "   np.float64(0.015012937214653741),\n",
       "   np.float64(0.015803431547596784),\n",
       "   np.float64(0.014851667110158446),\n",
       "   np.float64(0.016511384880667966),\n",
       "   np.float64(0.017113770700657862),\n",
       "   np.float64(0.01063332497276079),\n",
       "   np.float64(0.016505499880578357),\n",
       "   np.float64(0.015781219735697793),\n",
       "   np.float64(0.01729221811874275),\n",
       "   np.float64(0.013692311230797104),\n",
       "   np.float64(0.01645260283630196),\n",
       "   np.float64(0.015163949370883136),\n",
       "   np.float64(0.017812144942042776),\n",
       "   np.float64(0.017516678258265715),\n",
       "   np.float64(0.016121236000916393),\n",
       "   np.float64(0.01785172411581084),\n",
       "   np.float64(0.018176534216113788),\n",
       "   np.float64(0.013022323304153597),\n",
       "   np.float64(0.01684511160156981),\n",
       "   np.float64(0.0166715114668968),\n",
       "   np.float64(0.016337301748211103),\n",
       "   np.float64(0.016653793747183964),\n",
       "   np.float64(0.016569916473025963),\n",
       "   np.float64(0.01852206915855743)],\n",
       "  'val_mdn_loss_mean': [np.float64(1.4093201607465744),\n",
       "   np.float64(1.4158604815602303),\n",
       "   np.float64(1.3781334459781647),\n",
       "   np.float64(1.2944805100560188),\n",
       "   np.float64(1.266982302069664),\n",
       "   np.float64(1.204543799161911),\n",
       "   np.float64(1.1911872178316116),\n",
       "   np.float64(1.1827334687113762),\n",
       "   np.float64(1.1923871263861656),\n",
       "   np.float64(1.179050251841545),\n",
       "   np.float64(1.2263070568442345),\n",
       "   np.float64(1.2314450442790985),\n",
       "   np.float64(1.1658019050955772),\n",
       "   np.float64(1.208948791027069),\n",
       "   np.float64(1.2294455245137215),\n",
       "   np.float64(1.2097005769610405),\n",
       "   np.float64(1.1849682927131653),\n",
       "   np.float64(1.212527796626091),\n",
       "   np.float64(1.225673958659172),\n",
       "   np.float64(1.2113872393965721),\n",
       "   np.float64(1.1992117762565613),\n",
       "   np.float64(1.1892199590802193),\n",
       "   np.float64(1.197556808590889),\n",
       "   np.float64(1.1747356876730919),\n",
       "   np.float64(1.2569827511906624),\n",
       "   np.float64(1.2351080551743507),\n",
       "   np.float64(1.2813408747315407),\n",
       "   np.float64(1.2307517752051353),\n",
       "   np.float64(1.1869302839040756),\n",
       "   np.float64(1.1585145741701126),\n",
       "   np.float64(1.2277475073933601),\n",
       "   np.float64(1.2015371322631836),\n",
       "   np.float64(1.1907139867544174),\n",
       "   np.float64(1.1688749566674232),\n",
       "   np.float64(1.1798176914453506),\n",
       "   np.float64(1.2180152907967567),\n",
       "   np.float64(1.1821777299046516),\n",
       "   np.float64(1.1900948360562325),\n",
       "   np.float64(1.233198195695877),\n",
       "   np.float64(1.202888511121273),\n",
       "   np.float64(1.245870091021061),\n",
       "   np.float64(1.2269005551934242),\n",
       "   np.float64(1.2037028297781944),\n",
       "   np.float64(1.2403350248932838),\n",
       "   np.float64(1.1748264878988266),\n",
       "   np.float64(1.1812504976987839),\n",
       "   np.float64(1.176491230726242),\n",
       "   np.float64(1.211583562195301),\n",
       "   np.float64(1.152363620698452),\n",
       "   np.float64(1.1881884187459946),\n",
       "   np.float64(1.1603036671876907),\n",
       "   np.float64(1.1715524792671204),\n",
       "   np.float64(1.1799422800540924),\n",
       "   np.float64(1.1692789271473885),\n",
       "   np.float64(1.187048465013504),\n",
       "   np.float64(1.2097107842564583),\n",
       "   np.float64(1.2038616016507149),\n",
       "   np.float64(1.2224944233894348),\n",
       "   np.float64(1.2270110547542572),\n",
       "   np.float64(1.1850426346063614),\n",
       "   np.float64(1.177268847823143),\n",
       "   np.float64(1.1806490868330002),\n",
       "   np.float64(1.2225187718868256),\n",
       "   np.float64(1.1526978090405464),\n",
       "   np.float64(1.205808199942112),\n",
       "   np.float64(1.199236623942852),\n",
       "   np.float64(1.2328149899840355),\n",
       "   np.float64(1.2060933634638786),\n",
       "   np.float64(1.1793357878923416),\n",
       "   np.float64(1.2315816804766655),\n",
       "   np.float64(1.1863607093691826),\n",
       "   np.float64(1.2821923270821571),\n",
       "   np.float64(1.2260528057813644),\n",
       "   np.float64(1.205826923251152),\n",
       "   np.float64(1.2396279498934746),\n",
       "   np.float64(1.1918341666460037),\n",
       "   np.float64(1.1908442378044128),\n",
       "   np.float64(1.1758532375097275),\n",
       "   np.float64(1.1869776993989944),\n",
       "   np.float64(1.169295847415924),\n",
       "   np.float64(1.1922774836421013),\n",
       "   np.float64(1.2087978944182396),\n",
       "   np.float64(1.230291061103344),\n",
       "   np.float64(1.209245152771473),\n",
       "   np.float64(1.2426045760512352),\n",
       "   np.float64(1.1871298849582672),\n",
       "   np.float64(1.1843236684799194),\n",
       "   np.float64(1.1975838169455528),\n",
       "   np.float64(1.1796306297183037),\n",
       "   np.float64(1.231123887002468),\n",
       "   np.float64(1.2224613353610039),\n",
       "   np.float64(1.1806815937161446),\n",
       "   np.float64(1.2044059559702873),\n",
       "   np.float64(1.2111828699707985),\n",
       "   np.float64(1.1795638054609299),\n",
       "   np.float64(1.159586824476719),\n",
       "   np.float64(1.2156450748443604),\n",
       "   np.float64(1.237582579255104),\n",
       "   np.float64(1.220320075750351),\n",
       "   np.float64(1.2271000444889069),\n",
       "   np.float64(1.1814793944358826),\n",
       "   np.float64(1.2348551601171494),\n",
       "   np.float64(1.1349147036671638),\n",
       "   np.float64(1.2036752998828888),\n",
       "   np.float64(1.1748794913291931),\n",
       "   np.float64(1.1318443417549133),\n",
       "   np.float64(1.164637677371502),\n",
       "   np.float64(1.1622237116098404),\n",
       "   np.float64(1.1480810791254044),\n",
       "   np.float64(1.159345418214798),\n",
       "   np.float64(1.175105020403862),\n",
       "   np.float64(1.1655564904212952),\n",
       "   np.float64(1.1712959185242653),\n",
       "   np.float64(1.1639514788985252),\n",
       "   np.float64(1.1404479295015335),\n",
       "   np.float64(1.1600577980279922),\n",
       "   np.float64(1.1357559710741043),\n",
       "   np.float64(1.1244899705052376),\n",
       "   np.float64(1.1516545489430428),\n",
       "   np.float64(1.1304073706269264),\n",
       "   np.float64(1.1378885582089424),\n",
       "   np.float64(1.149973802268505),\n",
       "   np.float64(1.1339995488524437),\n",
       "   np.float64(1.1497232019901276),\n",
       "   np.float64(1.146314151585102),\n",
       "   np.float64(1.1356550082564354),\n",
       "   np.float64(1.1447736844420433),\n",
       "   np.float64(1.1426712572574615),\n",
       "   np.float64(1.1338208466768265),\n",
       "   np.float64(1.146225020289421),\n",
       "   np.float64(1.1400127857923508),\n",
       "   np.float64(1.1516961678862572),\n",
       "   np.float64(1.126403659582138),\n",
       "   np.float64(1.1320288926362991),\n",
       "   np.float64(1.1453805044293404),\n",
       "   np.float64(1.1452330201864243),\n",
       "   np.float64(1.1476066559553146),\n",
       "   np.float64(1.153347223997116),\n",
       "   np.float64(1.113201729953289),\n",
       "   np.float64(1.1457325518131256),\n",
       "   np.float64(1.1425243392586708),\n",
       "   np.float64(1.1109315752983093),\n",
       "   np.float64(1.1165009513497353),\n",
       "   np.float64(1.133963204920292),\n",
       "   np.float64(1.1206251978874207),\n",
       "   np.float64(1.109095737338066),\n",
       "   np.float64(1.1168520078063011),\n",
       "   np.float64(1.1285807639360428),\n",
       "   np.float64(1.1197769567370415),\n",
       "   np.float64(1.1275400146842003),\n",
       "   np.float64(1.1199184581637383),\n",
       "   np.float64(1.128189392387867),\n",
       "   np.float64(1.1142118349671364),\n",
       "   np.float64(1.1131756529211998),\n",
       "   np.float64(1.122918277978897),\n",
       "   np.float64(1.141160249710083),\n",
       "   np.float64(1.1162216439843178),\n",
       "   np.float64(1.1167365312576294),\n",
       "   np.float64(1.1382378041744232),\n",
       "   np.float64(1.1101602464914322),\n",
       "   np.float64(1.111673153936863),\n",
       "   np.float64(1.1080922335386276),\n",
       "   np.float64(1.106743298470974),\n",
       "   np.float64(1.1285369768738747),\n",
       "   np.float64(1.1295050531625748),\n",
       "   np.float64(1.1117849200963974),\n",
       "   np.float64(1.1257122680544853),\n",
       "   np.float64(1.126480557024479),\n",
       "   np.float64(1.099236749112606),\n",
       "   np.float64(1.0987723022699356),\n",
       "   np.float64(1.1206297352910042),\n",
       "   np.float64(1.1120468527078629),\n",
       "   np.float64(1.093968704342842),\n",
       "   np.float64(1.1023832336068153),\n",
       "   np.float64(1.0991425067186356),\n",
       "   np.float64(1.094695769250393),\n",
       "   np.float64(1.0909274369478226),\n",
       "   np.float64(1.0975326225161552),\n",
       "   np.float64(1.087830625474453),\n",
       "   np.float64(1.1148399487137794),\n",
       "   np.float64(1.1167199164628983),\n",
       "   np.float64(1.0964573472738266),\n",
       "   np.float64(1.098816379904747),\n",
       "   np.float64(1.0779033228754997),\n",
       "   np.float64(1.0999622195959091),\n",
       "   np.float64(1.0804563611745834),\n",
       "   np.float64(1.0715078860521317),\n",
       "   np.float64(1.0868035554885864),\n",
       "   np.float64(1.0755206122994423),\n",
       "   np.float64(1.0867051184177399),\n",
       "   np.float64(1.0892356261610985),\n",
       "   np.float64(1.0805740803480148),\n",
       "   np.float64(1.09846980124712),\n",
       "   np.float64(1.0712007656693459),\n",
       "   np.float64(1.0535061731934547),\n",
       "   np.float64(1.0941058546304703),\n",
       "   np.float64(1.1011311262845993),\n",
       "   np.float64(1.0770121589303017),\n",
       "   np.float64(1.0893071666359901),\n",
       "   np.float64(1.0862525030970573),\n",
       "   np.float64(1.0957779064774513),\n",
       "   np.float64(1.080857329070568),\n",
       "   np.float64(1.077351488173008),\n",
       "   np.float64(1.0704665705561638),\n",
       "   np.float64(1.063128039240837),\n",
       "   np.float64(1.0512015670537949),\n",
       "   np.float64(1.0726007372140884),\n",
       "   np.float64(1.0548347905278206),\n",
       "   np.float64(1.0795766338706017),\n",
       "   np.float64(1.0816110000014305),\n",
       "   np.float64(1.0772975087165833),\n",
       "   np.float64(1.0609489679336548),\n",
       "   np.float64(1.0745147317647934),\n",
       "   np.float64(1.063599057495594),\n",
       "   np.float64(1.0628416538238525),\n",
       "   np.float64(1.075711190700531),\n",
       "   np.float64(1.057539515197277),\n",
       "   np.float64(1.06160007417202),\n",
       "   np.float64(1.0585343167185783),\n",
       "   np.float64(1.0243518576025963),\n",
       "   np.float64(1.0724017396569252),\n",
       "   np.float64(1.0745978727936745),\n",
       "   np.float64(1.0511226207017899),\n",
       "   np.float64(1.0510014072060585),\n",
       "   np.float64(1.039089560508728),\n",
       "   np.float64(1.06037687510252),\n",
       "   np.float64(1.051592856645584),\n",
       "   np.float64(1.0529545992612839),\n",
       "   np.float64(1.0500795170664787),\n",
       "   np.float64(1.0576495602726936),\n",
       "   np.float64(1.0294411554932594),\n",
       "   np.float64(1.0416820347309113),\n",
       "   np.float64(1.0595181956887245),\n",
       "   np.float64(1.0536285489797592),\n",
       "   np.float64(1.052113763988018),\n",
       "   np.float64(1.0501756146550179),\n",
       "   np.float64(1.0573486387729645),\n",
       "   np.float64(1.0344000905752182),\n",
       "   np.float64(1.0473666116595268),\n",
       "   np.float64(1.0711103156208992),\n",
       "   np.float64(1.0514521673321724),\n",
       "   np.float64(1.0566187277436256),\n",
       "   np.float64(1.0394632294774055),\n",
       "   np.float64(1.0462799668312073),\n",
       "   np.float64(1.042055495083332),\n",
       "   np.float64(1.036373883485794),\n",
       "   np.float64(1.0376770347356796),\n",
       "   np.float64(1.034723088145256),\n",
       "   np.float64(1.042756237089634),\n",
       "   np.float64(1.047420583665371),\n",
       "   np.float64(1.0332810208201408),\n",
       "   np.float64(1.0410823673009872),\n",
       "   np.float64(1.0497692972421646),\n",
       "   np.float64(1.046223446726799),\n",
       "   np.float64(1.0456608980894089),\n",
       "   np.float64(1.0520164966583252),\n",
       "   np.float64(1.0524666756391525),\n",
       "   np.float64(1.0391707047820091),\n",
       "   np.float64(1.0569127649068832),\n",
       "   np.float64(1.0452075228095055),\n",
       "   np.float64(1.0517289564013481),\n",
       "   np.float64(1.0489405542612076),\n",
       "   np.float64(1.0557122007012367),\n",
       "   np.float64(1.036907635629177),\n",
       "   np.float64(1.0446272864937782),\n",
       "   np.float64(1.045570194721222),\n",
       "   np.float64(1.017493225634098),\n",
       "   np.float64(1.0372402966022491),\n",
       "   np.float64(1.0538835003972054),\n",
       "   np.float64(1.0511083826422691),\n",
       "   np.float64(1.0292945802211761),\n",
       "   np.float64(1.0464599281549454),\n",
       "   np.float64(1.0311002284288406),\n",
       "   np.float64(1.0398547798395157),\n",
       "   np.float64(1.0469019338488579),\n",
       "   np.float64(1.0331450626254082),\n",
       "   np.float64(1.0477862507104874),\n",
       "   np.float64(1.055956169962883),\n",
       "   np.float64(1.0380294397473335),\n",
       "   np.float64(1.0439979955554008),\n",
       "   np.float64(1.0355285853147507),\n",
       "   np.float64(1.0453893765807152),\n",
       "   np.float64(1.0318934544920921),\n",
       "   np.float64(1.0368543714284897),\n",
       "   np.float64(1.043793722987175),\n",
       "   np.float64(1.0480510741472244),\n",
       "   np.float64(1.0316607877612114),\n",
       "   np.float64(1.0365933924913406),\n",
       "   np.float64(1.0512344017624855),\n",
       "   np.float64(1.0454830005764961),\n",
       "   np.float64(1.048587180674076),\n",
       "   np.float64(1.0517827346920967),\n",
       "   np.float64(1.0463491156697273),\n",
       "   np.float64(1.036541923880577),\n",
       "   np.float64(1.0361586213111877),\n",
       "   np.float64(1.0448943823575974),\n",
       "   np.float64(1.0298675000667572),\n",
       "   np.float64(1.0490732640028),\n",
       "   np.float64(1.0360121130943298),\n",
       "   np.float64(1.045804537832737),\n",
       "   np.float64(1.0449707433581352),\n",
       "   np.float64(1.0261703059077263),\n",
       "   np.float64(1.045134149491787),\n",
       "   np.float64(1.0514711886644363),\n",
       "   np.float64(1.0424834042787552),\n",
       "   np.float64(1.0489242300391197),\n",
       "   np.float64(1.03728499263525),\n",
       "   np.float64(1.0531804040074348),\n",
       "   np.float64(1.0428488925099373),\n",
       "   np.float64(1.0423344150185585),\n",
       "   np.float64(1.0431079268455505),\n",
       "   np.float64(1.0383901223540306),\n",
       "   np.float64(1.0632989555597305),\n",
       "   np.float64(1.0356706604361534),\n",
       "   np.float64(1.056622289121151),\n",
       "   np.float64(1.031679280102253),\n",
       "   np.float64(1.031504049897194),\n",
       "   np.float64(1.0366704687476158),\n",
       "   np.float64(1.0476349666714668),\n",
       "   np.float64(1.0492714866995811),\n",
       "   np.float64(1.0331745967268944),\n",
       "   np.float64(1.0306490361690521),\n",
       "   np.float64(1.0247021242976189),\n",
       "   np.float64(1.0203451961278915),\n",
       "   np.float64(1.0166356563568115),\n",
       "   np.float64(1.0191396474838257),\n",
       "   np.float64(1.010299190878868),\n",
       "   np.float64(1.0010368190705776),\n",
       "   np.float64(0.9967083148658276),\n",
       "   np.float64(0.9972174093127251),\n",
       "   np.float64(0.9839017949998379),\n",
       "   np.float64(0.9865103513002396),\n",
       "   np.float64(0.9905906766653061),\n",
       "   np.float64(0.9964244067668915),\n",
       "   np.float64(0.9873540103435516),\n",
       "   np.float64(1.0011708661913872),\n",
       "   np.float64(0.9905469194054604),\n",
       "   np.float64(0.9884582497179508),\n",
       "   np.float64(1.0041573792696),\n",
       "   np.float64(0.9888358786702156),\n",
       "   np.float64(1.0036159567534924),\n",
       "   np.float64(1.0011800155043602),\n",
       "   np.float64(1.0110980533063412),\n",
       "   np.float64(0.9890865795314312),\n",
       "   np.float64(0.9946315661072731),\n",
       "   np.float64(0.9951268695294857),\n",
       "   np.float64(0.979911345988512),\n",
       "   np.float64(0.9794291406869888),\n",
       "   np.float64(0.9952871911227703),\n",
       "   np.float64(0.9876372218132019),\n",
       "   np.float64(0.9886338859796524),\n",
       "   np.float64(0.9822551645338535),\n",
       "   np.float64(1.0038208439946175),\n",
       "   np.float64(0.9992320127785206),\n",
       "   np.float64(0.9987012296915054),\n",
       "   np.float64(0.9908747971057892),\n",
       "   np.float64(0.9855969548225403),\n",
       "   np.float64(0.9779204726219177),\n",
       "   np.float64(0.9943475313484669),\n",
       "   np.float64(0.9902335181832314),\n",
       "   np.float64(0.9928617253899574),\n",
       "   np.float64(0.9950859472155571),\n",
       "   np.float64(0.9768875911831856),\n",
       "   np.float64(0.9915714338421822),\n",
       "   np.float64(0.9904015697538853),\n",
       "   np.float64(0.9871570579707623),\n",
       "   np.float64(0.9848017208278179),\n",
       "   np.float64(0.9952230416238308),\n",
       "   np.float64(0.9991679191589355),\n",
       "   np.float64(0.9852031767368317),\n",
       "   np.float64(0.9846492595970631),\n",
       "   np.float64(0.9922420270740986),\n",
       "   np.float64(0.983764786273241),\n",
       "   np.float64(0.9841180220246315),\n",
       "   np.float64(0.9937846399843693),\n",
       "   np.float64(0.9881024919450283),\n",
       "   np.float64(0.9886350221931934),\n",
       "   np.float64(1.0036413744091988),\n",
       "   np.float64(0.9955472499132156),\n",
       "   np.float64(0.9898177348077297),\n",
       "   np.float64(0.9961152635514736),\n",
       "   np.float64(0.9817541316151619),\n",
       "   np.float64(0.9899755492806435),\n",
       "   np.float64(0.9964620359241962),\n",
       "   np.float64(0.9914960823953152),\n",
       "   np.float64(0.9913586378097534),\n",
       "   np.float64(0.9865527376532555),\n",
       "   np.float64(0.9880344718694687),\n",
       "   np.float64(0.9872633665800095),\n",
       "   np.float64(0.9933620952069759),\n",
       "   np.float64(0.9849854335188866),\n",
       "   np.float64(0.9924084469676018),\n",
       "   np.float64(0.9933902844786644),\n",
       "   np.float64(0.9846860468387604),\n",
       "   np.float64(0.9913885183632374),\n",
       "   np.float64(0.997311994433403),\n",
       "   np.float64(1.0002280324697495),\n",
       "   np.float64(1.0136719346046448),\n",
       "   np.float64(0.9982145726680756),\n",
       "   np.float64(0.9848795272409916),\n",
       "   np.float64(0.974019892513752),\n",
       "   np.float64(0.9831044413149357),\n",
       "   np.float64(0.9875853024423122),\n",
       "   np.float64(0.9782232157886028),\n",
       "   np.float64(0.9826177433133125),\n",
       "   np.float64(0.9921088665723801),\n",
       "   np.float64(1.0131238587200642),\n",
       "   np.float64(0.9971767403185368),\n",
       "   np.float64(0.9812355563044548),\n",
       "   np.float64(0.9847632832825184),\n",
       "   np.float64(0.9986203648149967),\n",
       "   np.float64(0.989496823400259),\n",
       "   np.float64(0.9825347512960434),\n",
       "   np.float64(0.9841363281011581),\n",
       "   np.float64(0.9846818223595619),\n",
       "   np.float64(0.9873764850199223),\n",
       "   np.float64(0.986551396548748),\n",
       "   np.float64(1.0054578892886639),\n",
       "   np.float64(1.000601414591074),\n",
       "   np.float64(1.0081186592578888),\n",
       "   np.float64(0.9808675013482571),\n",
       "   np.float64(0.9952666014432907),\n",
       "   np.float64(0.9910878688097),\n",
       "   np.float64(1.001526240259409),\n",
       "   np.float64(0.9964150190353394),\n",
       "   np.float64(1.0046664960682392),\n",
       "   np.float64(0.9944864213466644),\n",
       "   np.float64(1.004241619259119),\n",
       "   np.float64(0.991623442620039),\n",
       "   np.float64(1.0165771655738354),\n",
       "   np.float64(0.9965076483786106),\n",
       "   np.float64(1.0150530524551868),\n",
       "   np.float64(1.0056835860013962),\n",
       "   np.float64(0.9966414570808411),\n",
       "   np.float64(0.9932312779128551),\n",
       "   np.float64(0.995793592184782),\n",
       "   np.float64(1.003034297376871)],\n",
       "  'val_mdn_loss_std': [np.float64(0.006481391908545653),\n",
       "   np.float64(0.010621812708218695),\n",
       "   np.float64(0.01382953357339719),\n",
       "   np.float64(0.01678385349284371),\n",
       "   np.float64(0.010104684122984094),\n",
       "   np.float64(0.011606434658498076),\n",
       "   np.float64(0.009929522831428712),\n",
       "   np.float64(0.010421616889056249),\n",
       "   np.float64(0.010963574316448992),\n",
       "   np.float64(0.010485633167281247),\n",
       "   np.float64(0.013267298163449643),\n",
       "   np.float64(0.015131796921863646),\n",
       "   np.float64(0.009954548986210324),\n",
       "   np.float64(0.012289911928261826),\n",
       "   np.float64(0.014207325759116879),\n",
       "   np.float64(0.012093609476103445),\n",
       "   np.float64(0.012140614520490768),\n",
       "   np.float64(0.011324866581647977),\n",
       "   np.float64(0.012726357202825453),\n",
       "   np.float64(0.012257712730747642),\n",
       "   np.float64(0.011670119712099523),\n",
       "   np.float64(0.010957016371179249),\n",
       "   np.float64(0.011390542414849613),\n",
       "   np.float64(0.01174762952812039),\n",
       "   np.float64(0.01229099494701454),\n",
       "   np.float64(0.014509616130599777),\n",
       "   np.float64(0.01730895716761135),\n",
       "   np.float64(0.012709720256828573),\n",
       "   np.float64(0.012104719749598833),\n",
       "   np.float64(0.011555951358667046),\n",
       "   np.float64(0.011128303076631506),\n",
       "   np.float64(0.011271577932375846),\n",
       "   np.float64(0.013234705739634171),\n",
       "   np.float64(0.011190324249290565),\n",
       "   np.float64(0.010441487519005999),\n",
       "   np.float64(0.011619664614439069),\n",
       "   np.float64(0.011284795042611753),\n",
       "   np.float64(0.01088671242649234),\n",
       "   np.float64(0.011751354743613444),\n",
       "   np.float64(0.010377484122666236),\n",
       "   np.float64(0.012836211400046739),\n",
       "   np.float64(0.01345346200346878),\n",
       "   np.float64(0.010760454273530366),\n",
       "   np.float64(0.010860315439933288),\n",
       "   np.float64(0.01195915348659166),\n",
       "   np.float64(0.010977352054848673),\n",
       "   np.float64(0.010197534686039593),\n",
       "   np.float64(0.011109288311277396),\n",
       "   np.float64(0.012091409000766223),\n",
       "   np.float64(0.012643372724394123),\n",
       "   np.float64(0.010937410332273294),\n",
       "   np.float64(0.009920559568822429),\n",
       "   np.float64(0.011140257208709058),\n",
       "   np.float64(0.012080674459527633),\n",
       "   np.float64(0.013221847409411782),\n",
       "   np.float64(0.0118898757776936),\n",
       "   np.float64(0.011150306715353747),\n",
       "   np.float64(0.010633532209758215),\n",
       "   np.float64(0.013387143145531374),\n",
       "   np.float64(0.009117846434677397),\n",
       "   np.float64(0.010069023547443953),\n",
       "   np.float64(0.010190476394120929),\n",
       "   np.float64(0.011142482199276407),\n",
       "   np.float64(0.010237023122127948),\n",
       "   np.float64(0.01217863880558917),\n",
       "   np.float64(0.011844384845142103),\n",
       "   np.float64(0.011265793652442397),\n",
       "   np.float64(0.011727933819758072),\n",
       "   np.float64(0.012156258053024133),\n",
       "   np.float64(0.01163485259261121),\n",
       "   np.float64(0.013428421926030333),\n",
       "   np.float64(0.013619534284732815),\n",
       "   np.float64(0.011999937087656364),\n",
       "   np.float64(0.011118580545859439),\n",
       "   np.float64(0.01207493730386715),\n",
       "   np.float64(0.011605766292833443),\n",
       "   np.float64(0.011367788647091502),\n",
       "   np.float64(0.010291042900872394),\n",
       "   np.float64(0.010954052030368587),\n",
       "   np.float64(0.010931000443283703),\n",
       "   np.float64(0.014040074492391386),\n",
       "   np.float64(0.01344809972583374),\n",
       "   np.float64(0.016880345206472882),\n",
       "   np.float64(0.010632527063720941),\n",
       "   np.float64(0.01445897222851162),\n",
       "   np.float64(0.010406276074049845),\n",
       "   np.float64(0.010747784807613738),\n",
       "   np.float64(0.014678250160387983),\n",
       "   np.float64(0.012433356476405286),\n",
       "   np.float64(0.012703522741877695),\n",
       "   np.float64(0.015342765881022026),\n",
       "   np.float64(0.0105174281186083),\n",
       "   np.float64(0.011905404464043732),\n",
       "   np.float64(0.010357651749510774),\n",
       "   np.float64(0.011946524448162696),\n",
       "   np.float64(0.011781497388603436),\n",
       "   np.float64(0.009905644868369506),\n",
       "   np.float64(0.014988672927573563),\n",
       "   np.float64(0.01655923601286347),\n",
       "   np.float64(0.012213614346026703),\n",
       "   np.float64(0.012079879549175759),\n",
       "   np.float64(0.012851860141245494),\n",
       "   np.float64(0.011663767211378613),\n",
       "   np.float64(0.011178054871906243),\n",
       "   np.float64(0.009580752614575003),\n",
       "   np.float64(0.009595473488726691),\n",
       "   np.float64(0.012441695078744666),\n",
       "   np.float64(0.013699509873649787),\n",
       "   np.float64(0.012210740024638154),\n",
       "   np.float64(0.010599000768442652),\n",
       "   np.float64(0.013150779827273721),\n",
       "   np.float64(0.011928468140014512),\n",
       "   np.float64(0.010040607932815969),\n",
       "   np.float64(0.011743728863597064),\n",
       "   np.float64(0.014218614438838154),\n",
       "   np.float64(0.010964196604669093),\n",
       "   np.float64(0.010166040851535683),\n",
       "   np.float64(0.012313256949810837),\n",
       "   np.float64(0.008655080115722212),\n",
       "   np.float64(0.008944565473684383),\n",
       "   np.float64(0.010632868735652562),\n",
       "   np.float64(0.008917290884597152),\n",
       "   np.float64(0.009904081083738232),\n",
       "   np.float64(0.010928878613564385),\n",
       "   np.float64(0.01030458477180442),\n",
       "   np.float64(0.008448183060985787),\n",
       "   np.float64(0.010342733352661854),\n",
       "   np.float64(0.008820804764506553),\n",
       "   np.float64(0.011724774421461023),\n",
       "   np.float64(0.011654577457428457),\n",
       "   np.float64(0.01146570645779359),\n",
       "   np.float64(0.008925804068795224),\n",
       "   np.float64(0.01272701505091341),\n",
       "   np.float64(0.010837229256083008),\n",
       "   np.float64(0.01108438492608474),\n",
       "   np.float64(0.010421942300828534),\n",
       "   np.float64(0.013794003292995375),\n",
       "   np.float64(0.011613147281627966),\n",
       "   np.float64(0.013206899075753231),\n",
       "   np.float64(0.013326233915913083),\n",
       "   np.float64(0.012236069793506164),\n",
       "   np.float64(0.012506488069496124),\n",
       "   np.float64(0.011974866989657544),\n",
       "   np.float64(0.0115811525372453),\n",
       "   np.float64(0.01043011708663997),\n",
       "   np.float64(0.012258029636337786),\n",
       "   np.float64(0.0115919676339965),\n",
       "   np.float64(0.009797952852611843),\n",
       "   np.float64(0.011794967040578258),\n",
       "   np.float64(0.01110321490707832),\n",
       "   np.float64(0.011093318312448981),\n",
       "   np.float64(0.010269812334120783),\n",
       "   np.float64(0.010163560236032768),\n",
       "   np.float64(0.012275030202496536),\n",
       "   np.float64(0.008542610753608824),\n",
       "   np.float64(0.011469342927067338),\n",
       "   np.float64(0.012598250146144178),\n",
       "   np.float64(0.010045691463653124),\n",
       "   np.float64(0.010137456538163264),\n",
       "   np.float64(0.009183235392427993),\n",
       "   np.float64(0.009481539428000316),\n",
       "   np.float64(0.009850880407417904),\n",
       "   np.float64(0.010498023980473774),\n",
       "   np.float64(0.01115400192155587),\n",
       "   np.float64(0.00999390800040959),\n",
       "   np.float64(0.010684354542699892),\n",
       "   np.float64(0.009365422096576983),\n",
       "   np.float64(0.00996431704054846),\n",
       "   np.float64(0.011768127050860175),\n",
       "   np.float64(0.010391812447646394),\n",
       "   np.float64(0.010680946443554426),\n",
       "   np.float64(0.012941802373631194),\n",
       "   np.float64(0.011196624873938412),\n",
       "   np.float64(0.012233394903395178),\n",
       "   np.float64(0.009335543631958347),\n",
       "   np.float64(0.010772535840554218),\n",
       "   np.float64(0.009849538460576235),\n",
       "   np.float64(0.010576787283858245),\n",
       "   np.float64(0.01241288866946143),\n",
       "   np.float64(0.01010734211489968),\n",
       "   np.float64(0.01052462358729764),\n",
       "   np.float64(0.011418002825988623),\n",
       "   np.float64(0.011932373546320168),\n",
       "   np.float64(0.01008202130363899),\n",
       "   np.float64(0.01086801642071679),\n",
       "   np.float64(0.012101989466539549),\n",
       "   np.float64(0.011299136555762219),\n",
       "   np.float64(0.011094886925388448),\n",
       "   np.float64(0.012016291342069914),\n",
       "   np.float64(0.010991901998425462),\n",
       "   np.float64(0.011368954121594348),\n",
       "   np.float64(0.011209107713056821),\n",
       "   np.float64(0.010567670912097643),\n",
       "   np.float64(0.010540891298420693),\n",
       "   np.float64(0.010849896527054563),\n",
       "   np.float64(0.010053733606018833),\n",
       "   np.float64(0.010448182927843335),\n",
       "   np.float64(0.008710417370009393),\n",
       "   np.float64(0.00912595841097238),\n",
       "   np.float64(0.009989387461245374),\n",
       "   np.float64(0.011921651121387388),\n",
       "   np.float64(0.0100818510587031),\n",
       "   np.float64(0.009166427598696188),\n",
       "   np.float64(0.009900714147218395),\n",
       "   np.float64(0.010848649369285668),\n",
       "   np.float64(0.009360495821068954),\n",
       "   np.float64(0.012161276968172109),\n",
       "   np.float64(0.009620968293290148),\n",
       "   np.float64(0.011518352200753373),\n",
       "   np.float64(0.01233546234706778),\n",
       "   np.float64(0.010478314736783062),\n",
       "   np.float64(0.01055562952238451),\n",
       "   np.float64(0.010536454897130442),\n",
       "   np.float64(0.010468029451559233),\n",
       "   np.float64(0.01020893025323518),\n",
       "   np.float64(0.012668003053029876),\n",
       "   np.float64(0.011341429427341027),\n",
       "   np.float64(0.010452406494883727),\n",
       "   np.float64(0.012085617937141462),\n",
       "   np.float64(0.012486206432102492),\n",
       "   np.float64(0.009828495375665472),\n",
       "   np.float64(0.0112359454233723),\n",
       "   np.float64(0.010958407968314956),\n",
       "   np.float64(0.011033036990484722),\n",
       "   np.float64(0.011563008225280377),\n",
       "   np.float64(0.013016055997799474),\n",
       "   np.float64(0.011141919606742691),\n",
       "   np.float64(0.011796920166414109),\n",
       "   np.float64(0.010733797825428848),\n",
       "   np.float64(0.010864328304362196),\n",
       "   np.float64(0.010184735474774513),\n",
       "   np.float64(0.0114746402333397),\n",
       "   np.float64(0.009837902284323765),\n",
       "   np.float64(0.00867110470803398),\n",
       "   np.float64(0.012568405070744453),\n",
       "   np.float64(0.012044502682151828),\n",
       "   np.float64(0.012318818742729233),\n",
       "   np.float64(0.00930219881305299),\n",
       "   np.float64(0.010629119242508754),\n",
       "   np.float64(0.011951260774371643),\n",
       "   np.float64(0.011964250575894187),\n",
       "   np.float64(0.012124696339735456),\n",
       "   np.float64(0.010453022060232388),\n",
       "   np.float64(0.011773605806924149),\n",
       "   np.float64(0.014177029485628732),\n",
       "   np.float64(0.009613551225463668),\n",
       "   np.float64(0.012139514131852455),\n",
       "   np.float64(0.01311453079934008),\n",
       "   np.float64(0.012490337587219425),\n",
       "   np.float64(0.013051232226983673),\n",
       "   np.float64(0.010948938665133856),\n",
       "   np.float64(0.011945908976515826),\n",
       "   np.float64(0.012301054481943682),\n",
       "   np.float64(0.009453530617223802),\n",
       "   np.float64(0.01202565677212648),\n",
       "   np.float64(0.013258599691603921),\n",
       "   np.float64(0.011411694312335473),\n",
       "   np.float64(0.014418016092519723),\n",
       "   np.float64(0.014404559049653492),\n",
       "   np.float64(0.011345081867017411),\n",
       "   np.float64(0.010881474742130148),\n",
       "   np.float64(0.012538709423055027),\n",
       "   np.float64(0.012368308090623989),\n",
       "   np.float64(0.014195289417103858),\n",
       "   np.float64(0.010687835967715154),\n",
       "   np.float64(0.014659691358414785),\n",
       "   np.float64(0.012235522175052827),\n",
       "   np.float64(0.01323075185666536),\n",
       "   np.float64(0.01281420809794035),\n",
       "   np.float64(0.010435631301391579),\n",
       "   np.float64(0.011917428284862766),\n",
       "   np.float64(0.012797393721430917),\n",
       "   np.float64(0.01175498883375737),\n",
       "   np.float64(0.01259670909882884),\n",
       "   np.float64(0.01422202370088713),\n",
       "   np.float64(0.013109645726995488),\n",
       "   np.float64(0.012582708147046415),\n",
       "   np.float64(0.010347287696712741),\n",
       "   np.float64(0.013570096853756703),\n",
       "   np.float64(0.01108950743655167),\n",
       "   np.float64(0.016358243188903606),\n",
       "   np.float64(0.009810073740919033),\n",
       "   np.float64(0.012182385211752559),\n",
       "   np.float64(0.013300340537349112),\n",
       "   np.float64(0.012020786674567014),\n",
       "   np.float64(0.014456351308604166),\n",
       "   np.float64(0.013937262579530074),\n",
       "   np.float64(0.013564489457516392),\n",
       "   np.float64(0.01181747850979422),\n",
       "   np.float64(0.011849030578372464),\n",
       "   np.float64(0.012399886326476615),\n",
       "   np.float64(0.013931224446327998),\n",
       "   np.float64(0.013263786221588192),\n",
       "   np.float64(0.01486232508882991),\n",
       "   np.float64(0.014093724465835409),\n",
       "   np.float64(0.012711986026340735),\n",
       "   np.float64(0.0133407485190231),\n",
       "   np.float64(0.01545777730130563),\n",
       "   np.float64(0.013135362664065085),\n",
       "   np.float64(0.01066697819402928),\n",
       "   np.float64(0.012379015567415323),\n",
       "   np.float64(0.01340145996599286),\n",
       "   np.float64(0.015572989252276266),\n",
       "   np.float64(0.012628273223233726),\n",
       "   np.float64(0.013694360940755431),\n",
       "   np.float64(0.015392732057914898),\n",
       "   np.float64(0.014087610145746728),\n",
       "   np.float64(0.01239435165610499),\n",
       "   np.float64(0.014385482267652263),\n",
       "   np.float64(0.01176655034442704),\n",
       "   np.float64(0.013759237232215952),\n",
       "   np.float64(0.012861116473159611),\n",
       "   np.float64(0.01275674662083692),\n",
       "   np.float64(0.01302240777874589),\n",
       "   np.float64(0.013119824609247661),\n",
       "   np.float64(0.014485996682515028),\n",
       "   np.float64(0.012296795937547345),\n",
       "   np.float64(0.01556917748199235),\n",
       "   np.float64(0.012767374413442795),\n",
       "   np.float64(0.014587026056460298),\n",
       "   np.float64(0.012791753056509075),\n",
       "   np.float64(0.011525878619077674),\n",
       "   np.float64(0.014417923074961675),\n",
       "   np.float64(0.011941727900944128),\n",
       "   np.float64(0.014035664368969051),\n",
       "   np.float64(0.021426564674885743),\n",
       "   np.float64(0.017269453615678595),\n",
       "   np.float64(0.017907900533967),\n",
       "   np.float64(0.013743200166006539),\n",
       "   np.float64(0.014489784157069197),\n",
       "   np.float64(0.01489529388741581),\n",
       "   np.float64(0.014639583691679472),\n",
       "   np.float64(0.013842188459250119),\n",
       "   np.float64(0.01802475654500199),\n",
       "   np.float64(0.0156178904396408),\n",
       "   np.float64(0.016732499281692657),\n",
       "   np.float64(0.012133514167034516),\n",
       "   np.float64(0.01515055429806017),\n",
       "   np.float64(0.013992651284345684),\n",
       "   np.float64(0.017110281831289938),\n",
       "   np.float64(0.013264739256671428),\n",
       "   np.float64(0.015374649289839462),\n",
       "   np.float64(0.014946721749268574),\n",
       "   np.float64(0.014552944554955018),\n",
       "   np.float64(0.014446381065885684),\n",
       "   np.float64(0.015137274461334395),\n",
       "   np.float64(0.01357873973956822),\n",
       "   np.float64(0.01527254470161754),\n",
       "   np.float64(0.016046072451452852),\n",
       "   np.float64(0.017313200045858117),\n",
       "   np.float64(0.01847429384064245),\n",
       "   np.float64(0.015980994779874195),\n",
       "   np.float64(0.014757476564686779),\n",
       "   np.float64(0.014553741153584848),\n",
       "   np.float64(0.016385963219574817),\n",
       "   np.float64(0.014091309829138268),\n",
       "   np.float64(0.016208496047872934),\n",
       "   np.float64(0.021286405628861943),\n",
       "   np.float64(0.015882147919607625),\n",
       "   np.float64(0.013086409842655997),\n",
       "   np.float64(0.015287109313285302),\n",
       "   np.float64(0.015675407887295743),\n",
       "   np.float64(0.019422265576556068),\n",
       "   np.float64(0.013903792899011166),\n",
       "   np.float64(0.01584552079212801),\n",
       "   np.float64(0.01947189554225976),\n",
       "   np.float64(0.013903885428680153),\n",
       "   np.float64(0.015452705901505107),\n",
       "   np.float64(0.01700986326053611),\n",
       "   np.float64(0.011305321060127214),\n",
       "   np.float64(0.015365164688676077),\n",
       "   np.float64(0.015019810593011874),\n",
       "   np.float64(0.01694233669718885),\n",
       "   np.float64(0.017041155831224182),\n",
       "   np.float64(0.017713173143089473),\n",
       "   np.float64(0.016065510346419867),\n",
       "   np.float64(0.017423068192455027),\n",
       "   np.float64(0.01748591551368671),\n",
       "   np.float64(0.016891709351026747),\n",
       "   np.float64(0.015077279260331339),\n",
       "   np.float64(0.017534728936283245),\n",
       "   np.float64(0.014552956940268381),\n",
       "   np.float64(0.016503340794925946),\n",
       "   np.float64(0.01944522980513984),\n",
       "   np.float64(0.012568789669347899),\n",
       "   np.float64(0.01565096366363304),\n",
       "   np.float64(0.017443673775917396),\n",
       "   np.float64(0.015174783826195146),\n",
       "   np.float64(0.0168132912534792),\n",
       "   np.float64(0.01637766946210188),\n",
       "   np.float64(0.01594122011103589),\n",
       "   np.float64(0.018374630845043233),\n",
       "   np.float64(0.016602299630400094),\n",
       "   np.float64(0.016058809429528673),\n",
       "   np.float64(0.017837487770394244),\n",
       "   np.float64(0.012404807281799454),\n",
       "   np.float64(0.01688795828923647),\n",
       "   np.float64(0.018220966903071806),\n",
       "   np.float64(0.02029581782806449),\n",
       "   np.float64(0.017746402715995084),\n",
       "   np.float64(0.016988061856833037),\n",
       "   np.float64(0.016020495745038194),\n",
       "   np.float64(0.016572218534466807),\n",
       "   np.float64(0.018380295457278727),\n",
       "   np.float64(0.014222812640594155),\n",
       "   np.float64(0.014220581316756867),\n",
       "   np.float64(0.02097094256564477),\n",
       "   np.float64(0.01731136256388991),\n",
       "   np.float64(0.018519824578524628),\n",
       "   np.float64(0.01509685327732847),\n",
       "   np.float64(0.017872497721696634),\n",
       "   np.float64(0.0155323812933319),\n",
       "   np.float64(0.016716521360115477),\n",
       "   np.float64(0.015012581632566717),\n",
       "   np.float64(0.01580319417776182),\n",
       "   np.float64(0.01485147046262314),\n",
       "   np.float64(0.016511121333972287),\n",
       "   np.float64(0.017113585919129322),\n",
       "   np.float64(0.010632918507274198),\n",
       "   np.float64(0.01650552591970442),\n",
       "   np.float64(0.015780769162641158),\n",
       "   np.float64(0.017291922183808782),\n",
       "   np.float64(0.013691946168541318),\n",
       "   np.float64(0.016452582010626313),\n",
       "   np.float64(0.015163689429466257),\n",
       "   np.float64(0.01781190952697299),\n",
       "   np.float64(0.017516351804333044),\n",
       "   np.float64(0.016121053350089818),\n",
       "   np.float64(0.017851419620458393),\n",
       "   np.float64(0.01817637813882475),\n",
       "   np.float64(0.013021989612327794),\n",
       "   np.float64(0.016844795048001116),\n",
       "   np.float64(0.016671220970606998),\n",
       "   np.float64(0.016337251633963865),\n",
       "   np.float64(0.016653502042881272),\n",
       "   np.float64(0.016569871160009367),\n",
       "   np.float64(0.018521891023338316)],\n",
       "  'val_affi_coeffs_mean': [np.float64(0.4162954790517688),\n",
       "   np.float64(0.4183841785416007),\n",
       "   np.float64(0.42236150056123734),\n",
       "   np.float64(0.429596371948719),\n",
       "   np.float64(0.4370566885918379),\n",
       "   np.float64(0.44118728302419186),\n",
       "   np.float64(0.4416899122297764),\n",
       "   np.float64(0.43729030154645443),\n",
       "   np.float64(0.4397790040820837),\n",
       "   np.float64(0.4394412264227867),\n",
       "   np.float64(0.43727187253534794),\n",
       "   np.float64(0.4403251726180315),\n",
       "   np.float64(0.43492033053189516),\n",
       "   np.float64(0.4329653764143586),\n",
       "   np.float64(0.43671117071062326),\n",
       "   np.float64(0.4355772715061903),\n",
       "   np.float64(0.43743592593818903),\n",
       "   np.float64(0.4319642661139369),\n",
       "   np.float64(0.4274906646460295),\n",
       "   np.float64(0.4312881790101528),\n",
       "   np.float64(0.43173836171627045),\n",
       "   np.float64(0.42933342047035694),\n",
       "   np.float64(0.42988890781998634),\n",
       "   np.float64(0.4323946926742792),\n",
       "   np.float64(0.4234439432621002),\n",
       "   np.float64(0.42851661145687103),\n",
       "   np.float64(0.42202836833894253),\n",
       "   np.float64(0.42422900162637234),\n",
       "   np.float64(0.4317217171192169),\n",
       "   np.float64(0.4286811500787735),\n",
       "   np.float64(0.42840473260730505),\n",
       "   np.float64(0.4267886681482196),\n",
       "   np.float64(0.42858408857136965),\n",
       "   np.float64(0.4325266629457474),\n",
       "   np.float64(0.43338488321751356),\n",
       "   np.float64(0.43253831937909126),\n",
       "   np.float64(0.4292340250685811),\n",
       "   np.float64(0.4304203549399972),\n",
       "   np.float64(0.4247433040291071),\n",
       "   np.float64(0.42898466624319553),\n",
       "   np.float64(0.42440289445221424),\n",
       "   np.float64(0.42921100463718176),\n",
       "   np.float64(0.4292035372927785),\n",
       "   np.float64(0.427666699513793),\n",
       "   np.float64(0.428959128446877),\n",
       "   np.float64(0.42857778910547495),\n",
       "   np.float64(0.42772759310901165),\n",
       "   np.float64(0.42451613768935204),\n",
       "   np.float64(0.4265210162848234),\n",
       "   np.float64(0.42285447753965855),\n",
       "   np.float64(0.4272183980792761),\n",
       "   np.float64(0.4246011395007372),\n",
       "   np.float64(0.4269998073577881),\n",
       "   np.float64(0.428173272870481),\n",
       "   np.float64(0.42188563011586666),\n",
       "   np.float64(0.42430727928876877),\n",
       "   np.float64(0.4292359631508589),\n",
       "   np.float64(0.42736100777983665),\n",
       "   np.float64(0.42614677734673023),\n",
       "   np.float64(0.4270078968256712),\n",
       "   np.float64(0.42980827018618584),\n",
       "   np.float64(0.4244729317724705),\n",
       "   np.float64(0.4240697482600808),\n",
       "   np.float64(0.42851131223142147),\n",
       "   np.float64(0.4195366110652685),\n",
       "   np.float64(0.4250351246446371),\n",
       "   np.float64(0.42587673105299473),\n",
       "   np.float64(0.4268213678151369),\n",
       "   np.float64(0.4226618241518736),\n",
       "   np.float64(0.42056012712419033),\n",
       "   np.float64(0.4273308962583542),\n",
       "   np.float64(0.4222963387146592),\n",
       "   np.float64(0.42453224398195744),\n",
       "   np.float64(0.4267243715003133),\n",
       "   np.float64(0.4221681263297796),\n",
       "   np.float64(0.42547095008194447),\n",
       "   np.float64(0.4180195704102516),\n",
       "   np.float64(0.4237199770286679),\n",
       "   np.float64(0.42147684562951326),\n",
       "   np.float64(0.42401592060923576),\n",
       "   np.float64(0.4243829697370529),\n",
       "   np.float64(0.4237089827656746),\n",
       "   np.float64(0.42130943946540356),\n",
       "   np.float64(0.4286282900720835),\n",
       "   np.float64(0.42528398893773556),\n",
       "   np.float64(0.42883529514074326),\n",
       "   np.float64(0.4274384677410126),\n",
       "   np.float64(0.42410314083099365),\n",
       "   np.float64(0.4261613031849265),\n",
       "   np.float64(0.4230975080281496),\n",
       "   np.float64(0.42558132763952017),\n",
       "   np.float64(0.42361052706837654),\n",
       "   np.float64(0.4248664937913418),\n",
       "   np.float64(0.42506267316639423),\n",
       "   np.float64(0.4291771948337555),\n",
       "   np.float64(0.4249679371714592),\n",
       "   np.float64(0.42509292159229517),\n",
       "   np.float64(0.41942256689071655),\n",
       "   np.float64(0.41740281134843826),\n",
       "   np.float64(0.4224436804652214),\n",
       "   np.float64(0.425674507394433),\n",
       "   np.float64(0.4221124164760113),\n",
       "   np.float64(0.42272776551544666),\n",
       "   np.float64(0.4228271283209324),\n",
       "   np.float64(0.42497411370277405),\n",
       "   np.float64(0.425612885504961),\n",
       "   np.float64(0.4245156366378069),\n",
       "   np.float64(0.42144424840807915),\n",
       "   np.float64(0.42293260246515274),\n",
       "   np.float64(0.42196106165647507),\n",
       "   np.float64(0.41913775634020567),\n",
       "   np.float64(0.4234017413109541),\n",
       "   np.float64(0.4230688326060772),\n",
       "   np.float64(0.4230731101706624),\n",
       "   np.float64(0.4222926627844572),\n",
       "   np.float64(0.42002291418612003),\n",
       "   np.float64(0.421340337023139),\n",
       "   np.float64(0.41956666857004166),\n",
       "   np.float64(0.42351526767015457),\n",
       "   np.float64(0.4217743370682001),\n",
       "   np.float64(0.4256517272442579),\n",
       "   np.float64(0.4241351345553994),\n",
       "   np.float64(0.42273787781596184),\n",
       "   np.float64(0.4225584398955107),\n",
       "   np.float64(0.4233258729800582),\n",
       "   np.float64(0.42579064704477787),\n",
       "   np.float64(0.41959756053984165),\n",
       "   np.float64(0.4239225145429373),\n",
       "   np.float64(0.42308856546878815),\n",
       "   np.float64(0.4245009794831276),\n",
       "   np.float64(0.4201628901064396),\n",
       "   np.float64(0.419383960776031),\n",
       "   np.float64(0.42154525965452194),\n",
       "   np.float64(0.4228320848196745),\n",
       "   np.float64(0.42192283645272255),\n",
       "   np.float64(0.41802296228706837),\n",
       "   np.float64(0.4179208492860198),\n",
       "   np.float64(0.42230373434722424),\n",
       "   np.float64(0.4242073129862547),\n",
       "   np.float64(0.41723316069692373),\n",
       "   np.float64(0.4234709721058607),\n",
       "   np.float64(0.423752136528492),\n",
       "   np.float64(0.4210525397211313),\n",
       "   np.float64(0.42131081130355597),\n",
       "   np.float64(0.42208653781563044),\n",
       "   np.float64(0.41802104376256466),\n",
       "   np.float64(0.4212835878133774),\n",
       "   np.float64(0.4215176850557327),\n",
       "   np.float64(0.4222194664180279),\n",
       "   np.float64(0.4186575748026371),\n",
       "   np.float64(0.41880552377551794),\n",
       "   np.float64(0.4225191492587328),\n",
       "   np.float64(0.42270364332944155),\n",
       "   np.float64(0.41831823624670506),\n",
       "   np.float64(0.4169934596866369),\n",
       "   np.float64(0.41427781246602535),\n",
       "   np.float64(0.41969489213079214),\n",
       "   np.float64(0.41733720153570175),\n",
       "   np.float64(0.4196762181818485),\n",
       "   np.float64(0.41764248069375753),\n",
       "   np.float64(0.4176224544644356),\n",
       "   np.float64(0.42348621878772974),\n",
       "   np.float64(0.4205537922680378),\n",
       "   np.float64(0.4172801487147808),\n",
       "   np.float64(0.4234850574284792),\n",
       "   np.float64(0.41663315054029226),\n",
       "   np.float64(0.4195285849273205),\n",
       "   np.float64(0.422199291177094),\n",
       "   np.float64(0.420208758674562),\n",
       "   np.float64(0.4195262622088194),\n",
       "   np.float64(0.41748919151723385),\n",
       "   np.float64(0.4159826347604394),\n",
       "   np.float64(0.41501326486468315),\n",
       "   np.float64(0.41873242426663637),\n",
       "   np.float64(0.42129495181143284),\n",
       "   np.float64(0.42419071309268475),\n",
       "   np.float64(0.42066262662410736),\n",
       "   np.float64(0.4187209941446781),\n",
       "   np.float64(0.4196419660001993),\n",
       "   np.float64(0.4203558936715126),\n",
       "   np.float64(0.41580422408878803),\n",
       "   np.float64(0.41713833156973124),\n",
       "   np.float64(0.41779795102775097),\n",
       "   np.float64(0.41908382065594196),\n",
       "   np.float64(0.4179268069565296),\n",
       "   np.float64(0.41793456487357616),\n",
       "   np.float64(0.4167596995830536),\n",
       "   np.float64(0.4178664395585656),\n",
       "   np.float64(0.4206385798752308),\n",
       "   np.float64(0.4133129846304655),\n",
       "   np.float64(0.41980389412492514),\n",
       "   np.float64(0.4177743075415492),\n",
       "   np.float64(0.4158058790490031),\n",
       "   np.float64(0.41956058517098427),\n",
       "   np.float64(0.42147903330624104),\n",
       "   np.float64(0.4222232326865196),\n",
       "   np.float64(0.4197312453761697),\n",
       "   np.float64(0.41876240633428097),\n",
       "   np.float64(0.42396043986082077),\n",
       "   np.float64(0.4206110769882798),\n",
       "   np.float64(0.4198584333062172),\n",
       "   np.float64(0.418075037188828),\n",
       "   np.float64(0.420083774253726),\n",
       "   np.float64(0.4219778794795275),\n",
       "   np.float64(0.42359144426882267),\n",
       "   np.float64(0.4175713649019599),\n",
       "   np.float64(0.4224142674356699),\n",
       "   np.float64(0.41732318326830864),\n",
       "   np.float64(0.4203796871006489),\n",
       "   np.float64(0.4235883429646492),\n",
       "   np.float64(0.4210045263171196),\n",
       "   np.float64(0.4208558667451143),\n",
       "   np.float64(0.41996717918664217),\n",
       "   np.float64(0.4214665908366442),\n",
       "   np.float64(0.418897426687181),\n",
       "   np.float64(0.4186666952446103),\n",
       "   np.float64(0.41951443813741207),\n",
       "   np.float64(0.4186405250802636),\n",
       "   np.float64(0.42074924148619175),\n",
       "   np.float64(0.423255518078804),\n",
       "   np.float64(0.41840107180178165),\n",
       "   np.float64(0.41731249913573265),\n",
       "   np.float64(0.41747679002583027),\n",
       "   np.float64(0.4177231304347515),\n",
       "   np.float64(0.41933298483490944),\n",
       "   np.float64(0.41823017224669456),\n",
       "   np.float64(0.4216476110741496),\n",
       "   np.float64(0.4188990229740739),\n",
       "   np.float64(0.4168480411171913),\n",
       "   np.float64(0.4138849303126335),\n",
       "   np.float64(0.4186273608356714),\n",
       "   np.float64(0.4213833548128605),\n",
       "   np.float64(0.4232495781034231),\n",
       "   np.float64(0.4178629796952009),\n",
       "   np.float64(0.4205482183024287),\n",
       "   np.float64(0.42055769823491573),\n",
       "   np.float64(0.4204846080392599),\n",
       "   np.float64(0.4232143759727478),\n",
       "   np.float64(0.41680420842021704),\n",
       "   np.float64(0.41999003291130066),\n",
       "   np.float64(0.4191878456622362),\n",
       "   np.float64(0.42076412960886955),\n",
       "   np.float64(0.42185454815626144),\n",
       "   np.float64(0.4209185130894184),\n",
       "   np.float64(0.4231670554727316),\n",
       "   np.float64(0.41629290394484997),\n",
       "   np.float64(0.41855904925614595),\n",
       "   np.float64(0.4173538815230131),\n",
       "   np.float64(0.41490922030061483),\n",
       "   np.float64(0.41928840428590775),\n",
       "   np.float64(0.4189613927155733),\n",
       "   np.float64(0.42063420079648495),\n",
       "   np.float64(0.42319659516215324),\n",
       "   np.float64(0.41951697412878275),\n",
       "   np.float64(0.4178899433463812),\n",
       "   np.float64(0.4225661847740412),\n",
       "   np.float64(0.41670408099889755),\n",
       "   np.float64(0.4235810525715351),\n",
       "   np.float64(0.41804315336048603),\n",
       "   np.float64(0.4193446505814791),\n",
       "   np.float64(0.42146122362464666),\n",
       "   np.float64(0.41891176998615265),\n",
       "   np.float64(0.4161373032256961),\n",
       "   np.float64(0.41880200430750847),\n",
       "   np.float64(0.4210981912910938),\n",
       "   np.float64(0.42185482010245323),\n",
       "   np.float64(0.418054873123765),\n",
       "   np.float64(0.4154613744467497),\n",
       "   np.float64(0.41873250156641006),\n",
       "   np.float64(0.4185259426012635),\n",
       "   np.float64(0.4229680057615042),\n",
       "   np.float64(0.41985696740448475),\n",
       "   np.float64(0.41202203556895256),\n",
       "   np.float64(0.41944256983697414),\n",
       "   np.float64(0.4159240238368511),\n",
       "   np.float64(0.41905019246041775),\n",
       "   np.float64(0.414385786280036),\n",
       "   np.float64(0.4213847117498517),\n",
       "   np.float64(0.41460167802870274),\n",
       "   np.float64(0.4200084526091814),\n",
       "   np.float64(0.41991453245282173),\n",
       "   np.float64(0.41816721949726343),\n",
       "   np.float64(0.417129946872592),\n",
       "   np.float64(0.41951541416347027),\n",
       "   np.float64(0.4192792857065797),\n",
       "   np.float64(0.41714021936059),\n",
       "   np.float64(0.4152706880122423),\n",
       "   np.float64(0.4173283353447914),\n",
       "   np.float64(0.41872053034603596),\n",
       "   np.float64(0.418591508641839),\n",
       "   np.float64(0.4207930825650692),\n",
       "   np.float64(0.41431825421750546),\n",
       "   np.float64(0.4199746251106262),\n",
       "   np.float64(0.41725030541419983),\n",
       "   np.float64(0.41884080972522497),\n",
       "   np.float64(0.4167711175978184),\n",
       "   np.float64(0.41862309724092484),\n",
       "   np.float64(0.4172586128115654),\n",
       "   np.float64(0.41596233285963535),\n",
       "   np.float64(0.41721615847200155),\n",
       "   np.float64(0.41550606302917004),\n",
       "   np.float64(0.4209825973957777),\n",
       "   np.float64(0.42214206233620644),\n",
       "   np.float64(0.4199972227215767),\n",
       "   np.float64(0.4189605135470629),\n",
       "   np.float64(0.4172546025365591),\n",
       "   np.float64(0.4190371874719858),\n",
       "   np.float64(0.41764810495078564),\n",
       "   np.float64(0.4220174588263035),\n",
       "   np.float64(0.4166896156966686),\n",
       "   np.float64(0.41684649139642715),\n",
       "   np.float64(0.4210465783253312),\n",
       "   np.float64(0.4173285700380802),\n",
       "   np.float64(0.418961551040411),\n",
       "   np.float64(0.41553627513349056),\n",
       "   np.float64(0.420954218134284),\n",
       "   np.float64(0.4197521712630987),\n",
       "   np.float64(0.42051043920218945),\n",
       "   np.float64(0.4173591732978821),\n",
       "   np.float64(0.42138387355953455),\n",
       "   np.float64(0.42158338613808155),\n",
       "   np.float64(0.4197510499507189),\n",
       "   np.float64(0.41731172800064087),\n",
       "   np.float64(0.4175634551793337),\n",
       "   np.float64(0.41898262314498425),\n",
       "   np.float64(0.4190331446006894),\n",
       "   np.float64(0.42008916661143303),\n",
       "   np.float64(0.4199787713587284),\n",
       "   np.float64(0.419626921415329),\n",
       "   np.float64(0.4165096264332533),\n",
       "   np.float64(0.42335825972259045),\n",
       "   np.float64(0.41795365884900093),\n",
       "   np.float64(0.42107037268579006),\n",
       "   np.float64(0.4195181131362915),\n",
       "   np.float64(0.42300451174378395),\n",
       "   np.float64(0.42192550748586655),\n",
       "   np.float64(0.420889550819993),\n",
       "   np.float64(0.42089470848441124),\n",
       "   np.float64(0.42564196325838566),\n",
       "   np.float64(0.4208269454538822),\n",
       "   np.float64(0.4218059815466404),\n",
       "   np.float64(0.41659841500222683),\n",
       "   np.float64(0.4208285305649042),\n",
       "   np.float64(0.4194164574146271),\n",
       "   np.float64(0.41970669478178024),\n",
       "   np.float64(0.4225825257599354),\n",
       "   np.float64(0.42339821718633175),\n",
       "   np.float64(0.4220952522009611),\n",
       "   np.float64(0.42064707167446613),\n",
       "   np.float64(0.4212605971843004),\n",
       "   np.float64(0.4225535746663809),\n",
       "   np.float64(0.4192647933959961),\n",
       "   np.float64(0.42478858679533005),\n",
       "   np.float64(0.421857887879014),\n",
       "   np.float64(0.42073889449238777),\n",
       "   np.float64(0.41973898001015186),\n",
       "   np.float64(0.42027865815907717),\n",
       "   np.float64(0.42383990809321404),\n",
       "   np.float64(0.4245669711381197),\n",
       "   np.float64(0.42189371958374977),\n",
       "   np.float64(0.42273667454719543),\n",
       "   np.float64(0.42183864675462246),\n",
       "   np.float64(0.4199595283716917),\n",
       "   np.float64(0.4205751083791256),\n",
       "   np.float64(0.4210667107254267),\n",
       "   np.float64(0.41734022833406925),\n",
       "   np.float64(0.42499602772295475),\n",
       "   np.float64(0.42362906225025654),\n",
       "   np.float64(0.42733676731586456),\n",
       "   np.float64(0.41982414573431015),\n",
       "   np.float64(0.4201594078913331),\n",
       "   np.float64(0.4187445640563965),\n",
       "   np.float64(0.42437218874692917),\n",
       "   np.float64(0.41944571770727634),\n",
       "   np.float64(0.42104973271489143),\n",
       "   np.float64(0.4223240911960602),\n",
       "   np.float64(0.42102535255253315),\n",
       "   np.float64(0.42046556156128645),\n",
       "   np.float64(0.421932278200984),\n",
       "   np.float64(0.4226820934563875),\n",
       "   np.float64(0.42170677706599236),\n",
       "   np.float64(0.41839723847806454),\n",
       "   np.float64(0.42284462973475456),\n",
       "   np.float64(0.4216548502445221),\n",
       "   np.float64(0.42044571600854397),\n",
       "   np.float64(0.42136948741972446),\n",
       "   np.float64(0.41960063949227333),\n",
       "   np.float64(0.4220293704420328),\n",
       "   np.float64(0.4181215260177851),\n",
       "   np.float64(0.41969567351043224),\n",
       "   np.float64(0.42067985609173775),\n",
       "   np.float64(0.42010238766670227),\n",
       "   np.float64(0.4183812104165554),\n",
       "   np.float64(0.4203035719692707),\n",
       "   np.float64(0.42348637990653515),\n",
       "   np.float64(0.4199929479509592),\n",
       "   np.float64(0.41991799511015415),\n",
       "   np.float64(0.42036065831780434),\n",
       "   np.float64(0.4187926705926657),\n",
       "   np.float64(0.42404797300696373),\n",
       "   np.float64(0.4218321070075035),\n",
       "   np.float64(0.4215190317481756),\n",
       "   np.float64(0.4186656679958105),\n",
       "   np.float64(0.4234452657401562),\n",
       "   np.float64(0.41941128112375736),\n",
       "   np.float64(0.4216918647289276),\n",
       "   np.float64(0.4203540813177824),\n",
       "   np.float64(0.41840696334838867),\n",
       "   np.float64(0.42084570601582527),\n",
       "   np.float64(0.4203292690217495),\n",
       "   np.float64(0.42410336434841156),\n",
       "   np.float64(0.4206170607358217),\n",
       "   np.float64(0.42376057989895344),\n",
       "   np.float64(0.424101235345006),\n",
       "   np.float64(0.4244004115462303),\n",
       "   np.float64(0.4212648905813694),\n",
       "   np.float64(0.4235534090548754),\n",
       "   np.float64(0.42145586758852005),\n",
       "   np.float64(0.42333528958261013),\n",
       "   np.float64(0.42429643124341965),\n",
       "   np.float64(0.4185591973364353),\n",
       "   np.float64(0.42017593793570995),\n",
       "   np.float64(0.4227786846458912),\n",
       "   np.float64(0.42478744871914387),\n",
       "   np.float64(0.42227939143776894),\n",
       "   np.float64(0.42259690165519714),\n",
       "   np.float64(0.42135264724493027),\n",
       "   np.float64(0.42226191237568855),\n",
       "   np.float64(0.4207616504281759),\n",
       "   np.float64(0.41902562603354454),\n",
       "   np.float64(0.42387026734650135),\n",
       "   np.float64(0.4252527914941311),\n",
       "   np.float64(0.42171492241322994),\n",
       "   np.float64(0.42637150548398495),\n",
       "   np.float64(0.4246321488171816),\n",
       "   np.float64(0.4242305029183626),\n",
       "   np.float64(0.4210401512682438)],\n",
       "  'val_affi_coeffs_std': [np.float64(0.07771027961681169),\n",
       "   np.float64(0.07696082873199907),\n",
       "   np.float64(0.07639616036517861),\n",
       "   np.float64(0.07508946605516578),\n",
       "   np.float64(0.07405360867024935),\n",
       "   np.float64(0.070817818879766),\n",
       "   np.float64(0.07099609940607883),\n",
       "   np.float64(0.06979643685741112),\n",
       "   np.float64(0.06923325504357426),\n",
       "   np.float64(0.07047949626424181),\n",
       "   np.float64(0.06903078949872107),\n",
       "   np.float64(0.06937860999409283),\n",
       "   np.float64(0.06965571195432639),\n",
       "   np.float64(0.06912950561717737),\n",
       "   np.float64(0.07096460582323882),\n",
       "   np.float64(0.06908317038420114),\n",
       "   np.float64(0.07016392226446704),\n",
       "   np.float64(0.07128446815971731),\n",
       "   np.float64(0.06934650339775918),\n",
       "   np.float64(0.0708277871647024),\n",
       "   np.float64(0.06907207899120038),\n",
       "   np.float64(0.07077142837889971),\n",
       "   np.float64(0.06750131924768309),\n",
       "   np.float64(0.06793584941157069),\n",
       "   np.float64(0.0746869278913374),\n",
       "   np.float64(0.07143301201489712),\n",
       "   np.float64(0.06865727024647311),\n",
       "   np.float64(0.07038537965170936),\n",
       "   np.float64(0.06818458295401264),\n",
       "   np.float64(0.0703691186945871),\n",
       "   np.float64(0.06897515297025701),\n",
       "   np.float64(0.069250032263758),\n",
       "   np.float64(0.0709059429967979),\n",
       "   np.float64(0.06634317315706571),\n",
       "   np.float64(0.06880264489421083),\n",
       "   np.float64(0.0691217664822317),\n",
       "   np.float64(0.06943844866972691),\n",
       "   np.float64(0.07096788900306197),\n",
       "   np.float64(0.07194423330098272),\n",
       "   np.float64(0.06842316390077852),\n",
       "   np.float64(0.07038382756519497),\n",
       "   np.float64(0.07057786152320594),\n",
       "   np.float64(0.06646352943693523),\n",
       "   np.float64(0.07022473183388889),\n",
       "   np.float64(0.06648168799359784),\n",
       "   np.float64(0.07133790750642066),\n",
       "   np.float64(0.06805830994358446),\n",
       "   np.float64(0.07256570104899399),\n",
       "   np.float64(0.06850363665206399),\n",
       "   np.float64(0.06941105600434674),\n",
       "   np.float64(0.07083062004363047),\n",
       "   np.float64(0.06731070739591205),\n",
       "   np.float64(0.07292476960786737),\n",
       "   np.float64(0.06910353324054071),\n",
       "   np.float64(0.06902076983637837),\n",
       "   np.float64(0.07099113267339442),\n",
       "   np.float64(0.06859070155144027),\n",
       "   np.float64(0.0708092763242558),\n",
       "   np.float64(0.06952676059856322),\n",
       "   np.float64(0.06924165617796528),\n",
       "   np.float64(0.07020113008292586),\n",
       "   np.float64(0.07026143116331404),\n",
       "   np.float64(0.07102188400416787),\n",
       "   np.float64(0.06500091001701891),\n",
       "   np.float64(0.06904339673210355),\n",
       "   np.float64(0.06647315354184179),\n",
       "   np.float64(0.06798691498898297),\n",
       "   np.float64(0.067170329148389),\n",
       "   np.float64(0.06784258642421502),\n",
       "   np.float64(0.06885856491927617),\n",
       "   np.float64(0.06878158657747127),\n",
       "   np.float64(0.06928331637861829),\n",
       "   np.float64(0.06978320799781648),\n",
       "   np.float64(0.06879702561572308),\n",
       "   np.float64(0.06960292229818782),\n",
       "   np.float64(0.06858281422577721),\n",
       "   np.float64(0.06953165648755631),\n",
       "   np.float64(0.06732969777089491),\n",
       "   np.float64(0.06767657844197517),\n",
       "   np.float64(0.07003514494689565),\n",
       "   np.float64(0.0659928915404826),\n",
       "   np.float64(0.06987874647157723),\n",
       "   np.float64(0.06529635617195742),\n",
       "   np.float64(0.06737931691065278),\n",
       "   np.float64(0.06909983013451876),\n",
       "   np.float64(0.06617707754971859),\n",
       "   np.float64(0.06785872238043451),\n",
       "   np.float64(0.06904471486148843),\n",
       "   np.float64(0.06746076359785023),\n",
       "   np.float64(0.06911336737776477),\n",
       "   np.float64(0.06799534656470163),\n",
       "   np.float64(0.06642213963785072),\n",
       "   np.float64(0.06849399160014995),\n",
       "   np.float64(0.06815845480926229),\n",
       "   np.float64(0.06567741109323359),\n",
       "   np.float64(0.06647465840556903),\n",
       "   np.float64(0.07067151658042062),\n",
       "   np.float64(0.06876547588010651),\n",
       "   np.float64(0.07014572849506652),\n",
       "   np.float64(0.06810489063887674),\n",
       "   np.float64(0.06624518575221836),\n",
       "   np.float64(0.07126533412102215),\n",
       "   np.float64(0.06497913372464728),\n",
       "   np.float64(0.06909901650104772),\n",
       "   np.float64(0.06722914712196783),\n",
       "   np.float64(0.06312310807062337),\n",
       "   np.float64(0.06701484445903601),\n",
       "   np.float64(0.06742502671756216),\n",
       "   np.float64(0.06519848762122571),\n",
       "   np.float64(0.06875032343786842),\n",
       "   np.float64(0.06773835971005197),\n",
       "   np.float64(0.06773855605779919),\n",
       "   np.float64(0.06817984367392141),\n",
       "   np.float64(0.06776498527469901),\n",
       "   np.float64(0.06419146132656167),\n",
       "   np.float64(0.0653418240475716),\n",
       "   np.float64(0.06579031254322008),\n",
       "   np.float64(0.06657933445775648),\n",
       "   np.float64(0.06853345907220211),\n",
       "   np.float64(0.06279546058452964),\n",
       "   np.float64(0.062310042152570834),\n",
       "   np.float64(0.06672774338408492),\n",
       "   np.float64(0.06550180981828402),\n",
       "   np.float64(0.06602167412736412),\n",
       "   np.float64(0.06694632129477869),\n",
       "   np.float64(0.06441526857916081),\n",
       "   np.float64(0.06855489597438126),\n",
       "   np.float64(0.06746463809288465),\n",
       "   np.float64(0.059941471635942205),\n",
       "   np.float64(0.06355305957518871),\n",
       "   np.float64(0.0651591514053286),\n",
       "   np.float64(0.06548886688654512),\n",
       "   np.float64(0.06621746865180399),\n",
       "   np.float64(0.06762549129245848),\n",
       "   np.float64(0.06805870783927133),\n",
       "   np.float64(0.06730843745247327),\n",
       "   np.float64(0.06800877464425825),\n",
       "   np.float64(0.06668493433512532),\n",
       "   np.float64(0.0634523537767828),\n",
       "   np.float64(0.06913254942461317),\n",
       "   np.float64(0.06831474297392516),\n",
       "   np.float64(0.06446980084898649),\n",
       "   np.float64(0.06745960140547694),\n",
       "   np.float64(0.06469716374783226),\n",
       "   np.float64(0.06555681334445403),\n",
       "   np.float64(0.06441305784756646),\n",
       "   np.float64(0.06319669780276448),\n",
       "   np.float64(0.06677297829574987),\n",
       "   np.float64(0.06508435977747354),\n",
       "   np.float64(0.06427366126666602),\n",
       "   np.float64(0.06583441233787725),\n",
       "   np.float64(0.06698048643132745),\n",
       "   np.float64(0.06643420297137345),\n",
       "   np.float64(0.06475408315190496),\n",
       "   np.float64(0.0675322679414075),\n",
       "   np.float64(0.06849581263881481),\n",
       "   np.float64(0.06638945261232394),\n",
       "   np.float64(0.0662775135801724),\n",
       "   np.float64(0.06854888087969054),\n",
       "   np.float64(0.06639729025727083),\n",
       "   np.float64(0.06522831798368514),\n",
       "   np.float64(0.06697237686864663),\n",
       "   np.float64(0.07004804493833013),\n",
       "   np.float64(0.0680364467219472),\n",
       "   np.float64(0.06731656396146093),\n",
       "   np.float64(0.06700822333417182),\n",
       "   np.float64(0.06806958471055166),\n",
       "   np.float64(0.06731719293862236),\n",
       "   np.float64(0.06608190309983647),\n",
       "   np.float64(0.06597580983589679),\n",
       "   np.float64(0.06846856284624842),\n",
       "   np.float64(0.06579468934748618),\n",
       "   np.float64(0.06718818569664955),\n",
       "   np.float64(0.06363921580603443),\n",
       "   np.float64(0.06725517936460938),\n",
       "   np.float64(0.06789780879285667),\n",
       "   np.float64(0.06566785674341694),\n",
       "   np.float64(0.06454287193864627),\n",
       "   np.float64(0.06732440347719157),\n",
       "   np.float64(0.06937427332184311),\n",
       "   np.float64(0.06706695670071293),\n",
       "   np.float64(0.06849200996443065),\n",
       "   np.float64(0.06748981350804983),\n",
       "   np.float64(0.06606718904108667),\n",
       "   np.float64(0.06701855344973832),\n",
       "   np.float64(0.06435500263172352),\n",
       "   np.float64(0.06763240726779375),\n",
       "   np.float64(0.06694524517119074),\n",
       "   np.float64(0.06222984107242878),\n",
       "   np.float64(0.06796150263470177),\n",
       "   np.float64(0.06669308171028154),\n",
       "   np.float64(0.06534161710936108),\n",
       "   np.float64(0.06547057056627387),\n",
       "   np.float64(0.06730718595235907),\n",
       "   np.float64(0.06545173553561083),\n",
       "   np.float64(0.06526825439058606),\n",
       "   np.float64(0.06612128224523199),\n",
       "   np.float64(0.06320593459509184),\n",
       "   np.float64(0.06535947124240439),\n",
       "   np.float64(0.06526315796848481),\n",
       "   np.float64(0.06559602856361056),\n",
       "   np.float64(0.062397116016068586),\n",
       "   np.float64(0.06609466472723509),\n",
       "   np.float64(0.06514822965137178),\n",
       "   np.float64(0.06420340336585548),\n",
       "   np.float64(0.066563079631343),\n",
       "   np.float64(0.06448867868028603),\n",
       "   np.float64(0.06317456890307929),\n",
       "   np.float64(0.06655086028051724),\n",
       "   np.float64(0.0672752215351444),\n",
       "   np.float64(0.0660762548391292),\n",
       "   np.float64(0.06607255065387584),\n",
       "   np.float64(0.064008145284925),\n",
       "   np.float64(0.06327205968899989),\n",
       "   np.float64(0.0673150744977258),\n",
       "   np.float64(0.06823925859982834),\n",
       "   np.float64(0.06671754971921072),\n",
       "   np.float64(0.06743788795503033),\n",
       "   np.float64(0.06553912823268881),\n",
       "   np.float64(0.06255234305407661),\n",
       "   np.float64(0.06621807977922928),\n",
       "   np.float64(0.06681107472268756),\n",
       "   np.float64(0.06566212807946378),\n",
       "   np.float64(0.0684586921422112),\n",
       "   np.float64(0.06434376414913832),\n",
       "   np.float64(0.06415815663899695),\n",
       "   np.float64(0.06787891925550396),\n",
       "   np.float64(0.06382524453068553),\n",
       "   np.float64(0.06775295235842939),\n",
       "   np.float64(0.06499255793915179),\n",
       "   np.float64(0.06499635393030333),\n",
       "   np.float64(0.061259320075298405),\n",
       "   np.float64(0.06557010318407495),\n",
       "   np.float64(0.06922772984886756),\n",
       "   np.float64(0.06861387150318299),\n",
       "   np.float64(0.0650277212100499),\n",
       "   np.float64(0.06333314080811568),\n",
       "   np.float64(0.06833727229724927),\n",
       "   np.float64(0.06705969137839714),\n",
       "   np.float64(0.06471173323248235),\n",
       "   np.float64(0.06321887148881002),\n",
       "   np.float64(0.06295090964232494),\n",
       "   np.float64(0.06631736968786878),\n",
       "   np.float64(0.06532406696406703),\n",
       "   np.float64(0.06455907371083941),\n",
       "   np.float64(0.06681452971801907),\n",
       "   np.float64(0.06635186492244612),\n",
       "   np.float64(0.06197255656676019),\n",
       "   np.float64(0.06673666831831726),\n",
       "   np.float64(0.06783008586750316),\n",
       "   np.float64(0.06524891304731213),\n",
       "   np.float64(0.06557957408147697),\n",
       "   np.float64(0.06418943460285424),\n",
       "   np.float64(0.0677287095692441),\n",
       "   np.float64(0.06389366836548213),\n",
       "   np.float64(0.06487525066091124),\n",
       "   np.float64(0.06642198048305925),\n",
       "   np.float64(0.06435707943143519),\n",
       "   np.float64(0.062287476010402035),\n",
       "   np.float64(0.06769377840426359),\n",
       "   np.float64(0.06462623335469661),\n",
       "   np.float64(0.06760414710035298),\n",
       "   np.float64(0.06753966473863585),\n",
       "   np.float64(0.06429079679895344),\n",
       "   np.float64(0.06399726119073476),\n",
       "   np.float64(0.06678986338676737),\n",
       "   np.float64(0.0670212132599543),\n",
       "   np.float64(0.06483546070049315),\n",
       "   np.float64(0.06698218390968749),\n",
       "   np.float64(0.06819192189136804),\n",
       "   np.float64(0.06653927609055582),\n",
       "   np.float64(0.06356438528824158),\n",
       "   np.float64(0.06767768147597893),\n",
       "   np.float64(0.06780996857244105),\n",
       "   np.float64(0.06558370196513125),\n",
       "   np.float64(0.06605634538459122),\n",
       "   np.float64(0.06565218354862075),\n",
       "   np.float64(0.06520128063096815),\n",
       "   np.float64(0.07066360436585076),\n",
       "   np.float64(0.06428897589949362),\n",
       "   np.float64(0.06770902057474368),\n",
       "   np.float64(0.06742261456539113),\n",
       "   np.float64(0.06536244121749811),\n",
       "   np.float64(0.06785245385980584),\n",
       "   np.float64(0.0651754548375186),\n",
       "   np.float64(0.06480013851194236),\n",
       "   np.float64(0.06596019184129677),\n",
       "   np.float64(0.06235182734023143),\n",
       "   np.float64(0.0658415135318752),\n",
       "   np.float64(0.06445886830362095),\n",
       "   np.float64(0.06481257711132676),\n",
       "   np.float64(0.06485861788422446),\n",
       "   np.float64(0.06172522275750814),\n",
       "   np.float64(0.06603052145532069),\n",
       "   np.float64(0.06819281795872034),\n",
       "   np.float64(0.06401357420049972),\n",
       "   np.float64(0.06478433894679628),\n",
       "   np.float64(0.06489453556037075),\n",
       "   np.float64(0.06360256555952831),\n",
       "   np.float64(0.06626512433478186),\n",
       "   np.float64(0.06846983387034523),\n",
       "   np.float64(0.06749347701645383),\n",
       "   np.float64(0.06709111605157247),\n",
       "   np.float64(0.06487409824572497),\n",
       "   np.float64(0.06967193997040443),\n",
       "   np.float64(0.06617528415186377),\n",
       "   np.float64(0.06352147933116443),\n",
       "   np.float64(0.06596336901946825),\n",
       "   np.float64(0.06553370025571154),\n",
       "   np.float64(0.06321823713317629),\n",
       "   np.float64(0.06220458125430967),\n",
       "   np.float64(0.06532638804534992),\n",
       "   np.float64(0.06379633090405047),\n",
       "   np.float64(0.06357981572258077),\n",
       "   np.float64(0.06381622575108967),\n",
       "   np.float64(0.06616518829157705),\n",
       "   np.float64(0.06151817760934228),\n",
       "   np.float64(0.06459884224960069),\n",
       "   np.float64(0.06746172989797593),\n",
       "   np.float64(0.0661752608606887),\n",
       "   np.float64(0.06240317615179358),\n",
       "   np.float64(0.06295503810360924),\n",
       "   np.float64(0.06676484253125024),\n",
       "   np.float64(0.06350748421954548),\n",
       "   np.float64(0.06395411605709231),\n",
       "   np.float64(0.0649170360222021),\n",
       "   np.float64(0.06464541993136999),\n",
       "   np.float64(0.06377441426080799),\n",
       "   np.float64(0.06866640986024103),\n",
       "   np.float64(0.06381226074369582),\n",
       "   np.float64(0.0657155024187538),\n",
       "   np.float64(0.06265256643981079),\n",
       "   np.float64(0.06505152130791227),\n",
       "   np.float64(0.0639049250033303),\n",
       "   np.float64(0.06435599476155969),\n",
       "   np.float64(0.063369044627783),\n",
       "   np.float64(0.06562232043242647),\n",
       "   np.float64(0.0628614153302847),\n",
       "   np.float64(0.06473366618080116),\n",
       "   np.float64(0.06409629602176968),\n",
       "   np.float64(0.06715047591746952),\n",
       "   np.float64(0.06348127280371119),\n",
       "   np.float64(0.06309268587807952),\n",
       "   np.float64(0.06258089758132719),\n",
       "   np.float64(0.06310337021609795),\n",
       "   np.float64(0.06599763931652264),\n",
       "   np.float64(0.06586876141734797),\n",
       "   np.float64(0.06475003143665425),\n",
       "   np.float64(0.06606642953667671),\n",
       "   np.float64(0.06740770553198137),\n",
       "   np.float64(0.06666410233350327),\n",
       "   np.float64(0.06339035965103948),\n",
       "   np.float64(0.06393617518307378),\n",
       "   np.float64(0.06628271497206123),\n",
       "   np.float64(0.06717824054132338),\n",
       "   np.float64(0.0642411645260071),\n",
       "   np.float64(0.06712111888866529),\n",
       "   np.float64(0.06351018565839185),\n",
       "   np.float64(0.06470331172090252),\n",
       "   np.float64(0.06469520687213054),\n",
       "   np.float64(0.06544725698882038),\n",
       "   np.float64(0.0655245974923815),\n",
       "   np.float64(0.06463014448954978),\n",
       "   np.float64(0.06558296191567999),\n",
       "   np.float64(0.06248111475629001),\n",
       "   np.float64(0.06443509614761019),\n",
       "   np.float64(0.06406934219563763),\n",
       "   np.float64(0.06447787148548749),\n",
       "   np.float64(0.06607532148816551),\n",
       "   np.float64(0.067742335280492),\n",
       "   np.float64(0.06690204331887543),\n",
       "   np.float64(0.06712568830073871),\n",
       "   np.float64(0.06547017652856679),\n",
       "   np.float64(0.06587005573215204),\n",
       "   np.float64(0.06382366163711815),\n",
       "   np.float64(0.06617836993463538),\n",
       "   np.float64(0.06453746389365601),\n",
       "   np.float64(0.06473547734561354),\n",
       "   np.float64(0.06494416157412013),\n",
       "   np.float64(0.06292250558571444),\n",
       "   np.float64(0.06495269788154641),\n",
       "   np.float64(0.06496565161776836),\n",
       "   np.float64(0.06721193166730781),\n",
       "   np.float64(0.06272900038838773),\n",
       "   np.float64(0.0635084739113839),\n",
       "   np.float64(0.0664644863997079),\n",
       "   np.float64(0.06584816199650903),\n",
       "   np.float64(0.06876075510429348),\n",
       "   np.float64(0.06444929705226522),\n",
       "   np.float64(0.06362175181030737),\n",
       "   np.float64(0.06474028368578805),\n",
       "   np.float64(0.06554246478156582),\n",
       "   np.float64(0.06496595450054128),\n",
       "   np.float64(0.06427003831610878),\n",
       "   np.float64(0.06643451137963186),\n",
       "   np.float64(0.06335749643550423),\n",
       "   np.float64(0.0666688246803134),\n",
       "   np.float64(0.06524239642424476),\n",
       "   np.float64(0.06378885855357919),\n",
       "   np.float64(0.06300701141037916),\n",
       "   np.float64(0.06479858218414436),\n",
       "   np.float64(0.06387887519198551),\n",
       "   np.float64(0.06410283101633642),\n",
       "   np.float64(0.06625999012147722),\n",
       "   np.float64(0.06742827471975645),\n",
       "   np.float64(0.06142787132953592),\n",
       "   np.float64(0.06404913202497736),\n",
       "   np.float64(0.06342104841840125),\n",
       "   np.float64(0.06395731242009402),\n",
       "   np.float64(0.06354130380329015),\n",
       "   np.float64(0.0628870819165554),\n",
       "   np.float64(0.06484751023313429),\n",
       "   np.float64(0.06448754203886897),\n",
       "   np.float64(0.06321213109304603),\n",
       "   np.float64(0.06466814651053598),\n",
       "   np.float64(0.06515094115467886),\n",
       "   np.float64(0.06291929971997565),\n",
       "   np.float64(0.06533383571263976),\n",
       "   np.float64(0.06341135435799918),\n",
       "   np.float64(0.06527961068239004),\n",
       "   np.float64(0.06104823518552293),\n",
       "   np.float64(0.0626928741403115),\n",
       "   np.float64(0.06460394986866527),\n",
       "   np.float64(0.06053349069698913),\n",
       "   np.float64(0.062466589406619935),\n",
       "   np.float64(0.06625195225057703),\n",
       "   np.float64(0.06362648475092733),\n",
       "   np.float64(0.06307761181022349),\n",
       "   np.float64(0.06314222667916695),\n",
       "   np.float64(0.0668499667915434),\n",
       "   np.float64(0.06294231057517392),\n",
       "   np.float64(0.0621403224681136),\n",
       "   np.float64(0.06414903003033208),\n",
       "   np.float64(0.06494127517099507),\n",
       "   np.float64(0.06376909871426252),\n",
       "   np.float64(0.06305671174934949),\n",
       "   np.float64(0.06437864353438928)],\n",
       "  'val_atom_loss_mean': [np.float64(0.48616435937583447),\n",
       "   np.float64(0.3575477618724108),\n",
       "   np.float64(0.2601267332211137),\n",
       "   np.float64(0.18480060808360577),\n",
       "   np.float64(0.1393530322238803),\n",
       "   np.float64(0.11676895106211305),\n",
       "   np.float64(0.0957238688133657),\n",
       "   np.float64(0.08366759866476059),\n",
       "   np.float64(0.07242207136005163),\n",
       "   np.float64(0.06500174151733518),\n",
       "   np.float64(0.05490927095524967),\n",
       "   np.float64(0.056048531318083405),\n",
       "   np.float64(0.047600809251889586),\n",
       "   np.float64(0.04352059098891914),\n",
       "   np.float64(0.040320687694475055),\n",
       "   np.float64(0.03970696707256138),\n",
       "   np.float64(0.04148751450702548),\n",
       "   np.float64(0.043149482225999236),\n",
       "   np.float64(0.04370038234628737),\n",
       "   np.float64(0.03947816719301045),\n",
       "   np.float64(0.037880580173805356),\n",
       "   np.float64(0.03516535693779588),\n",
       "   np.float64(0.03933340753428638),\n",
       "   np.float64(0.0372866615653038),\n",
       "   np.float64(0.0353344640461728),\n",
       "   np.float64(0.036059044068679214),\n",
       "   np.float64(0.04234951105900109),\n",
       "   np.float64(0.03407912061084062),\n",
       "   np.float64(0.03732922486960888),\n",
       "   np.float64(0.03553230327088386),\n",
       "   np.float64(0.03294252080377191),\n",
       "   np.float64(0.0347879669861868),\n",
       "   np.float64(0.033607253804802895),\n",
       "   np.float64(0.032655554823577404),\n",
       "   np.float64(0.03618989326059818),\n",
       "   np.float64(0.0345741705968976),\n",
       "   np.float64(0.03480410494375974),\n",
       "   np.float64(0.030343919759616256),\n",
       "   np.float64(0.034630328766070306),\n",
       "   np.float64(0.03515108325518668),\n",
       "   np.float64(0.03287717362400144),\n",
       "   np.float64(0.03227481141220778),\n",
       "   np.float64(0.037803978426381946),\n",
       "   np.float64(0.034695970360189676),\n",
       "   np.float64(0.03628568642307073),\n",
       "   np.float64(0.03565112629439682),\n",
       "   np.float64(0.034802744863554835),\n",
       "   np.float64(0.03621128806844354),\n",
       "   np.float64(0.03386977198533714),\n",
       "   np.float64(0.029728386783972383),\n",
       "   np.float64(0.028528919094242156),\n",
       "   np.float64(0.02948497177567333),\n",
       "   np.float64(0.03450418193824589),\n",
       "   np.float64(0.026778438594192266),\n",
       "   np.float64(0.02714879484847188),\n",
       "   np.float64(0.028882201528176665),\n",
       "   np.float64(0.028751524281688035),\n",
       "   np.float64(0.03008870896883309),\n",
       "   np.float64(0.028218117309734225),\n",
       "   np.float64(0.028325938736088574),\n",
       "   np.float64(0.025814450811594725),\n",
       "   np.float64(0.027477787691168487),\n",
       "   np.float64(0.027512683649547398),\n",
       "   np.float64(0.027176585514098406),\n",
       "   np.float64(0.028127800906077027),\n",
       "   np.float64(0.028966874233447015),\n",
       "   np.float64(0.02638104057405144),\n",
       "   np.float64(0.02482655760832131),\n",
       "   np.float64(0.026244744542054832),\n",
       "   np.float64(0.025171208893880248),\n",
       "   np.float64(0.02570656524039805),\n",
       "   np.float64(0.02219282102305442),\n",
       "   np.float64(0.02296724240295589),\n",
       "   np.float64(0.025833843392319977),\n",
       "   np.float64(0.02621708030346781),\n",
       "   np.float64(0.024724650313146412),\n",
       "   np.float64(0.02282678766641766),\n",
       "   np.float64(0.020748335868120193),\n",
       "   np.float64(0.019832632853649557),\n",
       "   np.float64(0.0262505920836702),\n",
       "   np.float64(0.02194642531685531),\n",
       "   np.float64(0.022862174082547426),\n",
       "   np.float64(0.019098054035566747),\n",
       "   np.float64(0.021573578473180532),\n",
       "   np.float64(0.018983751127962023),\n",
       "   np.float64(0.021007253206335008),\n",
       "   np.float64(0.020392167440149933),\n",
       "   np.float64(0.02116140362340957),\n",
       "   np.float64(0.01994239422492683),\n",
       "   np.float64(0.020793266128748655),\n",
       "   np.float64(0.020782092586159706),\n",
       "   np.float64(0.022335172281600535),\n",
       "   np.float64(0.02002882765373215),\n",
       "   np.float64(0.02132056950358674),\n",
       "   np.float64(0.019934867857955396),\n",
       "   np.float64(0.02002492581959814),\n",
       "   np.float64(0.01962007296970114),\n",
       "   np.float64(0.01878316409420222),\n",
       "   np.float64(0.0187428563949652),\n",
       "   np.float64(0.018722181324847043),\n",
       "   np.float64(0.019595258578192443),\n",
       "   np.float64(0.020640400471165776),\n",
       "   np.float64(0.017741088871844113),\n",
       "   np.float64(0.018632933613844216),\n",
       "   np.float64(0.018074991123285145),\n",
       "   np.float64(0.021044462802819908),\n",
       "   np.float64(0.018892096704803407),\n",
       "   np.float64(0.015168805606663227),\n",
       "   np.float64(0.01823913783300668),\n",
       "   np.float64(0.019382957078050822),\n",
       "   np.float64(0.016555910231545568),\n",
       "   np.float64(0.016543153033126146),\n",
       "   np.float64(0.016010664228815585),\n",
       "   np.float64(0.015142397780437022),\n",
       "   np.float64(0.01676717703230679),\n",
       "   np.float64(0.015553318138699979),\n",
       "   np.float64(0.015615041134878993),\n",
       "   np.float64(0.0166748475166969),\n",
       "   np.float64(0.015913094801362604),\n",
       "   np.float64(0.01641471340553835),\n",
       "   np.float64(0.014749024529010057),\n",
       "   np.float64(0.015343983308412135),\n",
       "   np.float64(0.014937791042029858),\n",
       "   np.float64(0.014469294575974345),\n",
       "   np.float64(0.015424941957462579),\n",
       "   np.float64(0.01380882819648832),\n",
       "   np.float64(0.016102818655781448),\n",
       "   np.float64(0.01602561998879537),\n",
       "   np.float64(0.013215211394708604),\n",
       "   np.float64(0.01601615030085668),\n",
       "   np.float64(0.015260863990988582),\n",
       "   np.float64(0.01415672292932868),\n",
       "   np.float64(0.01460377621697262),\n",
       "   np.float64(0.015034894226118922),\n",
       "   np.float64(0.014491042529698461),\n",
       "   np.float64(0.013926695275586098),\n",
       "   np.float64(0.0123835121630691),\n",
       "   np.float64(0.014349192788358778),\n",
       "   np.float64(0.013829185860231519),\n",
       "   np.float64(0.014413939788937569),\n",
       "   np.float64(0.01334685884648934),\n",
       "   np.float64(0.01458514848491177),\n",
       "   np.float64(0.012558598013129085),\n",
       "   np.float64(0.014556393900420517),\n",
       "   np.float64(0.013252062897663563),\n",
       "   np.float64(0.012858732079621404),\n",
       "   np.float64(0.012900342262582853),\n",
       "   np.float64(0.013119631330482662),\n",
       "   np.float64(0.014114618767052889),\n",
       "   np.float64(0.01335406064754352),\n",
       "   np.float64(0.014133712335024029),\n",
       "   np.float64(0.012794222449883819),\n",
       "   np.float64(0.01296287588775158),\n",
       "   np.float64(0.012390448449878022),\n",
       "   np.float64(0.011855161399580538),\n",
       "   np.float64(0.012353164609521627),\n",
       "   np.float64(0.011892098176758736),\n",
       "   np.float64(0.013780917041003704),\n",
       "   np.float64(0.013335926865693182),\n",
       "   np.float64(0.013579227845184505),\n",
       "   np.float64(0.01368446444394067),\n",
       "   np.float64(0.013881045975722373),\n",
       "   np.float64(0.0136399210896343),\n",
       "   np.float64(0.013581329898443073),\n",
       "   np.float64(0.012434685719199479),\n",
       "   np.float64(0.012060864886734635),\n",
       "   np.float64(0.012878642737632617),\n",
       "   np.float64(0.014129375398624688),\n",
       "   np.float64(0.013823186047375202),\n",
       "   np.float64(0.013235579244792461),\n",
       "   np.float64(0.01297694817185402),\n",
       "   np.float64(0.013237760693300515),\n",
       "   np.float64(0.012266134202945977),\n",
       "   np.float64(0.013019551057368517),\n",
       "   np.float64(0.012850587198045105),\n",
       "   np.float64(0.012151535251177847),\n",
       "   np.float64(0.012911039055325091),\n",
       "   np.float64(0.011132373940199614),\n",
       "   np.float64(0.011109464889159426),\n",
       "   np.float64(0.012012393970508128),\n",
       "   np.float64(0.013340151170268655),\n",
       "   np.float64(0.013111289765220135),\n",
       "   np.float64(0.011788273346610367),\n",
       "   np.float64(0.011294661031570286),\n",
       "   np.float64(0.01084573520347476),\n",
       "   np.float64(0.0122430837363936),\n",
       "   np.float64(0.011263348307693377),\n",
       "   np.float64(0.009574149298714474),\n",
       "   np.float64(0.009034802031237632),\n",
       "   np.float64(0.010206038830801845),\n",
       "   np.float64(0.010541735973674804),\n",
       "   np.float64(0.011159757734276354),\n",
       "   np.float64(0.009986761724576354),\n",
       "   np.float64(0.010770771186798811),\n",
       "   np.float64(0.010067434137454256),\n",
       "   np.float64(0.008930540061555803),\n",
       "   np.float64(0.00853878865018487),\n",
       "   np.float64(0.009488302195677534),\n",
       "   np.float64(0.009810643299715593),\n",
       "   np.float64(0.00974886491894722),\n",
       "   np.float64(0.008520817384123802),\n",
       "   np.float64(0.009293538081692532),\n",
       "   np.float64(0.0097268185345456),\n",
       "   np.float64(0.008597344072768465),\n",
       "   np.float64(0.010653010889654979),\n",
       "   np.float64(0.009055369708221406),\n",
       "   np.float64(0.009343705838546157),\n",
       "   np.float64(0.008770832879235968),\n",
       "   np.float64(0.01101888474659063),\n",
       "   np.float64(0.008856273663695902),\n",
       "   np.float64(0.008711480913916603),\n",
       "   np.float64(0.00920470681739971),\n",
       "   np.float64(0.009311333356890827),\n",
       "   np.float64(0.008565161493606865),\n",
       "   np.float64(0.0082596501451917),\n",
       "   np.float64(0.008352366829058155),\n",
       "   np.float64(0.008744271472096443),\n",
       "   np.float64(0.00812406386830844),\n",
       "   np.float64(0.00936175073729828),\n",
       "   np.float64(0.00828978480421938),\n",
       "   np.float64(0.008614361810032278),\n",
       "   np.float64(0.008712179347639903),\n",
       "   np.float64(0.008484684949507937),\n",
       "   np.float64(0.009275762975448743),\n",
       "   np.float64(0.008419957011938095),\n",
       "   np.float64(0.00917472515720874),\n",
       "   np.float64(0.007959668000694364),\n",
       "   np.float64(0.008278224850073457),\n",
       "   np.float64(0.00879997870652005),\n",
       "   np.float64(0.009073843277292326),\n",
       "   np.float64(0.008530629245797172),\n",
       "   np.float64(0.008757978823268786),\n",
       "   np.float64(0.00921939715044573),\n",
       "   np.float64(0.00787879916606471),\n",
       "   np.float64(0.009848140995018184),\n",
       "   np.float64(0.009325787978013977),\n",
       "   np.float64(0.008771323336986825),\n",
       "   np.float64(0.0072752381092868745),\n",
       "   np.float64(0.008920176362153143),\n",
       "   np.float64(0.009503125620540231),\n",
       "   np.float64(0.008428917702985927),\n",
       "   np.float64(0.008642312430310994),\n",
       "   np.float64(0.007987146760569885),\n",
       "   np.float64(0.007883668498834595),\n",
       "   np.float64(0.007651435531442985),\n",
       "   np.float64(0.0070333882758859545),\n",
       "   np.float64(0.00811576918931678),\n",
       "   np.float64(0.007819133781595156),\n",
       "   np.float64(0.009234232915332541),\n",
       "   np.float64(0.007742340269032866),\n",
       "   np.float64(0.007943392032757401),\n",
       "   np.float64(0.007176651561167091),\n",
       "   np.float64(0.0071349025529343635),\n",
       "   np.float64(0.0076605413923971355),\n",
       "   np.float64(0.00736728121410124),\n",
       "   np.float64(0.006823104078648612),\n",
       "   np.float64(0.006547292956383899),\n",
       "   np.float64(0.007563222985481843),\n",
       "   np.float64(0.00682104891166091),\n",
       "   np.float64(0.0072982601705007255),\n",
       "   np.float64(0.007879797572968528),\n",
       "   np.float64(0.007022223551757634),\n",
       "   np.float64(0.007524963584728539),\n",
       "   np.float64(0.00770775499404408),\n",
       "   np.float64(0.007517091347835958),\n",
       "   np.float64(0.007229363778606057),\n",
       "   np.float64(0.006571783480467275),\n",
       "   np.float64(0.007371611864073202),\n",
       "   np.float64(0.007503669825382531),\n",
       "   np.float64(0.006623941560974345),\n",
       "   np.float64(0.006438343363697641),\n",
       "   np.float64(0.006255746615352109),\n",
       "   np.float64(0.006109031004598364),\n",
       "   np.float64(0.006491297448519617),\n",
       "   np.float64(0.007038266281597316),\n",
       "   np.float64(0.0061327551666181535),\n",
       "   np.float64(0.006893304554978386),\n",
       "   np.float64(0.006037066545104608),\n",
       "   np.float64(0.007094424945535138),\n",
       "   np.float64(0.006729640677804127),\n",
       "   np.float64(0.007136512052966282),\n",
       "   np.float64(0.005861653917236254),\n",
       "   np.float64(0.005904477016883902),\n",
       "   np.float64(0.006378798629157245),\n",
       "   np.float64(0.00651866290718317),\n",
       "   np.float64(0.006259056914132088),\n",
       "   np.float64(0.005329454463208094),\n",
       "   np.float64(0.006266826356295496),\n",
       "   np.float64(0.005383360563428141),\n",
       "   np.float64(0.005146370574948378),\n",
       "   np.float64(0.005235000804532319),\n",
       "   np.float64(0.006046694237738848),\n",
       "   np.float64(0.006179085728945211),\n",
       "   np.float64(0.005548264569370076),\n",
       "   np.float64(0.005685789059498347),\n",
       "   np.float64(0.006496116620837711),\n",
       "   np.float64(0.0059407980006653816),\n",
       "   np.float64(0.006397571094566956),\n",
       "   np.float64(0.005886151397135109),\n",
       "   np.float64(0.00607111475255806),\n",
       "   np.float64(0.0052005678589921445),\n",
       "   np.float64(0.005570802706643008),\n",
       "   np.float64(0.0057728608371689916),\n",
       "   np.float64(0.006321373424725607),\n",
       "   np.float64(0.005740123873692937),\n",
       "   np.float64(0.0062033082358539104),\n",
       "   np.float64(0.0053541176312137395),\n",
       "   np.float64(0.005228297537541948),\n",
       "   np.float64(0.005534217518288642),\n",
       "   np.float64(0.005976599699351937),\n",
       "   np.float64(0.004857321357121691),\n",
       "   np.float64(0.0049158828624058515),\n",
       "   np.float64(0.005313164248946123),\n",
       "   np.float64(0.005512575793545693),\n",
       "   np.float64(0.005185909933061339),\n",
       "   np.float64(0.006047883434803225),\n",
       "   np.float64(0.004932510215439834),\n",
       "   np.float64(0.004991824869648553),\n",
       "   np.float64(0.00539491802919656),\n",
       "   np.float64(0.005301504716044292),\n",
       "   np.float64(0.005205652298172936),\n",
       "   np.float64(0.005247647291980684),\n",
       "   np.float64(0.005211263531236909),\n",
       "   np.float64(0.005563090438954532),\n",
       "   np.float64(0.0050293636886635795),\n",
       "   np.float64(0.0048307073302567005),\n",
       "   np.float64(0.005450066935736686),\n",
       "   np.float64(0.0055854555539553985),\n",
       "   np.float64(0.005701335248886608),\n",
       "   np.float64(0.005114172789035365),\n",
       "   np.float64(0.0046086579823167995),\n",
       "   np.float64(0.004422582263941877),\n",
       "   np.float64(0.0057805888645816594),\n",
       "   np.float64(0.005430906981928274),\n",
       "   np.float64(0.004992295958800241),\n",
       "   np.float64(0.004592366894939914),\n",
       "   np.float64(0.004919153536320664),\n",
       "   np.float64(0.0051961359567940235),\n",
       "   np.float64(0.004825517869903706),\n",
       "   np.float64(0.00508829444879666),\n",
       "   np.float64(0.005253947572782636),\n",
       "   np.float64(0.004478816976188682),\n",
       "   np.float64(0.004532205712166615),\n",
       "   np.float64(0.004545566771412268),\n",
       "   np.float64(0.0040633526659803465),\n",
       "   np.float64(0.004521245355135761),\n",
       "   np.float64(0.004174252011580393),\n",
       "   np.float64(0.00399161122913938),\n",
       "   np.float64(0.004089717331225984),\n",
       "   np.float64(0.004691998823545873),\n",
       "   np.float64(0.0041010887653101236),\n",
       "   np.float64(0.005008588472264819),\n",
       "   np.float64(0.0038243676972342655),\n",
       "   np.float64(0.004177225433522835),\n",
       "   np.float64(0.00497605356213171),\n",
       "   np.float64(0.0044754072005162016),\n",
       "   np.float64(0.004343858017819002),\n",
       "   np.float64(0.004108358232770115),\n",
       "   np.float64(0.004357493089628406),\n",
       "   np.float64(0.0038668178603984416),\n",
       "   np.float64(0.004328877999796532),\n",
       "   np.float64(0.004336096142651513),\n",
       "   np.float64(0.004309743097110186),\n",
       "   np.float64(0.003977852888056077),\n",
       "   np.float64(0.004577201732899994),\n",
       "   np.float64(0.004432982561411336),\n",
       "   np.float64(0.004679592224420048),\n",
       "   np.float64(0.003923864423995838),\n",
       "   np.float64(0.004266494550392963),\n",
       "   np.float64(0.004107449596631341),\n",
       "   np.float64(0.004238518493366428),\n",
       "   np.float64(0.0038907232228666544),\n",
       "   np.float64(0.0036909039044985548),\n",
       "   np.float64(0.004181871423497796),\n",
       "   np.float64(0.0035743869811994955),\n",
       "   np.float64(0.003720849817909766),\n",
       "   np.float64(0.0032825355738168582),\n",
       "   np.float64(0.004048802918987349),\n",
       "   np.float64(0.0038874218007549644),\n",
       "   np.float64(0.0034204435942228884),\n",
       "   np.float64(0.004148930820520036),\n",
       "   np.float64(0.003809275003732182),\n",
       "   np.float64(0.003936312030418776),\n",
       "   np.float64(0.003336644353112206),\n",
       "   np.float64(0.003672950726468116),\n",
       "   np.float64(0.004438323187059723),\n",
       "   np.float64(0.003817182790953666),\n",
       "   np.float64(0.0038677058037137613),\n",
       "   np.float64(0.0038471297011710703),\n",
       "   np.float64(0.003709058612002991),\n",
       "   np.float64(0.0037610869621858),\n",
       "   np.float64(0.0036305670510046184),\n",
       "   np.float64(0.0035256129631306976),\n",
       "   np.float64(0.0037256948999129236),\n",
       "   np.float64(0.0038172054337337613),\n",
       "   np.float64(0.0038533689366886392),\n",
       "   np.float64(0.0032851611758815125),\n",
       "   np.float64(0.003830684261629358),\n",
       "   np.float64(0.003485537235974334),\n",
       "   np.float64(0.003527756591211073),\n",
       "   np.float64(0.003492236886813771),\n",
       "   np.float64(0.0037745365116279572),\n",
       "   np.float64(0.003439253763644956),\n",
       "   np.float64(0.0029603855109598953),\n",
       "   np.float64(0.003413110156543553),\n",
       "   np.float64(0.00364550064841751),\n",
       "   np.float64(0.00331035943236202),\n",
       "   np.float64(0.003145171533105895),\n",
       "   np.float64(0.0031416160345543176),\n",
       "   np.float64(0.002634709540870972),\n",
       "   np.float64(0.0036606906287488528),\n",
       "   np.float64(0.0033387053990736604),\n",
       "   np.float64(0.003546728170476854),\n",
       "   np.float64(0.0032042220191215165),\n",
       "   np.float64(0.003985959046985954),\n",
       "   np.float64(0.0028410359445842914),\n",
       "   np.float64(0.002989117114339024),\n",
       "   np.float64(0.0035342788687557913),\n",
       "   np.float64(0.00342489400645718),\n",
       "   np.float64(0.003061691124457866),\n",
       "   np.float64(0.003238434743252583),\n",
       "   np.float64(0.00307405289640883),\n",
       "   np.float64(0.0033901546557899565),\n",
       "   np.float64(0.0035799035758827813),\n",
       "   np.float64(0.0034051230177283287),\n",
       "   np.float64(0.0035322000621818006),\n",
       "   np.float64(0.00349561964685563),\n",
       "   np.float64(0.0035899894719477743),\n",
       "   np.float64(0.0032819519547047094),\n",
       "   np.float64(0.002875364363717381),\n",
       "   np.float64(0.0030744278119527735),\n",
       "   np.float64(0.0033906296812347136),\n",
       "   np.float64(0.0030995126799098216),\n",
       "   np.float64(0.0030954322282923386),\n",
       "   np.float64(0.0031318879628088325),\n",
       "   np.float64(0.002634249860420823),\n",
       "   np.float64(0.0026540200706222095)],\n",
       "  'val_atom_loss_std': [np.float64(0.018285766171851993),\n",
       "   np.float64(0.019693517981571172),\n",
       "   np.float64(0.012709117033697032),\n",
       "   np.float64(0.013276246124913148),\n",
       "   np.float64(0.010865005525285234),\n",
       "   np.float64(0.009514502056629928),\n",
       "   np.float64(0.00810857976247078),\n",
       "   np.float64(0.006862408016450186),\n",
       "   np.float64(0.0061601633743459975),\n",
       "   np.float64(0.005107490055817873),\n",
       "   np.float64(0.005319767182160556),\n",
       "   np.float64(0.005137313169544987),\n",
       "   np.float64(0.004034122717407838),\n",
       "   np.float64(0.0031386357026531144),\n",
       "   np.float64(0.0031948170342332646),\n",
       "   np.float64(0.0030016597655191466),\n",
       "   np.float64(0.0035064987306406264),\n",
       "   np.float64(0.0025673464702581374),\n",
       "   np.float64(0.0043297351476687205),\n",
       "   np.float64(0.003602139913830092),\n",
       "   np.float64(0.003917651116650911),\n",
       "   np.float64(0.0032742183737066066),\n",
       "   np.float64(0.003272341956205958),\n",
       "   np.float64(0.003568022198676763),\n",
       "   np.float64(0.0038454425544514),\n",
       "   np.float64(0.0034650136966646924),\n",
       "   np.float64(0.004777267340880574),\n",
       "   np.float64(0.0028519756879015565),\n",
       "   np.float64(0.0030961947357434757),\n",
       "   np.float64(0.002852978596723285),\n",
       "   np.float64(0.004154545786552673),\n",
       "   np.float64(0.0031904285189912162),\n",
       "   np.float64(0.0036689792242383078),\n",
       "   np.float64(0.003103968797063702),\n",
       "   np.float64(0.0039656782388431825),\n",
       "   np.float64(0.0024664611835768027),\n",
       "   np.float64(0.0027958066316000484),\n",
       "   np.float64(0.002632455456796895),\n",
       "   np.float64(0.0029240460108055),\n",
       "   np.float64(0.003201203668463683),\n",
       "   np.float64(0.0035777452322289915),\n",
       "   np.float64(0.0025310636830868883),\n",
       "   np.float64(0.0036285234301272895),\n",
       "   np.float64(0.0035671325292275418),\n",
       "   np.float64(0.004596517567292416),\n",
       "   np.float64(0.0037526450881404548),\n",
       "   np.float64(0.0034755517168324314),\n",
       "   np.float64(0.004453060407251157),\n",
       "   np.float64(0.003623171769053042),\n",
       "   np.float64(0.002852761009743304),\n",
       "   np.float64(0.003188963448522047),\n",
       "   np.float64(0.0035043257372290217),\n",
       "   np.float64(0.0046596842836158945),\n",
       "   np.float64(0.0030072856086860627),\n",
       "   np.float64(0.0026385019684319355),\n",
       "   np.float64(0.0033450016446144697),\n",
       "   np.float64(0.003587459425179946),\n",
       "   np.float64(0.004070300277910261),\n",
       "   np.float64(0.004205592010119599),\n",
       "   np.float64(0.0037027831296173796),\n",
       "   np.float64(0.003127077642695178),\n",
       "   np.float64(0.0035322644818426337),\n",
       "   np.float64(0.002754539142693693),\n",
       "   np.float64(0.00341467172881955),\n",
       "   np.float64(0.0028541780704048904),\n",
       "   np.float64(0.003567388705732474),\n",
       "   np.float64(0.003922860315604076),\n",
       "   np.float64(0.002832413951511612),\n",
       "   np.float64(0.0032977627455626447),\n",
       "   np.float64(0.003602436792871676),\n",
       "   np.float64(0.0034320739180257022),\n",
       "   np.float64(0.002223928575296046),\n",
       "   np.float64(0.00430717155394033),\n",
       "   np.float64(0.0034113103578636826),\n",
       "   np.float64(0.002877545887983944),\n",
       "   np.float64(0.0038056017826828977),\n",
       "   np.float64(0.002808267712300734),\n",
       "   np.float64(0.00254393653759829),\n",
       "   np.float64(0.0029780825033880664),\n",
       "   np.float64(0.004623348210269316),\n",
       "   np.float64(0.003557494887435292),\n",
       "   np.float64(0.004006399212487574),\n",
       "   np.float64(0.0030049436145786483),\n",
       "   np.float64(0.0026637572997342215),\n",
       "   np.float64(0.0028698760397484224),\n",
       "   np.float64(0.003076420613229475),\n",
       "   np.float64(0.0028142902064292484),\n",
       "   np.float64(0.0031464526077989246),\n",
       "   np.float64(0.0026893084241394203),\n",
       "   np.float64(0.003230941001928431),\n",
       "   np.float64(0.003941059886781069),\n",
       "   np.float64(0.003641616491585681),\n",
       "   np.float64(0.0038666631901916547),\n",
       "   np.float64(0.003624631019648468),\n",
       "   np.float64(0.003083741273411758),\n",
       "   np.float64(0.002890930474715533),\n",
       "   np.float64(0.0033139712157752477),\n",
       "   np.float64(0.0032783444625283623),\n",
       "   np.float64(0.00371036576071666),\n",
       "   np.float64(0.002029985152683568),\n",
       "   np.float64(0.0038739661151348413),\n",
       "   np.float64(0.004205668388514312),\n",
       "   np.float64(0.003285038192955623),\n",
       "   np.float64(0.003539299649713169),\n",
       "   np.float64(0.0036660564811554176),\n",
       "   np.float64(0.004215149276091912),\n",
       "   np.float64(0.0028157372857301782),\n",
       "   np.float64(0.00303318767988059),\n",
       "   np.float64(0.0032502110737218947),\n",
       "   np.float64(0.0036036501642748545),\n",
       "   np.float64(0.003356481442320133),\n",
       "   np.float64(0.00320412159152325),\n",
       "   np.float64(0.002957836119037003),\n",
       "   np.float64(0.00273206661933842),\n",
       "   np.float64(0.00233461109533249),\n",
       "   np.float64(0.0028714877080984807),\n",
       "   np.float64(0.0028018534478688154),\n",
       "   np.float64(0.00252192346559875),\n",
       "   np.float64(0.0024936780884341593),\n",
       "   np.float64(0.0030320749721224807),\n",
       "   np.float64(0.0026412357005787227),\n",
       "   np.float64(0.0022380386300395365),\n",
       "   np.float64(0.002637602526992851),\n",
       "   np.float64(0.0024160358153481947),\n",
       "   np.float64(0.0027726437983809507),\n",
       "   np.float64(0.002534807626802534),\n",
       "   np.float64(0.002865500318429498),\n",
       "   np.float64(0.003249922028412368),\n",
       "   np.float64(0.0028443937341067847),\n",
       "   np.float64(0.0033840053151757907),\n",
       "   np.float64(0.003717006024525378),\n",
       "   np.float64(0.002446459643658318),\n",
       "   np.float64(0.0028071545181305273),\n",
       "   np.float64(0.0030629535090555076),\n",
       "   np.float64(0.0030748542333360936),\n",
       "   np.float64(0.002704501840053073),\n",
       "   np.float64(0.002566902663911683),\n",
       "   np.float64(0.0029318849537123288),\n",
       "   np.float64(0.0027331156968548207),\n",
       "   np.float64(0.0024871340762168876),\n",
       "   np.float64(0.0027370740698338275),\n",
       "   np.float64(0.0030637572237290174),\n",
       "   np.float64(0.0024638061973041863),\n",
       "   np.float64(0.002340639249857577),\n",
       "   np.float64(0.0018788218824283755),\n",
       "   np.float64(0.0024665327635161457),\n",
       "   np.float64(0.0029261696069974117),\n",
       "   np.float64(0.0026127740858190945),\n",
       "   np.float64(0.0028949759691856286),\n",
       "   np.float64(0.0023749757999060287),\n",
       "   np.float64(0.002486507149796922),\n",
       "   np.float64(0.0026663206849462905),\n",
       "   np.float64(0.0030370655580841815),\n",
       "   np.float64(0.002648988788165257),\n",
       "   np.float64(0.0017792705385251315),\n",
       "   np.float64(0.0024491354543037893),\n",
       "   np.float64(0.002572817237236837),\n",
       "   np.float64(0.0029385477718541575),\n",
       "   np.float64(0.0030672071208058555),\n",
       "   np.float64(0.0024478133267696396),\n",
       "   np.float64(0.0024950668546855326),\n",
       "   np.float64(0.0026505472208126386),\n",
       "   np.float64(0.0029262697376025936),\n",
       "   np.float64(0.00275832857760983),\n",
       "   np.float64(0.0024028147986942415),\n",
       "   np.float64(0.0021245598154749174),\n",
       "   np.float64(0.0033255698357669613),\n",
       "   np.float64(0.002306394690110794),\n",
       "   np.float64(0.002485722670846159),\n",
       "   np.float64(0.002595523561111815),\n",
       "   np.float64(0.0025226541061479724),\n",
       "   np.float64(0.0020034360068711546),\n",
       "   np.float64(0.0020258483324479635),\n",
       "   np.float64(0.001974090116250044),\n",
       "   np.float64(0.0023632792703525483),\n",
       "   np.float64(0.002383048966641029),\n",
       "   np.float64(0.0021785107181627778),\n",
       "   np.float64(0.002240106988700576),\n",
       "   np.float64(0.0021712273051874924),\n",
       "   np.float64(0.002474206374793152),\n",
       "   np.float64(0.0034571752625022637),\n",
       "   np.float64(0.002775608505255863),\n",
       "   np.float64(0.0021468274731793213),\n",
       "   np.float64(0.0023199923755016024),\n",
       "   np.float64(0.0025388944891904398),\n",
       "   np.float64(0.003034920556369572),\n",
       "   np.float64(0.00250055319086626),\n",
       "   np.float64(0.0022939169978649808),\n",
       "   np.float64(0.0020507127578171277),\n",
       "   np.float64(0.0021366854068658593),\n",
       "   np.float64(0.0020234314716436144),\n",
       "   np.float64(0.002871669424616832),\n",
       "   np.float64(0.0015740419482817472),\n",
       "   np.float64(0.0017996803523037612),\n",
       "   np.float64(0.001815068290351692),\n",
       "   np.float64(0.001316724460471843),\n",
       "   np.float64(0.0017946247562742248),\n",
       "   np.float64(0.0019882864218819375),\n",
       "   np.float64(0.001970703017497562),\n",
       "   np.float64(0.0017447405843885823),\n",
       "   np.float64(0.0015544558037432897),\n",
       "   np.float64(0.002000942158184422),\n",
       "   np.float64(0.002074408781134128),\n",
       "   np.float64(0.0019749770442761003),\n",
       "   np.float64(0.002080056930742151),\n",
       "   np.float64(0.0014504610911738626),\n",
       "   np.float64(0.0021736641098919923),\n",
       "   np.float64(0.0018665121736328482),\n",
       "   np.float64(0.002234979197794016),\n",
       "   np.float64(0.002070597007473269),\n",
       "   np.float64(0.0015249334438257453),\n",
       "   np.float64(0.0014367335995822215),\n",
       "   np.float64(0.0019409502755133503),\n",
       "   np.float64(0.0021232355785733535),\n",
       "   np.float64(0.0012867509967314568),\n",
       "   np.float64(0.0016179274460395854),\n",
       "   np.float64(0.001440926098626007),\n",
       "   np.float64(0.0015580077144223892),\n",
       "   np.float64(0.001862133964084468),\n",
       "   np.float64(0.0014737856315420472),\n",
       "   np.float64(0.002252714197652134),\n",
       "   np.float64(0.0021158955074645465),\n",
       "   np.float64(0.0017439496324545692),\n",
       "   np.float64(0.0015046749345013965),\n",
       "   np.float64(0.001405925229724554),\n",
       "   np.float64(0.0018740270527476636),\n",
       "   np.float64(0.001650972875009656),\n",
       "   np.float64(0.0019254675495748636),\n",
       "   np.float64(0.0020754991881879667),\n",
       "   np.float64(0.0018497403133684836),\n",
       "   np.float64(0.002072606438567609),\n",
       "   np.float64(0.0017121541489362826),\n",
       "   np.float64(0.001881900589520331),\n",
       "   np.float64(0.001354375926602514),\n",
       "   np.float64(0.0016259680540357494),\n",
       "   np.float64(0.0019224147439522862),\n",
       "   np.float64(0.0019165009057539111),\n",
       "   np.float64(0.0012466615406569045),\n",
       "   np.float64(0.0021475076980008987),\n",
       "   np.float64(0.0015444611148149397),\n",
       "   np.float64(0.002046245790659413),\n",
       "   np.float64(0.0015893064416985633),\n",
       "   np.float64(0.0018893402482296848),\n",
       "   np.float64(0.0016115314797109119),\n",
       "   np.float64(0.0015960062033348547),\n",
       "   np.float64(0.0015157059842788253),\n",
       "   np.float64(0.0017414273851646864),\n",
       "   np.float64(0.001623816129254129),\n",
       "   np.float64(0.0018822643237494934),\n",
       "   np.float64(0.001718291860225255),\n",
       "   np.float64(0.0013036111158870457),\n",
       "   np.float64(0.0010442038609201046),\n",
       "   np.float64(0.0016070005561691426),\n",
       "   np.float64(0.0017011814198823797),\n",
       "   np.float64(0.00156026209621106),\n",
       "   np.float64(0.001506243415996975),\n",
       "   np.float64(0.001670167537621481),\n",
       "   np.float64(0.0015012065047501262),\n",
       "   np.float64(0.00190586383136716),\n",
       "   np.float64(0.0021081725412950923),\n",
       "   np.float64(0.0015617362807196465),\n",
       "   np.float64(0.0020669572969487728),\n",
       "   np.float64(0.0019374088615980182),\n",
       "   np.float64(0.0015841741729356905),\n",
       "   np.float64(0.0020117281354748552),\n",
       "   np.float64(0.001428066554661118),\n",
       "   np.float64(0.001665564612608402),\n",
       "   np.float64(0.0015977452600538916),\n",
       "   np.float64(0.0019346243616618123),\n",
       "   np.float64(0.0015648697471116338),\n",
       "   np.float64(0.0012990261273184514),\n",
       "   np.float64(0.0011938391382475767),\n",
       "   np.float64(0.001280004796850652),\n",
       "   np.float64(0.002115124707068386),\n",
       "   np.float64(0.0016375105724690787),\n",
       "   np.float64(0.0014172464781552698),\n",
       "   np.float64(0.0013102396266503032),\n",
       "   np.float64(0.0013431206118248759),\n",
       "   np.float64(0.001867794522795995),\n",
       "   np.float64(0.0017848323112713215),\n",
       "   np.float64(0.0014868842333504155),\n",
       "   np.float64(0.0014814055092987448),\n",
       "   np.float64(0.0012942511529187538),\n",
       "   np.float64(0.001683123423315292),\n",
       "   np.float64(0.0015381227235179923),\n",
       "   np.float64(0.001345596219339577),\n",
       "   np.float64(0.0012458142753849367),\n",
       "   np.float64(0.0016401968284589716),\n",
       "   np.float64(0.0014084294478625547),\n",
       "   np.float64(0.0013205297321279813),\n",
       "   np.float64(0.0012283230659911734),\n",
       "   np.float64(0.0015727146870856397),\n",
       "   np.float64(0.0014202374983094746),\n",
       "   np.float64(0.0014220875485728502),\n",
       "   np.float64(0.0019082568440259042),\n",
       "   np.float64(0.0018353786016405721),\n",
       "   np.float64(0.0015326462401470938),\n",
       "   np.float64(0.001321972130099913),\n",
       "   np.float64(0.0012872587060415337),\n",
       "   np.float64(0.0018944456971809048),\n",
       "   np.float64(0.0015801995862769654),\n",
       "   np.float64(0.001304526288905996),\n",
       "   np.float64(0.0017546867638877875),\n",
       "   np.float64(0.0013436044875595949),\n",
       "   np.float64(0.0015200876523990732),\n",
       "   np.float64(0.0014626006196199032),\n",
       "   np.float64(0.0012756648004161341),\n",
       "   np.float64(0.0016672751527624045),\n",
       "   np.float64(0.0009560090359562142),\n",
       "   np.float64(0.0010821784749885856),\n",
       "   np.float64(0.0014103020822099522),\n",
       "   np.float64(0.0014859549918382765),\n",
       "   np.float64(0.0015298242108112857),\n",
       "   np.float64(0.0013920035654900042),\n",
       "   np.float64(0.0012533941878564788),\n",
       "   np.float64(0.0012629890776437662),\n",
       "   np.float64(0.0005973173458560304),\n",
       "   np.float64(0.0012746867402740425),\n",
       "   np.float64(0.0007554838156576901),\n",
       "   np.float64(0.0011915867689182007),\n",
       "   np.float64(0.0012672403281549969),\n",
       "   np.float64(0.0014471727843993216),\n",
       "   np.float64(0.0016235121577634498),\n",
       "   np.float64(0.0014274250974031632),\n",
       "   np.float64(0.0013327717600442252),\n",
       "   np.float64(0.0008771449874452547),\n",
       "   np.float64(0.001327301636864076),\n",
       "   np.float64(0.001210625867859357),\n",
       "   np.float64(0.001986376389534372),\n",
       "   np.float64(0.0012281725200346955),\n",
       "   np.float64(0.0011741058622340421),\n",
       "   np.float64(0.0008091742724736954),\n",
       "   np.float64(0.0012319121278304883),\n",
       "   np.float64(0.0014651622242631912),\n",
       "   np.float64(0.0012150149917185037),\n",
       "   np.float64(0.0008940603710850144),\n",
       "   np.float64(0.001105494699418179),\n",
       "   np.float64(0.0012306448563550502),\n",
       "   np.float64(0.001366494307044934),\n",
       "   np.float64(0.001016617619674335),\n",
       "   np.float64(0.0010990121966147428),\n",
       "   np.float64(0.0012497407568310213),\n",
       "   np.float64(0.0012061504318167761),\n",
       "   np.float64(0.0012040790529322506),\n",
       "   np.float64(0.0011094835913841722),\n",
       "   np.float64(0.0008537475861280999),\n",
       "   np.float64(0.0010825087541959048),\n",
       "   np.float64(0.0010600904658269782),\n",
       "   np.float64(0.0011578916254216913),\n",
       "   np.float64(0.0018140303622782117),\n",
       "   np.float64(0.0008431263873598745),\n",
       "   np.float64(0.0011524709547795094),\n",
       "   np.float64(0.000774174710549106),\n",
       "   np.float64(0.0011243472151763766),\n",
       "   np.float64(0.0012205523233179963),\n",
       "   np.float64(0.0013368675601823632),\n",
       "   np.float64(0.001048503902229262),\n",
       "   np.float64(0.0014908826218842986),\n",
       "   np.float64(0.0011446147513745064),\n",
       "   np.float64(0.001083272615796866),\n",
       "   np.float64(0.0011535505164536623),\n",
       "   np.float64(0.0014440720599058461),\n",
       "   np.float64(0.0013477921569041255),\n",
       "   np.float64(0.0014323617009456158),\n",
       "   np.float64(0.0013712324842898454),\n",
       "   np.float64(0.001434805268567678),\n",
       "   np.float64(0.0014943521929389328),\n",
       "   np.float64(0.0010361858003460317),\n",
       "   np.float64(0.0012000907298728833),\n",
       "   np.float64(0.0011416159666504074),\n",
       "   np.float64(0.0010710371896956448),\n",
       "   np.float64(0.0013672289054239902),\n",
       "   np.float64(0.0008453199722231186),\n",
       "   np.float64(0.0015610197275010413),\n",
       "   np.float64(0.0010945260013848301),\n",
       "   np.float64(0.001042945307407956),\n",
       "   np.float64(0.0006586273601167765),\n",
       "   np.float64(0.0007223766913771257),\n",
       "   np.float64(0.001599137580587708),\n",
       "   np.float64(0.0009449527141590768),\n",
       "   np.float64(0.0013247981169775985),\n",
       "   np.float64(0.0010663474007870106),\n",
       "   np.float64(0.0013170213110848359),\n",
       "   np.float64(0.0011446424325087053),\n",
       "   np.float64(0.0009679961477326523),\n",
       "   np.float64(0.0012201274409529717),\n",
       "   np.float64(0.0012198452050576872),\n",
       "   np.float64(0.0008658541320389629),\n",
       "   np.float64(0.0012031083021048243),\n",
       "   np.float64(0.0014321235218132456),\n",
       "   np.float64(0.0009587867444584899),\n",
       "   np.float64(0.0012546232269706077),\n",
       "   np.float64(0.0008489013232200721),\n",
       "   np.float64(0.001076865565946191),\n",
       "   np.float64(0.0010213685118862846),\n",
       "   np.float64(0.0010596785235461863),\n",
       "   np.float64(0.0009809153019644905),\n",
       "   np.float64(0.0010807820332814195),\n",
       "   np.float64(0.0012769000746735452),\n",
       "   np.float64(0.0013122241809461086),\n",
       "   np.float64(0.0012522243277622242),\n",
       "   np.float64(0.00152565933140159),\n",
       "   np.float64(0.0012217478105322169),\n",
       "   np.float64(0.0010486332730070187),\n",
       "   np.float64(0.0009391320123721662),\n",
       "   np.float64(0.0010883461024414516),\n",
       "   np.float64(0.0011424632638574721),\n",
       "   np.float64(0.0010233287144976043),\n",
       "   np.float64(0.001041065845401686),\n",
       "   np.float64(0.0008416526105951865),\n",
       "   np.float64(0.0013020299345096334),\n",
       "   np.float64(0.0007949575331219369),\n",
       "   np.float64(0.0016469412413769218),\n",
       "   np.float64(0.0011474504533198196),\n",
       "   np.float64(0.0012046805456956905),\n",
       "   np.float64(0.0008376289873107983),\n",
       "   np.float64(0.0008778181660763448),\n",
       "   np.float64(0.0012591188976295274),\n",
       "   np.float64(0.0009892673101537703),\n",
       "   np.float64(0.0010906358810498),\n",
       "   np.float64(0.0010280707687902342),\n",
       "   np.float64(0.0009750567756216475),\n",
       "   np.float64(0.0014337318160413937),\n",
       "   np.float64(0.001676214980913053),\n",
       "   np.float64(0.0009739098853548658),\n",
       "   np.float64(0.0012889157455476415),\n",
       "   np.float64(0.001143158689257514),\n",
       "   np.float64(0.001706083313531902),\n",
       "   np.float64(0.00136726344256368),\n",
       "   np.float64(0.0010989644305970073),\n",
       "   np.float64(0.0007166459250851605),\n",
       "   np.float64(0.0008703902707744324),\n",
       "   np.float64(0.0013328912915805335),\n",
       "   np.float64(0.0006629904683336364),\n",
       "   np.float64(0.0009081814716246628),\n",
       "   np.float64(0.0008582916036775798),\n",
       "   np.float64(0.0010797114223648685)],\n",
       "  'val_bond_loss_mean': [np.float64(0.2956282664090395),\n",
       "   np.float64(0.2694352278485894),\n",
       "   np.float64(0.20895620342344046),\n",
       "   np.float64(0.20508977957069874),\n",
       "   np.float64(0.1942386543378234),\n",
       "   np.float64(0.18128166813403368),\n",
       "   np.float64(0.16285611875355244),\n",
       "   np.float64(0.16272694058716297),\n",
       "   np.float64(0.1517978524789214),\n",
       "   np.float64(0.13716451451182365),\n",
       "   np.float64(0.14017631392925978),\n",
       "   np.float64(0.12519317353144288),\n",
       "   np.float64(0.12021526787430048),\n",
       "   np.float64(0.12422039359807968),\n",
       "   np.float64(0.11981945717707276),\n",
       "   np.float64(0.10798177123069763),\n",
       "   np.float64(0.11112838517874479),\n",
       "   np.float64(0.10854959161952138),\n",
       "   np.float64(0.10335918050259352),\n",
       "   np.float64(0.1040397104807198),\n",
       "   np.float64(0.09094618260860443),\n",
       "   np.float64(0.09434515424072742),\n",
       "   np.float64(0.09580299630761147),\n",
       "   np.float64(0.09116459684446454),\n",
       "   np.float64(0.09490582998842001),\n",
       "   np.float64(0.08704262413084507),\n",
       "   np.float64(0.09038409823551774),\n",
       "   np.float64(0.09108308330178261),\n",
       "   np.float64(0.09116527298465371),\n",
       "   np.float64(0.08386750053614378),\n",
       "   np.float64(0.08282879134640098),\n",
       "   np.float64(0.08453766023740172),\n",
       "   np.float64(0.08333226945251226),\n",
       "   np.float64(0.08634552685543895),\n",
       "   np.float64(0.0879651503637433),\n",
       "   np.float64(0.08763904683291912),\n",
       "   np.float64(0.08700623456388712),\n",
       "   np.float64(0.0886191725730896),\n",
       "   np.float64(0.08609523810446262),\n",
       "   np.float64(0.09146008314564824),\n",
       "   np.float64(0.0832741460762918),\n",
       "   np.float64(0.08410000195726752),\n",
       "   np.float64(0.08889916213229299),\n",
       "   np.float64(0.09044522373005748),\n",
       "   np.float64(0.08606645371764898),\n",
       "   np.float64(0.09206551872193813),\n",
       "   np.float64(0.0998909454792738),\n",
       "   np.float64(0.08969727205112576),\n",
       "   np.float64(0.09975358098745346),\n",
       "   np.float64(0.09268997143954039),\n",
       "   np.float64(0.09103981358930469),\n",
       "   np.float64(0.08637246116995811),\n",
       "   np.float64(0.10056581674143672),\n",
       "   np.float64(0.09045677445828915),\n",
       "   np.float64(0.09899477986618876),\n",
       "   np.float64(0.09126721043139696),\n",
       "   np.float64(0.09745228895917535),\n",
       "   np.float64(0.09897040063515306),\n",
       "   np.float64(0.09109386382624507),\n",
       "   np.float64(0.09396909084171057),\n",
       "   np.float64(0.08535859268158674),\n",
       "   np.float64(0.08940047537907958),\n",
       "   np.float64(0.08740744274109602),\n",
       "   np.float64(0.08704548748210073),\n",
       "   np.float64(0.08944664150476456),\n",
       "   np.float64(0.08918517222627997),\n",
       "   np.float64(0.08784728962928057),\n",
       "   np.float64(0.08743529673665762),\n",
       "   np.float64(0.08790591545403004),\n",
       "   np.float64(0.09496563812717795),\n",
       "   np.float64(0.09107980085536838),\n",
       "   np.float64(0.09012784622609615),\n",
       "   np.float64(0.08521314291283488),\n",
       "   np.float64(0.089843874797225),\n",
       "   np.float64(0.08763788640499115),\n",
       "   np.float64(0.08600334590300918),\n",
       "   np.float64(0.09129200084134936),\n",
       "   np.float64(0.09110995335504413),\n",
       "   np.float64(0.0901543996296823),\n",
       "   np.float64(0.08867084188386798),\n",
       "   np.float64(0.09417058387771249),\n",
       "   np.float64(0.08641720796003938),\n",
       "   np.float64(0.08843890484422445),\n",
       "   np.float64(0.08509525284171104),\n",
       "   np.float64(0.08566123014315963),\n",
       "   np.float64(0.08566716825589538),\n",
       "   np.float64(0.08694731071591377),\n",
       "   np.float64(0.08373294537886977),\n",
       "   np.float64(0.08371075429022312),\n",
       "   np.float64(0.08564149914309382),\n",
       "   np.float64(0.08565506106242537),\n",
       "   np.float64(0.08452562429010868),\n",
       "   np.float64(0.0865222942084074),\n",
       "   np.float64(0.08349953452125192),\n",
       "   np.float64(0.08269560942426324),\n",
       "   np.float64(0.08869007369503379),\n",
       "   np.float64(0.08362969849258661),\n",
       "   np.float64(0.09167425334453583),\n",
       "   np.float64(0.09221134521067142),\n",
       "   np.float64(0.0965566192753613),\n",
       "   np.float64(0.09309827676042914),\n",
       "   np.float64(0.09184979368001223),\n",
       "   np.float64(0.09585590427741408),\n",
       "   np.float64(0.0932148159481585),\n",
       "   np.float64(0.09195418749004602),\n",
       "   np.float64(0.09423782955855131),\n",
       "   np.float64(0.0969374100677669),\n",
       "   np.float64(0.09486750606447458),\n",
       "   np.float64(0.09803396509960294),\n",
       "   np.float64(0.09803205961361527),\n",
       "   np.float64(0.09280041651800275),\n",
       "   np.float64(0.091140182223171),\n",
       "   np.float64(0.09740642970427871),\n",
       "   np.float64(0.09505539061501622),\n",
       "   np.float64(0.09545629983767867),\n",
       "   np.float64(0.09563490888103843),\n",
       "   np.float64(0.09402837604284286),\n",
       "   np.float64(0.09879096923395991),\n",
       "   np.float64(0.09541229205206037),\n",
       "   np.float64(0.09419163269922137),\n",
       "   np.float64(0.08639417076483369),\n",
       "   np.float64(0.09136961027979851),\n",
       "   np.float64(0.09280389733612537),\n",
       "   np.float64(0.09453133912757039),\n",
       "   np.float64(0.10166510567069054),\n",
       "   np.float64(0.09644761821255088),\n",
       "   np.float64(0.09872219106182456),\n",
       "   np.float64(0.10317958425730467),\n",
       "   np.float64(0.09430882800370455),\n",
       "   np.float64(0.09643956227228045),\n",
       "   np.float64(0.09747164137661457),\n",
       "   np.float64(0.09209089959040284),\n",
       "   np.float64(0.0965683339163661),\n",
       "   np.float64(0.0918870409950614),\n",
       "   np.float64(0.09632939519360662),\n",
       "   np.float64(0.10094119003042579),\n",
       "   np.float64(0.09910034434869885),\n",
       "   np.float64(0.10116792563349009),\n",
       "   np.float64(0.09962240792810917),\n",
       "   np.float64(0.09864210523664951),\n",
       "   np.float64(0.10435586050152779),\n",
       "   np.float64(0.10803418839350343),\n",
       "   np.float64(0.10374723048880696),\n",
       "   np.float64(0.10374167002737522),\n",
       "   np.float64(0.10838675405830145),\n",
       "   np.float64(0.09747956087812781),\n",
       "   np.float64(0.10230494197458029),\n",
       "   np.float64(0.10077774478122592),\n",
       "   np.float64(0.09947506245225668),\n",
       "   np.float64(0.09697304479777813),\n",
       "   np.float64(0.09785163775086403),\n",
       "   np.float64(0.10523544857278466),\n",
       "   np.float64(0.10193395102396607),\n",
       "   np.float64(0.09885214641690254),\n",
       "   np.float64(0.09954365715384483),\n",
       "   np.float64(0.09837637981399894),\n",
       "   np.float64(0.10331487189978361),\n",
       "   np.float64(0.10334374103695154),\n",
       "   np.float64(0.09666789090260863),\n",
       "   np.float64(0.09914493001997471),\n",
       "   np.float64(0.09136289358139038),\n",
       "   np.float64(0.09613229054957628),\n",
       "   np.float64(0.09309812635183334),\n",
       "   np.float64(0.09343609726056457),\n",
       "   np.float64(0.1000393875874579),\n",
       "   np.float64(0.09369795396924019),\n",
       "   np.float64(0.10755438264459372),\n",
       "   np.float64(0.10287560662254691),\n",
       "   np.float64(0.10110160941258073),\n",
       "   np.float64(0.11065221996977925),\n",
       "   np.float64(0.1121978429146111),\n",
       "   np.float64(0.1022518971003592),\n",
       "   np.float64(0.10079761035740376),\n",
       "   np.float64(0.10192447528243065),\n",
       "   np.float64(0.10253726411610842),\n",
       "   np.float64(0.10072352271527052),\n",
       "   np.float64(0.10328742256388068),\n",
       "   np.float64(0.10539832059293985),\n",
       "   np.float64(0.10207699285820127),\n",
       "   np.float64(0.09864145517349243),\n",
       "   np.float64(0.10183343943208456),\n",
       "   np.float64(0.10644632950425148),\n",
       "   np.float64(0.0969818439334631),\n",
       "   np.float64(0.10212700441479683),\n",
       "   np.float64(0.10636993264779449),\n",
       "   np.float64(0.10391291324049234),\n",
       "   np.float64(0.09790555387735367),\n",
       "   np.float64(0.10115926433354616),\n",
       "   np.float64(0.10089397709816694),\n",
       "   np.float64(0.10156889958307147),\n",
       "   np.float64(0.11063291085883975),\n",
       "   np.float64(0.10345239704474807),\n",
       "   np.float64(0.10136394714936614),\n",
       "   np.float64(0.10392029210925102),\n",
       "   np.float64(0.10279850196093321),\n",
       "   np.float64(0.09788556769490242),\n",
       "   np.float64(0.09559325594455004),\n",
       "   np.float64(0.09595483401790261),\n",
       "   np.float64(0.10601082630455494),\n",
       "   np.float64(0.099543247371912),\n",
       "   np.float64(0.09672249620780349),\n",
       "   np.float64(0.10145954601466656),\n",
       "   np.float64(0.10058170836418867),\n",
       "   np.float64(0.10028539225459099),\n",
       "   np.float64(0.10039467364549637),\n",
       "   np.float64(0.09933236753568053),\n",
       "   np.float64(0.10515103116631508),\n",
       "   np.float64(0.09523901948705316),\n",
       "   np.float64(0.09807245200499892),\n",
       "   np.float64(0.10160015802830458),\n",
       "   np.float64(0.1046497612260282),\n",
       "   np.float64(0.09863859880715609),\n",
       "   np.float64(0.1024719588458538),\n",
       "   np.float64(0.10362480068579316),\n",
       "   np.float64(0.10783685185015202),\n",
       "   np.float64(0.10091570392251015),\n",
       "   np.float64(0.10356869455426931),\n",
       "   np.float64(0.10349693102762103),\n",
       "   np.float64(0.10413883114233613),\n",
       "   np.float64(0.10505492333322763),\n",
       "   np.float64(0.104701470118016),\n",
       "   np.float64(0.10987395234405994),\n",
       "   np.float64(0.10683748265728354),\n",
       "   np.float64(0.10684782266616821),\n",
       "   np.float64(0.10307280533015728),\n",
       "   np.float64(0.10063793323934078),\n",
       "   np.float64(0.10109372530132532),\n",
       "   np.float64(0.09994488907977939),\n",
       "   np.float64(0.10601416602730751),\n",
       "   np.float64(0.09813190624117851),\n",
       "   np.float64(0.0995436068624258),\n",
       "   np.float64(0.09972384478896856),\n",
       "   np.float64(0.10344766965135932),\n",
       "   np.float64(0.09928696416318417),\n",
       "   np.float64(0.1036748755723238),\n",
       "   np.float64(0.0990477679297328),\n",
       "   np.float64(0.10067790606990457),\n",
       "   np.float64(0.09854330820962787),\n",
       "   np.float64(0.0910669774748385),\n",
       "   np.float64(0.09879593271762133),\n",
       "   np.float64(0.09841554518789053),\n",
       "   np.float64(0.10724946577101946),\n",
       "   np.float64(0.09815329313278198),\n",
       "   np.float64(0.09868186060339212),\n",
       "   np.float64(0.10016412800177932),\n",
       "   np.float64(0.09562843013554811),\n",
       "   np.float64(0.10126754269003868),\n",
       "   np.float64(0.09860070142894983),\n",
       "   np.float64(0.09848492499440908),\n",
       "   np.float64(0.09827585611492395),\n",
       "   np.float64(0.09888674831017852),\n",
       "   np.float64(0.10091064684092999),\n",
       "   np.float64(0.09882089123129845),\n",
       "   np.float64(0.10549704544246197),\n",
       "   np.float64(0.10153826745226979),\n",
       "   np.float64(0.0989469587802887),\n",
       "   np.float64(0.10034598968923092),\n",
       "   np.float64(0.10183287225663662),\n",
       "   np.float64(0.09896497847512364),\n",
       "   np.float64(0.10268296487629414),\n",
       "   np.float64(0.10170134855434299),\n",
       "   np.float64(0.10285499086603522),\n",
       "   np.float64(0.0974615765735507),\n",
       "   np.float64(0.099057137966156),\n",
       "   np.float64(0.09785581193864346),\n",
       "   np.float64(0.09434188483282924),\n",
       "   np.float64(0.09732621302828193),\n",
       "   np.float64(0.09576270543038845),\n",
       "   np.float64(0.1013651518151164),\n",
       "   np.float64(0.09841267997398973),\n",
       "   np.float64(0.0959459668956697),\n",
       "   np.float64(0.09786001732572913),\n",
       "   np.float64(0.10281036794185638),\n",
       "   np.float64(0.09829075960442424),\n",
       "   np.float64(0.10228569200262427),\n",
       "   np.float64(0.09851156966760755),\n",
       "   np.float64(0.09623623080551624),\n",
       "   np.float64(0.09803661797195673),\n",
       "   np.float64(0.10048494813963771),\n",
       "   np.float64(0.10303435428068042),\n",
       "   np.float64(0.10050975577905774),\n",
       "   np.float64(0.09923508670181036),\n",
       "   np.float64(0.10029352316632867),\n",
       "   np.float64(0.10180817823857069),\n",
       "   np.float64(0.09855166543275118),\n",
       "   np.float64(0.0995810548774898),\n",
       "   np.float64(0.09780189162120223),\n",
       "   np.float64(0.09763448778539896),\n",
       "   np.float64(0.09446061961352825),\n",
       "   np.float64(0.09696115739643574),\n",
       "   np.float64(0.10023546777665615),\n",
       "   np.float64(0.09390439838171005),\n",
       "   np.float64(0.09615761786699295),\n",
       "   np.float64(0.10444423882290721),\n",
       "   np.float64(0.09825688414275646),\n",
       "   np.float64(0.09649696154519916),\n",
       "   np.float64(0.09813439659774303),\n",
       "   np.float64(0.09830336086452007),\n",
       "   np.float64(0.09863338340073824),\n",
       "   np.float64(0.10377693921327591),\n",
       "   np.float64(0.09829905163496733),\n",
       "   np.float64(0.0958865387365222),\n",
       "   np.float64(0.09847750002518296),\n",
       "   np.float64(0.09368570800870657),\n",
       "   np.float64(0.10018473584204912),\n",
       "   np.float64(0.10103731090202928),\n",
       "   np.float64(0.10528671694919467),\n",
       "   np.float64(0.10098489746451378),\n",
       "   np.float64(0.09621095238253474),\n",
       "   np.float64(0.0963941914960742),\n",
       "   np.float64(0.09892956912517548),\n",
       "   np.float64(0.10018638847395778),\n",
       "   np.float64(0.0981504269875586),\n",
       "   np.float64(0.09902926394715905),\n",
       "   np.float64(0.0954445842653513),\n",
       "   np.float64(0.09645189624279737),\n",
       "   np.float64(0.09773905016481876),\n",
       "   np.float64(0.09813167853280902),\n",
       "   np.float64(0.09646783489733934),\n",
       "   np.float64(0.09622662095353007),\n",
       "   np.float64(0.09921805886551738),\n",
       "   np.float64(0.09817341342568398),\n",
       "   np.float64(0.09751431411132216),\n",
       "   np.float64(0.0965823270380497),\n",
       "   np.float64(0.09357222262769938),\n",
       "   np.float64(0.0958950431086123),\n",
       "   np.float64(0.09751943545415998),\n",
       "   np.float64(0.09942716546356678),\n",
       "   np.float64(0.09570920607075095),\n",
       "   np.float64(0.0978627079166472),\n",
       "   np.float64(0.09756792290136218),\n",
       "   np.float64(0.09555682260543108),\n",
       "   np.float64(0.09060143399983644),\n",
       "   np.float64(0.09308447502553463),\n",
       "   np.float64(0.09849760634824634),\n",
       "   np.float64(0.09883972490206361),\n",
       "   np.float64(0.09925131360068917),\n",
       "   np.float64(0.09236516943201423),\n",
       "   np.float64(0.09525124216452241),\n",
       "   np.float64(0.09817496035248041),\n",
       "   np.float64(0.09851970244199038),\n",
       "   np.float64(0.0965549829415977),\n",
       "   np.float64(0.09811838995665312),\n",
       "   np.float64(0.09501569019630551),\n",
       "   np.float64(0.09622546937316656),\n",
       "   np.float64(0.09493312006816268),\n",
       "   np.float64(0.093599290587008),\n",
       "   np.float64(0.09532042499631643),\n",
       "   np.float64(0.09644531877711415),\n",
       "   np.float64(0.0925739100202918),\n",
       "   np.float64(0.09619721071794629),\n",
       "   np.float64(0.09582845773547888),\n",
       "   np.float64(0.0960901235230267),\n",
       "   np.float64(0.09359389310702682),\n",
       "   np.float64(0.08864102931693196),\n",
       "   np.float64(0.09467245824635029),\n",
       "   np.float64(0.09170085564255714),\n",
       "   np.float64(0.09015238890424371),\n",
       "   np.float64(0.09303932916373014),\n",
       "   np.float64(0.09477420849725604),\n",
       "   np.float64(0.09438148839399219),\n",
       "   np.float64(0.09556807111948729),\n",
       "   np.float64(0.0945882466621697),\n",
       "   np.float64(0.09394898591563106),\n",
       "   np.float64(0.08982648281380534),\n",
       "   np.float64(0.09326258674263954),\n",
       "   np.float64(0.0912408814765513),\n",
       "   np.float64(0.09316022833809257),\n",
       "   np.float64(0.09547952422872186),\n",
       "   np.float64(0.09712810115888715),\n",
       "   np.float64(0.09477252559736371),\n",
       "   np.float64(0.09087530756369233),\n",
       "   np.float64(0.09327558474615216),\n",
       "   np.float64(0.09202253306284547),\n",
       "   np.float64(0.09077015053480864),\n",
       "   np.float64(0.09746271092444658),\n",
       "   np.float64(0.09286909084767103),\n",
       "   np.float64(0.0931833335198462),\n",
       "   np.float64(0.09272949583828449),\n",
       "   np.float64(0.09246595855802298),\n",
       "   np.float64(0.09725217893719673),\n",
       "   np.float64(0.09607492666691542),\n",
       "   np.float64(0.0943433390930295),\n",
       "   np.float64(0.0915517215617001),\n",
       "   np.float64(0.09589034831151366),\n",
       "   np.float64(0.09380278689786792),\n",
       "   np.float64(0.09220077889040112),\n",
       "   np.float64(0.0943048489280045),\n",
       "   np.float64(0.09669374814257026),\n",
       "   np.float64(0.09574918076395988),\n",
       "   np.float64(0.09618686186149716),\n",
       "   np.float64(0.09279227675870061),\n",
       "   np.float64(0.09147884277626872),\n",
       "   np.float64(0.08890712587162852),\n",
       "   np.float64(0.09167258162051439),\n",
       "   np.float64(0.09166362509131432),\n",
       "   np.float64(0.09163337852805853),\n",
       "   np.float64(0.09068402135744691),\n",
       "   np.float64(0.09438833175227046),\n",
       "   np.float64(0.09416420990601182),\n",
       "   np.float64(0.09544349554926157),\n",
       "   np.float64(0.09210798889398575),\n",
       "   np.float64(0.09528252203017473),\n",
       "   np.float64(0.09433507919311523),\n",
       "   np.float64(0.09278829162940383),\n",
       "   np.float64(0.09077344369143248),\n",
       "   np.float64(0.09012734238058329),\n",
       "   np.float64(0.09069594601169229),\n",
       "   np.float64(0.09339820360764861),\n",
       "   np.float64(0.089778708294034),\n",
       "   np.float64(0.09318870259448886),\n",
       "   np.float64(0.08847409207373857),\n",
       "   np.float64(0.09572339756414294),\n",
       "   np.float64(0.08974707499146461),\n",
       "   np.float64(0.08874299377202988),\n",
       "   np.float64(0.09039320796728134),\n",
       "   np.float64(0.08853097865357995),\n",
       "   np.float64(0.09343091957271099),\n",
       "   np.float64(0.09288514079526067),\n",
       "   np.float64(0.09588971734046936),\n",
       "   np.float64(0.08656878443434834),\n",
       "   np.float64(0.09024362312629819),\n",
       "   np.float64(0.09322154149413109),\n",
       "   np.float64(0.0920273712836206),\n",
       "   np.float64(0.09013404883444309),\n",
       "   np.float64(0.08861525356769562),\n",
       "   np.float64(0.09106625011190772),\n",
       "   np.float64(0.09046780643984675),\n",
       "   np.float64(0.08915385464206338),\n",
       "   np.float64(0.09069304680451751),\n",
       "   np.float64(0.09001264721155167),\n",
       "   np.float64(0.0947847911156714),\n",
       "   np.float64(0.09404979087412357),\n",
       "   np.float64(0.08816584292799234),\n",
       "   np.float64(0.09159982251003385),\n",
       "   np.float64(0.08980918396264315),\n",
       "   np.float64(0.08900996344164014)],\n",
       "  'val_bond_loss_std': [np.float64(0.012147818724588453),\n",
       "   np.float64(0.014202968272306153),\n",
       "   np.float64(0.008638800502864382),\n",
       "   np.float64(0.009112930490048734),\n",
       "   np.float64(0.009422168000645511),\n",
       "   np.float64(0.009074641392349264),\n",
       "   np.float64(0.009343573450240714),\n",
       "   np.float64(0.010405153327793383),\n",
       "   np.float64(0.010285453847042731),\n",
       "   np.float64(0.0083214646179408),\n",
       "   np.float64(0.008318187353037824),\n",
       "   np.float64(0.00871639554228011),\n",
       "   np.float64(0.006471461736518231),\n",
       "   np.float64(0.008095084182870093),\n",
       "   np.float64(0.0067932502706468615),\n",
       "   np.float64(0.00671651289797876),\n",
       "   np.float64(0.005877078269424931),\n",
       "   np.float64(0.007142835644783069),\n",
       "   np.float64(0.007268575372027169),\n",
       "   np.float64(0.006792465628405127),\n",
       "   np.float64(0.008070785552070244),\n",
       "   np.float64(0.00672779685181953),\n",
       "   np.float64(0.006101171623143091),\n",
       "   np.float64(0.0065213644405564515),\n",
       "   np.float64(0.006703097354466379),\n",
       "   np.float64(0.007224511820339825),\n",
       "   np.float64(0.006578166292381666),\n",
       "   np.float64(0.007091751932373792),\n",
       "   np.float64(0.00771840622308713),\n",
       "   np.float64(0.006432756805467549),\n",
       "   np.float64(0.0076246905607389),\n",
       "   np.float64(0.006994115135335778),\n",
       "   np.float64(0.005815488669927888),\n",
       "   np.float64(0.006656982979305816),\n",
       "   np.float64(0.005983737967976606),\n",
       "   np.float64(0.007372372432158559),\n",
       "   np.float64(0.007203920345035784),\n",
       "   np.float64(0.007499823616941167),\n",
       "   np.float64(0.007036138751771462),\n",
       "   np.float64(0.009359920754206282),\n",
       "   np.float64(0.006874299133430902),\n",
       "   np.float64(0.008124057831600586),\n",
       "   np.float64(0.00783848987963818),\n",
       "   np.float64(0.006901697315952591),\n",
       "   np.float64(0.00793455177250531),\n",
       "   np.float64(0.006964287882087409),\n",
       "   np.float64(0.008797332609588213),\n",
       "   np.float64(0.006397879090323348),\n",
       "   np.float64(0.009147578494968315),\n",
       "   np.float64(0.008393903850449264),\n",
       "   np.float64(0.006624057138037405),\n",
       "   np.float64(0.007148969341048539),\n",
       "   np.float64(0.007524878575864154),\n",
       "   np.float64(0.007742053732220177),\n",
       "   np.float64(0.00893947010375147),\n",
       "   np.float64(0.008233396982510468),\n",
       "   np.float64(0.008940733864742152),\n",
       "   np.float64(0.007314018450958966),\n",
       "   np.float64(0.007105735055006299),\n",
       "   np.float64(0.009081969024526128),\n",
       "   np.float64(0.007449536453313346),\n",
       "   np.float64(0.00893199423835672),\n",
       "   np.float64(0.007926856340787844),\n",
       "   np.float64(0.008653232426188592),\n",
       "   np.float64(0.00893399971219882),\n",
       "   np.float64(0.00747775661541819),\n",
       "   np.float64(0.008051885834271336),\n",
       "   np.float64(0.008724756034766837),\n",
       "   np.float64(0.008024101507193442),\n",
       "   np.float64(0.008144343595276802),\n",
       "   np.float64(0.006681005390956773),\n",
       "   np.float64(0.006977767842228922),\n",
       "   np.float64(0.007650964073811032),\n",
       "   np.float64(0.00778144929970503),\n",
       "   np.float64(0.007051870026564673),\n",
       "   np.float64(0.008583369708102547),\n",
       "   np.float64(0.007759449054963532),\n",
       "   np.float64(0.009435509977837809),\n",
       "   np.float64(0.008416541417755023),\n",
       "   np.float64(0.008872581984517954),\n",
       "   np.float64(0.00865255727207232),\n",
       "   np.float64(0.008105118639640616),\n",
       "   np.float64(0.007958767220866773),\n",
       "   np.float64(0.008857816209861119),\n",
       "   np.float64(0.007296380413033575),\n",
       "   np.float64(0.009140981193522822),\n",
       "   np.float64(0.007054587299162315),\n",
       "   np.float64(0.007572512843090938),\n",
       "   np.float64(0.006440422265053446),\n",
       "   np.float64(0.007433274157476045),\n",
       "   np.float64(0.00949083434997035),\n",
       "   np.float64(0.006172710000564691),\n",
       "   np.float64(0.00853564675508372),\n",
       "   np.float64(0.008591255313068227),\n",
       "   np.float64(0.007715041513195777),\n",
       "   np.float64(0.00905713613158616),\n",
       "   np.float64(0.0068143234043554326),\n",
       "   np.float64(0.008601052279200537),\n",
       "   np.float64(0.008119617013874722),\n",
       "   np.float64(0.009874922753065724),\n",
       "   np.float64(0.00953442377694388),\n",
       "   np.float64(0.008912197649433297),\n",
       "   np.float64(0.009197547127254367),\n",
       "   np.float64(0.009501042521850783),\n",
       "   np.float64(0.008840799183885405),\n",
       "   np.float64(0.01108350698184727),\n",
       "   np.float64(0.009967592530466569),\n",
       "   np.float64(0.00991325621635588),\n",
       "   np.float64(0.00907352618162181),\n",
       "   np.float64(0.00944494089840928),\n",
       "   np.float64(0.009002663327788915),\n",
       "   np.float64(0.009375031345454787),\n",
       "   np.float64(0.01099847153185474),\n",
       "   np.float64(0.009414170741266175),\n",
       "   np.float64(0.009269898431251563),\n",
       "   np.float64(0.01004256854991448),\n",
       "   np.float64(0.0103356106266751),\n",
       "   np.float64(0.009530746848214238),\n",
       "   np.float64(0.010232653834706945),\n",
       "   np.float64(0.010120074252036768),\n",
       "   np.float64(0.010018936371009828),\n",
       "   np.float64(0.00929768072606979),\n",
       "   np.float64(0.008571014583526432),\n",
       "   np.float64(0.007710987890542436),\n",
       "   np.float64(0.010178946110247266),\n",
       "   np.float64(0.0077686026211998185),\n",
       "   np.float64(0.008366693392772066),\n",
       "   np.float64(0.011412705981982197),\n",
       "   np.float64(0.009423214893392),\n",
       "   np.float64(0.00925005397705187),\n",
       "   np.float64(0.009809373917301325),\n",
       "   np.float64(0.008839285779713827),\n",
       "   np.float64(0.01048778637307587),\n",
       "   np.float64(0.008546265901793115),\n",
       "   np.float64(0.009974407664934775),\n",
       "   np.float64(0.009693593036781509),\n",
       "   np.float64(0.008418494296352557),\n",
       "   np.float64(0.010347882869615948),\n",
       "   np.float64(0.009151403565986909),\n",
       "   np.float64(0.010654412243260837),\n",
       "   np.float64(0.012023442805435342),\n",
       "   np.float64(0.008075876287975245),\n",
       "   np.float64(0.011349409876287282),\n",
       "   np.float64(0.01133806978151396),\n",
       "   np.float64(0.009942938682359547),\n",
       "   np.float64(0.008944903167703288),\n",
       "   np.float64(0.007790462485937867),\n",
       "   np.float64(0.009835356367403932),\n",
       "   np.float64(0.009617784511739357),\n",
       "   np.float64(0.010196300459544422),\n",
       "   np.float64(0.008545870518227326),\n",
       "   np.float64(0.009373172877699886),\n",
       "   np.float64(0.009046100100384341),\n",
       "   np.float64(0.011712057316773489),\n",
       "   np.float64(0.009580130848121928),\n",
       "   np.float64(0.010487501310362439),\n",
       "   np.float64(0.011661410990447008),\n",
       "   np.float64(0.01098798653248791),\n",
       "   np.float64(0.010205805296715836),\n",
       "   np.float64(0.010162944245765805),\n",
       "   np.float64(0.008371341769850478),\n",
       "   np.float64(0.009593291563447702),\n",
       "   np.float64(0.010298410230022814),\n",
       "   np.float64(0.008874134073455395),\n",
       "   np.float64(0.011467030115892337),\n",
       "   np.float64(0.011534180813083326),\n",
       "   np.float64(0.010593458286593537),\n",
       "   np.float64(0.01140931559224851),\n",
       "   np.float64(0.009827313103432662),\n",
       "   np.float64(0.01284756834268786),\n",
       "   np.float64(0.012336269183172497),\n",
       "   np.float64(0.010378519450922214),\n",
       "   np.float64(0.0096169661699406),\n",
       "   np.float64(0.009039208833007862),\n",
       "   np.float64(0.010177584130844516),\n",
       "   np.float64(0.00790484365897717),\n",
       "   np.float64(0.010850118588412757),\n",
       "   np.float64(0.010958053527253748),\n",
       "   np.float64(0.01013773973379354),\n",
       "   np.float64(0.00986223566088905),\n",
       "   np.float64(0.010077011106053142),\n",
       "   np.float64(0.011301281954385949),\n",
       "   np.float64(0.010130656713929353),\n",
       "   np.float64(0.010402004012143367),\n",
       "   np.float64(0.01075826263164795),\n",
       "   np.float64(0.010253045092637508),\n",
       "   np.float64(0.011499163467397197),\n",
       "   np.float64(0.00920210126627555),\n",
       "   np.float64(0.007896462113444818),\n",
       "   np.float64(0.009260936229429553),\n",
       "   np.float64(0.011548963913313367),\n",
       "   np.float64(0.01095381531208591),\n",
       "   np.float64(0.011348806211842121),\n",
       "   np.float64(0.00984110887311198),\n",
       "   np.float64(0.007931620576794874),\n",
       "   np.float64(0.010304747664310472),\n",
       "   np.float64(0.008515348644586479),\n",
       "   np.float64(0.009561431207241403),\n",
       "   np.float64(0.009748945710485912),\n",
       "   np.float64(0.008448973715352215),\n",
       "   np.float64(0.00903118072176967),\n",
       "   np.float64(0.010420491834058541),\n",
       "   np.float64(0.010101385616871225),\n",
       "   np.float64(0.011389533044073233),\n",
       "   np.float64(0.010685757450654696),\n",
       "   np.float64(0.011016118623943683),\n",
       "   np.float64(0.0095280292493633),\n",
       "   np.float64(0.011406344436754423),\n",
       "   np.float64(0.009523885315745944),\n",
       "   np.float64(0.010874936434389984),\n",
       "   np.float64(0.01102012708198315),\n",
       "   np.float64(0.009596357825569168),\n",
       "   np.float64(0.007925290778227864),\n",
       "   np.float64(0.009193150419283133),\n",
       "   np.float64(0.01183230420987617),\n",
       "   np.float64(0.008440867350886188),\n",
       "   np.float64(0.010361552143236235),\n",
       "   np.float64(0.007760357470359711),\n",
       "   np.float64(0.00876280986682775),\n",
       "   np.float64(0.00995702284132286),\n",
       "   np.float64(0.009067138732030702),\n",
       "   np.float64(0.010674202167246823),\n",
       "   np.float64(0.009789853352057088),\n",
       "   np.float64(0.009122622896069525),\n",
       "   np.float64(0.008966070285964911),\n",
       "   np.float64(0.011347321526441781),\n",
       "   np.float64(0.009217669266619296),\n",
       "   np.float64(0.008385456368185362),\n",
       "   np.float64(0.009013643966716204),\n",
       "   np.float64(0.009002532037178157),\n",
       "   np.float64(0.009821350544994879),\n",
       "   np.float64(0.009179914159295066),\n",
       "   np.float64(0.010206732823561371),\n",
       "   np.float64(0.01042757377297021),\n",
       "   np.float64(0.008105899544176235),\n",
       "   np.float64(0.011094212405352863),\n",
       "   np.float64(0.008460935026710475),\n",
       "   np.float64(0.009328001351667996),\n",
       "   np.float64(0.00883551439244182),\n",
       "   np.float64(0.008269446358477105),\n",
       "   np.float64(0.008143985561170304),\n",
       "   np.float64(0.009689338284863994),\n",
       "   np.float64(0.007804111582318798),\n",
       "   np.float64(0.0095220373275926),\n",
       "   np.float64(0.010733939935137144),\n",
       "   np.float64(0.009605462482200306),\n",
       "   np.float64(0.009447791342174579),\n",
       "   np.float64(0.009470455417211945),\n",
       "   np.float64(0.009494972743429631),\n",
       "   np.float64(0.010642787946760643),\n",
       "   np.float64(0.008958065941340278),\n",
       "   np.float64(0.010436947266444889),\n",
       "   np.float64(0.009074041759972012),\n",
       "   np.float64(0.010257464863285997),\n",
       "   np.float64(0.010868315943741828),\n",
       "   np.float64(0.00910534391201655),\n",
       "   np.float64(0.01012751491571282),\n",
       "   np.float64(0.009268319437312444),\n",
       "   np.float64(0.008464292970962881),\n",
       "   np.float64(0.01010042456369246),\n",
       "   np.float64(0.00917729668271215),\n",
       "   np.float64(0.009390058008111504),\n",
       "   np.float64(0.008187468291630754),\n",
       "   np.float64(0.010193013950142072),\n",
       "   np.float64(0.009912992289334495),\n",
       "   np.float64(0.008471986717032578),\n",
       "   np.float64(0.008390339179695512),\n",
       "   np.float64(0.009142649327795573),\n",
       "   np.float64(0.009075686200205012),\n",
       "   np.float64(0.009580511351239017),\n",
       "   np.float64(0.008409225497769067),\n",
       "   np.float64(0.010158792168643387),\n",
       "   np.float64(0.010050689092160047),\n",
       "   np.float64(0.010925041228086506),\n",
       "   np.float64(0.00901762326419571),\n",
       "   np.float64(0.010219574087639119),\n",
       "   np.float64(0.008462376753900144),\n",
       "   np.float64(0.010268976295937992),\n",
       "   np.float64(0.00970522347403867),\n",
       "   np.float64(0.009347255372123059),\n",
       "   np.float64(0.010536591881145895),\n",
       "   np.float64(0.009656575814947908),\n",
       "   np.float64(0.00938289694530104),\n",
       "   np.float64(0.009485725934457161),\n",
       "   np.float64(0.009075676085463778),\n",
       "   np.float64(0.010153298809344362),\n",
       "   np.float64(0.009754237417349966),\n",
       "   np.float64(0.00934316929281924),\n",
       "   np.float64(0.008332712440048812),\n",
       "   np.float64(0.009912308848573363),\n",
       "   np.float64(0.008455629295244139),\n",
       "   np.float64(0.009249409728338074),\n",
       "   np.float64(0.008135374535528874),\n",
       "   np.float64(0.009967722954309933),\n",
       "   np.float64(0.008100037008360716),\n",
       "   np.float64(0.009110742503996211),\n",
       "   np.float64(0.007994019535243277),\n",
       "   np.float64(0.009279332123204531),\n",
       "   np.float64(0.011676690590126224),\n",
       "   np.float64(0.008084530155230845),\n",
       "   np.float64(0.00945641590675994),\n",
       "   np.float64(0.010511456241977126),\n",
       "   np.float64(0.009013837505034561),\n",
       "   np.float64(0.011024736967856474),\n",
       "   np.float64(0.011537720845077332),\n",
       "   np.float64(0.01116195368219837),\n",
       "   np.float64(0.010111562979443366),\n",
       "   np.float64(0.009856083709886413),\n",
       "   np.float64(0.009977454942680665),\n",
       "   np.float64(0.010408201740705692),\n",
       "   np.float64(0.010060117293184574),\n",
       "   np.float64(0.010801635786614519),\n",
       "   np.float64(0.010391248999515745),\n",
       "   np.float64(0.009557078964846559),\n",
       "   np.float64(0.010560297166972014),\n",
       "   np.float64(0.009802654165801677),\n",
       "   np.float64(0.010182231698202484),\n",
       "   np.float64(0.009975261383370437),\n",
       "   np.float64(0.009102429142220918),\n",
       "   np.float64(0.0074414011322201795),\n",
       "   np.float64(0.010546708144559463),\n",
       "   np.float64(0.010955566948388583),\n",
       "   np.float64(0.009125695242268722),\n",
       "   np.float64(0.009677858257939827),\n",
       "   np.float64(0.009158913298950754),\n",
       "   np.float64(0.010030637589968016),\n",
       "   np.float64(0.009612301194848825),\n",
       "   np.float64(0.01031997140266515),\n",
       "   np.float64(0.010299254795375249),\n",
       "   np.float64(0.01033755463952785),\n",
       "   np.float64(0.00996156330846609),\n",
       "   np.float64(0.01056506799942019),\n",
       "   np.float64(0.008380702890573199),\n",
       "   np.float64(0.007997989951956677),\n",
       "   np.float64(0.009565470680535506),\n",
       "   np.float64(0.009777865935771731),\n",
       "   np.float64(0.009785063110843767),\n",
       "   np.float64(0.009520954854556821),\n",
       "   np.float64(0.00999828206934013),\n",
       "   np.float64(0.008065849735174779),\n",
       "   np.float64(0.0097213116752215),\n",
       "   np.float64(0.010909528735198761),\n",
       "   np.float64(0.010068581633119474),\n",
       "   np.float64(0.009458125703578317),\n",
       "   np.float64(0.008259474527940822),\n",
       "   np.float64(0.010441299910269911),\n",
       "   np.float64(0.008736388374735475),\n",
       "   np.float64(0.008425419618210256),\n",
       "   np.float64(0.009051869787965901),\n",
       "   np.float64(0.009916901903674304),\n",
       "   np.float64(0.009517108886827403),\n",
       "   np.float64(0.00883951398525525),\n",
       "   np.float64(0.007899556204314652),\n",
       "   np.float64(0.007546311566853451),\n",
       "   np.float64(0.010280013391081302),\n",
       "   np.float64(0.008523270561226962),\n",
       "   np.float64(0.010155235539986688),\n",
       "   np.float64(0.009328463950253569),\n",
       "   np.float64(0.008749728273132938),\n",
       "   np.float64(0.009577448459370643),\n",
       "   np.float64(0.00989099215641171),\n",
       "   np.float64(0.008636270147650794),\n",
       "   np.float64(0.008850670285269277),\n",
       "   np.float64(0.009516080755481252),\n",
       "   np.float64(0.00955383435658456),\n",
       "   np.float64(0.01040036346121566),\n",
       "   np.float64(0.008379739409562298),\n",
       "   np.float64(0.008022496747004868),\n",
       "   np.float64(0.008982553684474799),\n",
       "   np.float64(0.010967197821621793),\n",
       "   np.float64(0.012829650277617333),\n",
       "   np.float64(0.00916386201254059),\n",
       "   np.float64(0.011240058436571953),\n",
       "   np.float64(0.008961861855398353),\n",
       "   np.float64(0.009041369961445339),\n",
       "   np.float64(0.00953305431659598),\n",
       "   np.float64(0.008952051952197738),\n",
       "   np.float64(0.009010832068102145),\n",
       "   np.float64(0.008500510990123934),\n",
       "   np.float64(0.008807269588776293),\n",
       "   np.float64(0.008619195462542311),\n",
       "   np.float64(0.008823690682000545),\n",
       "   np.float64(0.0092794098606454),\n",
       "   np.float64(0.010751544272954185),\n",
       "   np.float64(0.010762347931704462),\n",
       "   np.float64(0.007533099909747543),\n",
       "   np.float64(0.0076682539555417185),\n",
       "   np.float64(0.007913290848143146),\n",
       "   np.float64(0.007970334241318851),\n",
       "   np.float64(0.009153412321497708),\n",
       "   np.float64(0.010430359133316655),\n",
       "   np.float64(0.008703113125419132),\n",
       "   np.float64(0.008831142955154124),\n",
       "   np.float64(0.008367754332940304),\n",
       "   np.float64(0.009866246316859575),\n",
       "   np.float64(0.009530826685548338),\n",
       "   np.float64(0.008982414936457413),\n",
       "   np.float64(0.007688603728753423),\n",
       "   np.float64(0.009351332659083255),\n",
       "   np.float64(0.009744377918573773),\n",
       "   np.float64(0.01032782427176424),\n",
       "   np.float64(0.007780206752680315),\n",
       "   np.float64(0.008597929528708081),\n",
       "   np.float64(0.008162657186832087),\n",
       "   np.float64(0.008109016931294238),\n",
       "   np.float64(0.009202464600300749),\n",
       "   np.float64(0.008205724894433072),\n",
       "   np.float64(0.010241731601490096),\n",
       "   np.float64(0.010043058740858077),\n",
       "   np.float64(0.00762567193901872),\n",
       "   np.float64(0.009318910555611735),\n",
       "   np.float64(0.00743879200833285),\n",
       "   np.float64(0.010209396814602426),\n",
       "   np.float64(0.008425323620157221),\n",
       "   np.float64(0.009354168809756057),\n",
       "   np.float64(0.008063595716724051),\n",
       "   np.float64(0.008562370104002539),\n",
       "   np.float64(0.009065306606815555),\n",
       "   np.float64(0.008491833034557302),\n",
       "   np.float64(0.009382177101300045),\n",
       "   np.float64(0.009566218078778879),\n",
       "   np.float64(0.009488900471433054),\n",
       "   np.float64(0.009330373580247285),\n",
       "   np.float64(0.007524436831408055),\n",
       "   np.float64(0.010782882617752452),\n",
       "   np.float64(0.008111607608946705),\n",
       "   np.float64(0.008627804526956186),\n",
       "   np.float64(0.009271186525334783),\n",
       "   np.float64(0.009359052689733213),\n",
       "   np.float64(0.007624036410655673),\n",
       "   np.float64(0.010133973411231215),\n",
       "   np.float64(0.008789745427508456),\n",
       "   np.float64(0.008922679408114224),\n",
       "   np.float64(0.00837634961839562),\n",
       "   np.float64(0.009049188236480082),\n",
       "   np.float64(0.009045647492091398),\n",
       "   np.float64(0.008521208126775872)],\n",
       "  'val_total_affi_coeff_mean': [np.float64(0.4047946036853406),\n",
       "   np.float64(0.4077438752650696),\n",
       "   np.float64(0.41235019004692436),\n",
       "   np.float64(0.4195560579152179),\n",
       "   np.float64(0.42644783081229876),\n",
       "   np.float64(0.430531973361108),\n",
       "   np.float64(0.431522878011105),\n",
       "   np.float64(0.42810368194290416),\n",
       "   np.float64(0.4308708627692919),\n",
       "   np.float64(0.43123814732855525),\n",
       "   np.float64(0.42922314983518345),\n",
       "   np.float64(0.4325200357216241),\n",
       "   np.float64(0.4267545315807028),\n",
       "   np.float64(0.4249917172131451),\n",
       "   np.float64(0.42859877300327),\n",
       "   np.float64(0.4277094336038397),\n",
       "   np.float64(0.4308426070242891),\n",
       "   np.float64(0.4243933254722818),\n",
       "   np.float64(0.4209890379201624),\n",
       "   np.float64(0.4235860272010912),\n",
       "   np.float64(0.42420262276071635),\n",
       "   np.float64(0.42263890627038386),\n",
       "   np.float64(0.42357230182980277),\n",
       "   np.float64(0.4257363180725082),\n",
       "   np.float64(0.41687910883921486),\n",
       "   np.float64(0.421389289841286),\n",
       "   np.float64(0.41631517193570555),\n",
       "   np.float64(0.4184112625572225),\n",
       "   np.float64(0.4254139592408638),\n",
       "   np.float64(0.42161719967488565),\n",
       "   np.float64(0.42155515124640125),\n",
       "   np.float64(0.4200466810419352),\n",
       "   np.float64(0.4211154362702059),\n",
       "   np.float64(0.4243129180802528),\n",
       "   np.float64(0.42667048986972855),\n",
       "   np.float64(0.42632657851943623),\n",
       "   np.float64(0.42177846697940563),\n",
       "   np.float64(0.42399204493361237),\n",
       "   np.float64(0.41791658104716195),\n",
       "   np.float64(0.4222494835057694),\n",
       "   np.float64(0.418790290200337),\n",
       "   np.float64(0.42224654461689093),\n",
       "   np.float64(0.4225559427210984),\n",
       "   np.float64(0.4224519555695067),\n",
       "   np.float64(0.42274146486123976),\n",
       "   np.float64(0.4218330720322817),\n",
       "   np.float64(0.42195883260935196),\n",
       "   np.float64(0.418203964891362),\n",
       "   np.float64(0.4204328243297827),\n",
       "   np.float64(0.41671768591497127),\n",
       "   np.float64(0.4211499475983491),\n",
       "   np.float64(0.41844939872466913),\n",
       "   np.float64(0.4214016347544402),\n",
       "   np.float64(0.42171852967612594),\n",
       "   np.float64(0.41671845986842226),\n",
       "   np.float64(0.41836895899753634),\n",
       "   np.float64(0.4234683556993729),\n",
       "   np.float64(0.421208669831525),\n",
       "   np.float64(0.42001511993468865),\n",
       "   np.float64(0.4201306947372965),\n",
       "   np.float64(0.4237643791782363),\n",
       "   np.float64(0.4166438049154796),\n",
       "   np.float64(0.4179800139682701),\n",
       "   np.float64(0.42180852934239776),\n",
       "   np.float64(0.41281575147752275),\n",
       "   np.float64(0.4189418927296434),\n",
       "   np.float64(0.4190547270672982),\n",
       "   np.float64(0.42063448028523465),\n",
       "   np.float64(0.41658065802532684),\n",
       "   np.float64(0.4146047834965374),\n",
       "   np.float64(0.42081724778999624),\n",
       "   np.float64(0.4166841386481205),\n",
       "   np.float64(0.41819168270479223),\n",
       "   np.float64(0.4200304748401654),\n",
       "   np.float64(0.4158253370033061),\n",
       "   np.float64(0.4178302988000514),\n",
       "   np.float64(0.412032173720627),\n",
       "   np.float64(0.417190306675581),\n",
       "   np.float64(0.4156491681781169),\n",
       "   np.float64(0.41700520730310386),\n",
       "   np.float64(0.4177593973645528),\n",
       "   np.float64(0.41770783341057394),\n",
       "   np.float64(0.4151304865461331),\n",
       "   np.float64(0.42182597858838),\n",
       "   np.float64(0.41889633587299757),\n",
       "   np.float64(0.42235137768960695),\n",
       "   np.float64(0.42185779086524866),\n",
       "   np.float64(0.4182296356952222),\n",
       "   np.float64(0.4196632902557761),\n",
       "   np.float64(0.4166563070454382),\n",
       "   np.float64(0.41967710083864485),\n",
       "   np.float64(0.4181397196751721),\n",
       "   np.float64(0.41840034209600535),\n",
       "   np.float64(0.4183453720764754),\n",
       "   np.float64(0.4221710597641193),\n",
       "   np.float64(0.4188681102547851),\n",
       "   np.float64(0.4183198742921869),\n",
       "   np.float64(0.41354412395089657),\n",
       "   np.float64(0.4120976624014141),\n",
       "   np.float64(0.41581378747026765),\n",
       "   np.float64(0.4192596352954313),\n",
       "   np.float64(0.41572221829114875),\n",
       "   np.float64(0.41567544154582703),\n",
       "   np.float64(0.4162618885749852),\n",
       "   np.float64(0.41825936158579413),\n",
       "   np.float64(0.41911990925933146),\n",
       "   np.float64(0.4188343915293977),\n",
       "   np.float64(0.41480800604856705),\n",
       "   np.float64(0.4161100726455732),\n",
       "   np.float64(0.41586197648170886),\n",
       "   np.float64(0.4135136548070711),\n",
       "   np.float64(0.41754782690572567),\n",
       "   np.float64(0.41637656388835176),\n",
       "   np.float64(0.41718576069958985),\n",
       "   np.float64(0.41586041831090226),\n",
       "   np.float64(0.41356554639863896),\n",
       "   np.float64(0.41547352212055944),\n",
       "   np.float64(0.4127915821181899),\n",
       "   np.float64(0.41724062041982846),\n",
       "   np.float64(0.41629265423299916),\n",
       "   np.float64(0.4190295680831433),\n",
       "   np.float64(0.41769529164027674),\n",
       "   np.float64(0.41677363739613416),\n",
       "   np.float64(0.41858764544652993),\n",
       "   np.float64(0.4175920619245069),\n",
       "   np.float64(0.4194463362405413),\n",
       "   np.float64(0.4125928744616323),\n",
       "   np.float64(0.41755572637934907),\n",
       "   np.float64(0.41697677458770405),\n",
       "   np.float64(0.41773203030008876),\n",
       "   np.float64(0.41356029679399275),\n",
       "   np.float64(0.4132509803175545),\n",
       "   np.float64(0.4143272352009157),\n",
       "   np.float64(0.41622682704427894),\n",
       "   np.float64(0.41547741550898576),\n",
       "   np.float64(0.41147166398006135),\n",
       "   np.float64(0.4125094852105988),\n",
       "   np.float64(0.4158647218091186),\n",
       "   np.float64(0.4171649221253611),\n",
       "   np.float64(0.41223897907006585),\n",
       "   np.float64(0.41732015991154436),\n",
       "   np.float64(0.41764467003519584),\n",
       "   np.float64(0.414971822678564),\n",
       "   np.float64(0.4144468221690911),\n",
       "   np.float64(0.41514397354979277),\n",
       "   np.float64(0.4118838541275119),\n",
       "   np.float64(0.4147677818911867),\n",
       "   np.float64(0.4144367615871826),\n",
       "   np.float64(0.41610910059755346),\n",
       "   np.float64(0.4127005463538619),\n",
       "   np.float64(0.41257278614024157),\n",
       "   np.float64(0.417167335432585),\n",
       "   np.float64(0.4157072775804191),\n",
       "   np.float64(0.41205276313243755),\n",
       "   np.float64(0.41125389866352313),\n",
       "   np.float64(0.40675988804447594),\n",
       "   np.float64(0.4125156150207804),\n",
       "   np.float64(0.40874170328194437),\n",
       "   np.float64(0.4126642042517723),\n",
       "   np.float64(0.40992170783942505),\n",
       "   np.float64(0.41139106862821495),\n",
       "   np.float64(0.4164538568364564),\n",
       "   np.float64(0.41390088798867997),\n",
       "   np.float64(0.4115425132443556),\n",
       "   np.float64(0.41587465905873233),\n",
       "   np.float64(0.40901788262948147),\n",
       "   np.float64(0.4123062642607269),\n",
       "   np.float64(0.4139443890219038),\n",
       "   np.float64(0.41282454993420087),\n",
       "   np.float64(0.4117272688517577),\n",
       "   np.float64(0.4104857615158789),\n",
       "   np.float64(0.4081313623778863),\n",
       "   np.float64(0.40780290310715883),\n",
       "   np.float64(0.4117831296835991),\n",
       "   np.float64(0.4140460106372934),\n",
       "   np.float64(0.4166331410937816),\n",
       "   np.float64(0.41282296997128465),\n",
       "   np.float64(0.4117796913515571),\n",
       "   np.float64(0.41219373012299865),\n",
       "   np.float64(0.41260641940017345),\n",
       "   np.float64(0.4086257301962655),\n",
       "   np.float64(0.4106468014130995),\n",
       "   np.float64(0.40971310846917164),\n",
       "   np.float64(0.4113161532777928),\n",
       "   np.float64(0.4106636720183352),\n",
       "   np.float64(0.40904983657268484),\n",
       "   np.float64(0.40825474001389606),\n",
       "   np.float64(0.4098771634936687),\n",
       "   np.float64(0.4134574562422557),\n",
       "   np.float64(0.40578921993947625),\n",
       "   np.float64(0.4121732396423129),\n",
       "   np.float64(0.4109746430949772),\n",
       "   np.float64(0.40926646120591603),\n",
       "   np.float64(0.41170164720093344),\n",
       "   np.float64(0.4135316828711503),\n",
       "   np.float64(0.4155043622643104),\n",
       "   np.float64(0.41227555837468643),\n",
       "   np.float64(0.41173473933309956),\n",
       "   np.float64(0.41559678788300775),\n",
       "   np.float64(0.4136170631049497),\n",
       "   np.float64(0.4119042770851541),\n",
       "   np.float64(0.40978992390135616),\n",
       "   np.float64(0.4116492938054671),\n",
       "   np.float64(0.41385251137451967),\n",
       "   np.float64(0.4156330404602609),\n",
       "   np.float64(0.4107350561708408),\n",
       "   np.float64(0.41455393540295105),\n",
       "   np.float64(0.4088672992992606),\n",
       "   np.float64(0.4122509149637747),\n",
       "   np.float64(0.4159728916879072),\n",
       "   np.float64(0.41326059907508206),\n",
       "   np.float64(0.41349112858651443),\n",
       "   np.float64(0.41179552260738056),\n",
       "   np.float64(0.41296762542350673),\n",
       "   np.float64(0.41189607431734465),\n",
       "   np.float64(0.41158968872634794),\n",
       "   np.float64(0.41193670355131157),\n",
       "   np.float64(0.4107417099248293),\n",
       "   np.float64(0.41322705848057584),\n",
       "   np.float64(0.4151532441875333),\n",
       "   np.float64(0.4095623660042785),\n",
       "   np.float64(0.40894142235521436),\n",
       "   np.float64(0.4106914137740047),\n",
       "   np.float64(0.40960518679499647),\n",
       "   np.float64(0.4107449385674712),\n",
       "   np.float64(0.41047602675442185),\n",
       "   np.float64(0.413305239209069),\n",
       "   np.float64(0.41032268739516686),\n",
       "   np.float64(0.4078933799342377),\n",
       "   np.float64(0.4057143140882867),\n",
       "   np.float64(0.4100792385995584),\n",
       "   np.float64(0.4137693242173745),\n",
       "   np.float64(0.4152291761754964),\n",
       "   np.float64(0.410386816145874),\n",
       "   np.float64(0.4131164449637023),\n",
       "   np.float64(0.4127967374769969),\n",
       "   np.float64(0.4117201742537163),\n",
       "   np.float64(0.41504526641839334),\n",
       "   np.float64(0.4088619074106836),\n",
       "   np.float64(0.41152908568727675),\n",
       "   np.float64(0.4100789569082838),\n",
       "   np.float64(0.4113795945913747),\n",
       "   np.float64(0.41240055495510264),\n",
       "   np.float64(0.41255959558957095),\n",
       "   np.float64(0.414844496648904),\n",
       "   np.float64(0.40773357122244164),\n",
       "   np.float64(0.4105757790538589),\n",
       "   np.float64(0.4086785748831219),\n",
       "   np.float64(0.40703191492096213),\n",
       "   np.float64(0.40990118433763023),\n",
       "   np.float64(0.41082370755860653),\n",
       "   np.float64(0.412186478912277),\n",
       "   np.float64(0.4141516577536155),\n",
       "   np.float64(0.41095350536208247),\n",
       "   np.float64(0.410030519270164),\n",
       "   np.float64(0.41363342225072386),\n",
       "   np.float64(0.4070911919935667),\n",
       "   np.float64(0.4144865810864826),\n",
       "   np.float64(0.409708430648778),\n",
       "   np.float64(0.41234418492571606),\n",
       "   np.float64(0.4133132244450677),\n",
       "   np.float64(0.41061486720100804),\n",
       "   np.float64(0.40686726361720926),\n",
       "   np.float64(0.4089460912201869),\n",
       "   np.float64(0.4114680315390944),\n",
       "   np.float64(0.4125689444393779),\n",
       "   np.float64(0.4096531137852485),\n",
       "   np.float64(0.40799740531466344),\n",
       "   np.float64(0.4091575780706298),\n",
       "   np.float64(0.40983922754369534),\n",
       "   np.float64(0.414221710555187),\n",
       "   np.float64(0.41120792411651375),\n",
       "   np.float64(0.4026318456532535),\n",
       "   np.float64(0.41104889216015345),\n",
       "   np.float64(0.407848116062332),\n",
       "   np.float64(0.4103646056446194),\n",
       "   np.float64(0.4059793788494721),\n",
       "   np.float64(0.4110113938917187),\n",
       "   np.float64(0.4065378341939061),\n",
       "   np.float64(0.41137465138103735),\n",
       "   np.float64(0.4108738040944151),\n",
       "   np.float64(0.40992709951706785),\n",
       "   np.float64(0.4072784122113074),\n",
       "   np.float64(0.41057397434924153),\n",
       "   np.float64(0.4097746892929396),\n",
       "   np.float64(0.40786523802218877),\n",
       "   np.float64(0.40589220088581773),\n",
       "   np.float64(0.4090206677398268),\n",
       "   np.float64(0.4098791872548124),\n",
       "   np.float64(0.4098511258376772),\n",
       "   np.float64(0.4116112671502766),\n",
       "   np.float64(0.40526489500255236),\n",
       "   np.float64(0.4107527152043172),\n",
       "   np.float64(0.4089565742000141),\n",
       "   np.float64(0.41044627676213674),\n",
       "   np.float64(0.4060507925447211),\n",
       "   np.float64(0.40957442990036647),\n",
       "   np.float64(0.40874132140647523),\n",
       "   np.float64(0.4065231022344104),\n",
       "   np.float64(0.4080749438501241),\n",
       "   np.float64(0.40777888708965215),\n",
       "   np.float64(0.41111498666917023),\n",
       "   np.float64(0.41139298340269365),\n",
       "   np.float64(0.4106180598638297),\n",
       "   np.float64(0.4091126335590292),\n",
       "   np.float64(0.40822979025571593),\n",
       "   np.float64(0.40923257477382535),\n",
       "   np.float64(0.4097202816235323),\n",
       "   np.float64(0.4139308198095413),\n",
       "   np.float64(0.4075323191282074),\n",
       "   np.float64(0.40722989541362703),\n",
       "   np.float64(0.41168621814972395),\n",
       "   np.float64(0.40780293306604054),\n",
       "   np.float64(0.40899638295389906),\n",
       "   np.float64(0.40750975033856257),\n",
       "   np.float64(0.4116149776566109),\n",
       "   np.float64(0.410921280050383),\n",
       "   np.float64(0.41155696988391927),\n",
       "   np.float64(0.408829549860808),\n",
       "   np.float64(0.41235990545111983),\n",
       "   np.float64(0.41360891064721245),\n",
       "   np.float64(0.41117923652783905),\n",
       "   np.float64(0.40935518865407355),\n",
       "   np.float64(0.40971170183419464),\n",
       "   np.float64(0.41067647127105467),\n",
       "   np.float64(0.41009964593814086),\n",
       "   np.float64(0.4101225945302805),\n",
       "   np.float64(0.4117401764363813),\n",
       "   np.float64(0.4088508733701117),\n",
       "   np.float64(0.4074071578158107),\n",
       "   np.float64(0.41358271120668294),\n",
       "   np.float64(0.40808415623096717),\n",
       "   np.float64(0.41164255038511005),\n",
       "   np.float64(0.4099184396694473),\n",
       "   np.float64(0.4138497613635691),\n",
       "   np.float64(0.41299945944967603),\n",
       "   np.float64(0.41127884179877267),\n",
       "   np.float64(0.4118779006944914),\n",
       "   np.float64(0.4156288566688073),\n",
       "   np.float64(0.41171167860681407),\n",
       "   np.float64(0.41137537693309156),\n",
       "   np.float64(0.40778786704387804),\n",
       "   np.float64(0.4111006122578491),\n",
       "   np.float64(0.40973717588178044),\n",
       "   np.float64(0.41055425176205207),\n",
       "   np.float64(0.4122374004623628),\n",
       "   np.float64(0.4137147516666262),\n",
       "   np.float64(0.41236560321762605),\n",
       "   np.float64(0.4111289197612688),\n",
       "   np.float64(0.4105408044014954),\n",
       "   np.float64(0.41290657350408455),\n",
       "   np.float64(0.4091386847515266),\n",
       "   np.float64(0.4151855724352975),\n",
       "   np.float64(0.41242537179096245),\n",
       "   np.float64(0.4112526397995509),\n",
       "   np.float64(0.4097255119117126),\n",
       "   np.float64(0.4111078946834092),\n",
       "   np.float64(0.41439930609091324),\n",
       "   np.float64(0.41498887076733154),\n",
       "   np.float64(0.4110576913395425),\n",
       "   np.float64(0.41396169574411357),\n",
       "   np.float64(0.4115755379021847),\n",
       "   np.float64(0.4107500746106834),\n",
       "   np.float64(0.41173205550295655),\n",
       "   np.float64(0.41160995081332624),\n",
       "   np.float64(0.40676636923361986),\n",
       "   np.float64(0.41532546926260616),\n",
       "   np.float64(0.41263441314637506),\n",
       "   np.float64(0.4171120321284739),\n",
       "   np.float64(0.40898782564949454),\n",
       "   np.float64(0.40899259649795205),\n",
       "   np.float64(0.40856299726859996),\n",
       "   np.float64(0.41300933983649224),\n",
       "   np.float64(0.4095319267559747),\n",
       "   np.float64(0.41227649713910997),\n",
       "   np.float64(0.4117637435744118),\n",
       "   np.float64(0.4109485986546385),\n",
       "   np.float64(0.4112768708610451),\n",
       "   np.float64(0.4138757881452889),\n",
       "   np.float64(0.41327242244702755),\n",
       "   np.float64(0.4109020290556981),\n",
       "   np.float64(0.4097995506987401),\n",
       "   np.float64(0.41341139121134257),\n",
       "   np.float64(0.41363746532893897),\n",
       "   np.float64(0.4102133523206464),\n",
       "   np.float64(0.41231639348083665),\n",
       "   np.float64(0.4091608339184106),\n",
       "   np.float64(0.41209266695386537),\n",
       "   np.float64(0.40855686882973474),\n",
       "   np.float64(0.40869321384523105),\n",
       "   np.float64(0.40895274005764215),\n",
       "   np.float64(0.40885879419098425),\n",
       "   np.float64(0.4106399728893042),\n",
       "   np.float64(0.41100688224296217),\n",
       "   np.float64(0.4134875304662511),\n",
       "   np.float64(0.410118879998512),\n",
       "   np.float64(0.4110195784042294),\n",
       "   np.float64(0.4111534737153819),\n",
       "   np.float64(0.4091677909143453),\n",
       "   np.float64(0.41447100459350505),\n",
       "   np.float64(0.41112982969731526),\n",
       "   np.float64(0.412519755954053),\n",
       "   np.float64(0.4089825889765135),\n",
       "   np.float64(0.41396262438534326),\n",
       "   np.float64(0.4093538442674714),\n",
       "   np.float64(0.41181484020791326),\n",
       "   np.float64(0.41023261478429296),\n",
       "   np.float64(0.4079070225120135),\n",
       "   np.float64(0.4125171771633005),\n",
       "   np.float64(0.4120307702415144),\n",
       "   np.float64(0.4136405542588036),\n",
       "   np.float64(0.4111761548643694),\n",
       "   np.float64(0.41367741220493803),\n",
       "   np.float64(0.41293366373192725),\n",
       "   np.float64(0.4149770029692885),\n",
       "   np.float64(0.4127876164159251),\n",
       "   np.float64(0.4152771920716571),\n",
       "   np.float64(0.4104079645225355),\n",
       "   np.float64(0.4130711018021075),\n",
       "   np.float64(0.41396915267016277),\n",
       "   np.float64(0.40822607846899195),\n",
       "   np.float64(0.41260442246758966),\n",
       "   np.float64(0.4121682371399631),\n",
       "   np.float64(0.41455576563525176),\n",
       "   np.float64(0.4136372708626629),\n",
       "   np.float64(0.41394850476594075),\n",
       "   np.float64(0.4121158992621201),\n",
       "   np.float64(0.4124599158737932),\n",
       "   np.float64(0.41033520481641295),\n",
       "   np.float64(0.41002699448993785),\n",
       "   np.float64(0.41436770949061796),\n",
       "   np.float64(0.41568042892538026),\n",
       "   np.float64(0.41318946115958266),\n",
       "   np.float64(0.41700162322143414),\n",
       "   np.float64(0.4164333174502124),\n",
       "   np.float64(0.41432188146696186),\n",
       "   np.float64(0.4122513355264002)],\n",
       "  'val_total_affi_coeff_std': [np.float64(0.063609698192252),\n",
       "   np.float64(0.06435619728607331),\n",
       "   np.float64(0.06361158096093032),\n",
       "   np.float64(0.06256387599513105),\n",
       "   np.float64(0.06212244301025179),\n",
       "   np.float64(0.05989617970184475),\n",
       "   np.float64(0.05983557591601035),\n",
       "   np.float64(0.059827847410151055),\n",
       "   np.float64(0.05902186354761552),\n",
       "   np.float64(0.06094886646465415),\n",
       "   np.float64(0.05986988791276432),\n",
       "   np.float64(0.05867709823237838),\n",
       "   np.float64(0.05980695541733493),\n",
       "   np.float64(0.058515701417003935),\n",
       "   np.float64(0.0594225610024833),\n",
       "   np.float64(0.05918212808584044),\n",
       "   np.float64(0.0606895448868493),\n",
       "   np.float64(0.06080681460811847),\n",
       "   np.float64(0.05904948697073662),\n",
       "   np.float64(0.0589568808691365),\n",
       "   np.float64(0.05845301074485522),\n",
       "   np.float64(0.05968791964529919),\n",
       "   np.float64(0.057596410721907375),\n",
       "   np.float64(0.05788723915343191),\n",
       "   np.float64(0.06323019071804231),\n",
       "   np.float64(0.060329435874218025),\n",
       "   np.float64(0.05905918297117967),\n",
       "   np.float64(0.06019373857396966),\n",
       "   np.float64(0.05768428672425446),\n",
       "   np.float64(0.059065497488664145),\n",
       "   np.float64(0.058551969193364396),\n",
       "   np.float64(0.058330722222949154),\n",
       "   np.float64(0.05960050264139296),\n",
       "   np.float64(0.05632784371635843),\n",
       "   np.float64(0.05734630489968034),\n",
       "   np.float64(0.05861327643899614),\n",
       "   np.float64(0.05831839829982348),\n",
       "   np.float64(0.059950748583247976),\n",
       "   np.float64(0.06097474530499105),\n",
       "   np.float64(0.05832584632410005),\n",
       "   np.float64(0.060112933750195635),\n",
       "   np.float64(0.059620204891809095),\n",
       "   np.float64(0.05653940681877046),\n",
       "   np.float64(0.060656592942309524),\n",
       "   np.float64(0.055765856138989094),\n",
       "   np.float64(0.061910576166440655),\n",
       "   np.float64(0.057687102678785795),\n",
       "   np.float64(0.061547024921444225),\n",
       "   np.float64(0.05807454421012818),\n",
       "   np.float64(0.05948240840475076),\n",
       "   np.float64(0.06012407861886929),\n",
       "   np.float64(0.05712668241004075),\n",
       "   np.float64(0.06253105604393445),\n",
       "   np.float64(0.05836947095375069),\n",
       "   np.float64(0.05819149037616823),\n",
       "   np.float64(0.060058397157192855),\n",
       "   np.float64(0.058214969415239196),\n",
       "   np.float64(0.06142394430883833),\n",
       "   np.float64(0.05812159422269004),\n",
       "   np.float64(0.059659897682520564),\n",
       "   np.float64(0.06014372083613839),\n",
       "   np.float64(0.05930280647172194),\n",
       "   np.float64(0.06093615678375622),\n",
       "   np.float64(0.0556441062195906),\n",
       "   np.float64(0.058887402986915124),\n",
       "   np.float64(0.0563321493811623),\n",
       "   np.float64(0.05814749950983887),\n",
       "   np.float64(0.05756166640819561),\n",
       "   np.float64(0.05850452383569388),\n",
       "   np.float64(0.05855401680087071),\n",
       "   np.float64(0.05933982600289868),\n",
       "   np.float64(0.060474978398244186),\n",
       "   np.float64(0.05992360562707384),\n",
       "   np.float64(0.057625147386183635),\n",
       "   np.float64(0.059119515897986266),\n",
       "   np.float64(0.05809933402677976),\n",
       "   np.float64(0.05844660365082889),\n",
       "   np.float64(0.05640748975485776),\n",
       "   np.float64(0.05829398397902094),\n",
       "   np.float64(0.05905287327957211),\n",
       "   np.float64(0.055324283365934104),\n",
       "   np.float64(0.059361633881844136),\n",
       "   np.float64(0.05551739508416889),\n",
       "   np.float64(0.05761857594936243),\n",
       "   np.float64(0.05903712034006638),\n",
       "   np.float64(0.05640325234615476),\n",
       "   np.float64(0.057365362340039035),\n",
       "   np.float64(0.05868651969049624),\n",
       "   np.float64(0.05764455553209047),\n",
       "   np.float64(0.05924449079479956),\n",
       "   np.float64(0.05774955437467964),\n",
       "   np.float64(0.05636011565505738),\n",
       "   np.float64(0.05782057809073551),\n",
       "   np.float64(0.05632817709860757),\n",
       "   np.float64(0.054906323736693644),\n",
       "   np.float64(0.05639038718727645),\n",
       "   np.float64(0.0600262613747081),\n",
       "   np.float64(0.05849358264700623),\n",
       "   np.float64(0.059349325855039034),\n",
       "   np.float64(0.05770273073143176),\n",
       "   np.float64(0.055878779422699955),\n",
       "   np.float64(0.061553458526453865),\n",
       "   np.float64(0.054826877232882),\n",
       "   np.float64(0.059180195677358036),\n",
       "   np.float64(0.05595397399560444),\n",
       "   np.float64(0.052990658426310754),\n",
       "   np.float64(0.05568815606136121),\n",
       "   np.float64(0.0576495391801574),\n",
       "   np.float64(0.054020942834293706),\n",
       "   np.float64(0.05965183258471232),\n",
       "   np.float64(0.05862055904706509),\n",
       "   np.float64(0.057571476636555124),\n",
       "   np.float64(0.057301177943340775),\n",
       "   np.float64(0.057293064155348815),\n",
       "   np.float64(0.0541503171239219),\n",
       "   np.float64(0.05648309387918035),\n",
       "   np.float64(0.055386945880353526),\n",
       "   np.float64(0.05697079118074646),\n",
       "   np.float64(0.05858138138213347),\n",
       "   np.float64(0.053324439333740335),\n",
       "   np.float64(0.05190987643391569),\n",
       "   np.float64(0.05734635505259225),\n",
       "   np.float64(0.0560068397145997),\n",
       "   np.float64(0.05623443341124833),\n",
       "   np.float64(0.05707141313896683),\n",
       "   np.float64(0.05492277272813582),\n",
       "   np.float64(0.05825254898591766),\n",
       "   np.float64(0.05765183581177734),\n",
       "   np.float64(0.051089909076528585),\n",
       "   np.float64(0.05416905330286565),\n",
       "   np.float64(0.055237936719518516),\n",
       "   np.float64(0.05579366107749989),\n",
       "   np.float64(0.055817467001804386),\n",
       "   np.float64(0.05815101261824518),\n",
       "   np.float64(0.05886488041927551),\n",
       "   np.float64(0.05672932879785733),\n",
       "   np.float64(0.05829081606712656),\n",
       "   np.float64(0.056507895009010115),\n",
       "   np.float64(0.05352596763256977),\n",
       "   np.float64(0.05817798477537501),\n",
       "   np.float64(0.058821189394856456),\n",
       "   np.float64(0.05511159089373263),\n",
       "   np.float64(0.05722923227702505),\n",
       "   np.float64(0.05573553897060436),\n",
       "   np.float64(0.056769253651151395),\n",
       "   np.float64(0.054716246357915214),\n",
       "   np.float64(0.052675481568667894),\n",
       "   np.float64(0.05771567051953221),\n",
       "   np.float64(0.0549374900906182),\n",
       "   np.float64(0.05445490176449686),\n",
       "   np.float64(0.05576099362077081),\n",
       "   np.float64(0.05643913052501595),\n",
       "   np.float64(0.05682735751311231),\n",
       "   np.float64(0.055262883102191775),\n",
       "   np.float64(0.05809234836118365),\n",
       "   np.float64(0.05878118604579654),\n",
       "   np.float64(0.056881947533896096),\n",
       "   np.float64(0.0564230598160137),\n",
       "   np.float64(0.05958246817922676),\n",
       "   np.float64(0.056485532849479575),\n",
       "   np.float64(0.05560012522751828),\n",
       "   np.float64(0.05767545235890158),\n",
       "   np.float64(0.059167242143461016),\n",
       "   np.float64(0.058878611051152496),\n",
       "   np.float64(0.05652363788731664),\n",
       "   np.float64(0.05863155752249857),\n",
       "   np.float64(0.05795478213601519),\n",
       "   np.float64(0.05775005052336822),\n",
       "   np.float64(0.055898357791828814),\n",
       "   np.float64(0.055406742672172604),\n",
       "   np.float64(0.0576590082958988),\n",
       "   np.float64(0.05687414504821671),\n",
       "   np.float64(0.057792293897773225),\n",
       "   np.float64(0.053802421774927105),\n",
       "   np.float64(0.058473287005979785),\n",
       "   np.float64(0.058290972971834394),\n",
       "   np.float64(0.05575860803315289),\n",
       "   np.float64(0.054677876434083436),\n",
       "   np.float64(0.05764573438794532),\n",
       "   np.float64(0.058911105302225264),\n",
       "   np.float64(0.05645987747710404),\n",
       "   np.float64(0.058834675593185386),\n",
       "   np.float64(0.05739555705439024),\n",
       "   np.float64(0.057424200228687786),\n",
       "   np.float64(0.05700765493548908),\n",
       "   np.float64(0.054223537245692656),\n",
       "   np.float64(0.05737487538897939),\n",
       "   np.float64(0.05725785495172911),\n",
       "   np.float64(0.05240616087833689),\n",
       "   np.float64(0.05871060139035681),\n",
       "   np.float64(0.0556192454353613),\n",
       "   np.float64(0.05671498130304284),\n",
       "   np.float64(0.055306679204502586),\n",
       "   np.float64(0.057339071759839065),\n",
       "   np.float64(0.0546274689232972),\n",
       "   np.float64(0.05437435490880098),\n",
       "   np.float64(0.056499515303038754),\n",
       "   np.float64(0.05337086444555961),\n",
       "   np.float64(0.05541980166315578),\n",
       "   np.float64(0.05522205304158596),\n",
       "   np.float64(0.055418223452647596),\n",
       "   np.float64(0.0531452338035201),\n",
       "   np.float64(0.056268837988058366),\n",
       "   np.float64(0.05566016795168797),\n",
       "   np.float64(0.0540932589368762),\n",
       "   np.float64(0.05599709021324866),\n",
       "   np.float64(0.05397235150750986),\n",
       "   np.float64(0.051323857638753384),\n",
       "   np.float64(0.05669492258789789),\n",
       "   np.float64(0.056787709287877686),\n",
       "   np.float64(0.05638008026078546),\n",
       "   np.float64(0.054677067049592694),\n",
       "   np.float64(0.05337153670034006),\n",
       "   np.float64(0.05368377417104772),\n",
       "   np.float64(0.055495163128805945),\n",
       "   np.float64(0.05853643044164614),\n",
       "   np.float64(0.05587435120923781),\n",
       "   np.float64(0.05603536664032285),\n",
       "   np.float64(0.05353825077368964),\n",
       "   np.float64(0.052525568069508935),\n",
       "   np.float64(0.05509103228620468),\n",
       "   np.float64(0.055505997461196575),\n",
       "   np.float64(0.05609980096699822),\n",
       "   np.float64(0.05723760704150802),\n",
       "   np.float64(0.05463431134184622),\n",
       "   np.float64(0.052904477625093004),\n",
       "   np.float64(0.056651969502390695),\n",
       "   np.float64(0.05412133947279329),\n",
       "   np.float64(0.058907383532741685),\n",
       "   np.float64(0.0536711571956796),\n",
       "   np.float64(0.055069593020801735),\n",
       "   np.float64(0.051591525184709205),\n",
       "   np.float64(0.0546636819311645),\n",
       "   np.float64(0.05636804518517139),\n",
       "   np.float64(0.05846918742417526),\n",
       "   np.float64(0.05619058053729536),\n",
       "   np.float64(0.054187250055663876),\n",
       "   np.float64(0.05866998086726064),\n",
       "   np.float64(0.0580407028289128),\n",
       "   np.float64(0.053879615760750184),\n",
       "   np.float64(0.051847745137952254),\n",
       "   np.float64(0.05221652248804191),\n",
       "   np.float64(0.05528566153361287),\n",
       "   np.float64(0.05427490036901665),\n",
       "   np.float64(0.054727683910919034),\n",
       "   np.float64(0.05726064945972344),\n",
       "   np.float64(0.05541505952500153),\n",
       "   np.float64(0.052970568575850946),\n",
       "   np.float64(0.05581192459210525),\n",
       "   np.float64(0.05702685948292131),\n",
       "   np.float64(0.05511441127211072),\n",
       "   np.float64(0.05523182685114893),\n",
       "   np.float64(0.0536536331181576),\n",
       "   np.float64(0.05789431752329674),\n",
       "   np.float64(0.05461205701554162),\n",
       "   np.float64(0.05428377527093215),\n",
       "   np.float64(0.05588826837345317),\n",
       "   np.float64(0.053770831065551715),\n",
       "   np.float64(0.051946884352885796),\n",
       "   np.float64(0.058547481487390376),\n",
       "   np.float64(0.05541575507706013),\n",
       "   np.float64(0.05671107356176573),\n",
       "   np.float64(0.0559087924427751),\n",
       "   np.float64(0.05447891163059547),\n",
       "   np.float64(0.05357067984398461),\n",
       "   np.float64(0.056236461422065794),\n",
       "   np.float64(0.056514061038890624),\n",
       "   np.float64(0.0548296263248783),\n",
       "   np.float64(0.05572840752520922),\n",
       "   np.float64(0.05769885608643957),\n",
       "   np.float64(0.05628564412327868),\n",
       "   np.float64(0.05372435781879183),\n",
       "   np.float64(0.05584634607051319),\n",
       "   np.float64(0.05725100897959967),\n",
       "   np.float64(0.05587391171361749),\n",
       "   np.float64(0.056142257290752194),\n",
       "   np.float64(0.055990672747722826),\n",
       "   np.float64(0.0553185089011547),\n",
       "   np.float64(0.06090654096555077),\n",
       "   np.float64(0.054243772756740534),\n",
       "   np.float64(0.05597120627904997),\n",
       "   np.float64(0.057385477065589954),\n",
       "   np.float64(0.05663550805811988),\n",
       "   np.float64(0.057523349045316634),\n",
       "   np.float64(0.05647927674561016),\n",
       "   np.float64(0.05490886374286349),\n",
       "   np.float64(0.055669303768100364),\n",
       "   np.float64(0.05199741422621863),\n",
       "   np.float64(0.05532723794810762),\n",
       "   np.float64(0.05496284462164696),\n",
       "   np.float64(0.055367898337075346),\n",
       "   np.float64(0.0544383556439486),\n",
       "   np.float64(0.05118816794115319),\n",
       "   np.float64(0.05421292952739166),\n",
       "   np.float64(0.058137552818443655),\n",
       "   np.float64(0.05326779726190098),\n",
       "   np.float64(0.05269026587550944),\n",
       "   np.float64(0.053166755510261274),\n",
       "   np.float64(0.05300005444241004),\n",
       "   np.float64(0.05558775668219996),\n",
       "   np.float64(0.05812170024218809),\n",
       "   np.float64(0.05767207769364896),\n",
       "   np.float64(0.05732843811091309),\n",
       "   np.float64(0.05586294875431776),\n",
       "   np.float64(0.05841850093100444),\n",
       "   np.float64(0.05665006264134049),\n",
       "   np.float64(0.05381411525331037),\n",
       "   np.float64(0.0554436280310072),\n",
       "   np.float64(0.05475484575188015),\n",
       "   np.float64(0.0522489443514582),\n",
       "   np.float64(0.050936562787852004),\n",
       "   np.float64(0.05603031036119476),\n",
       "   np.float64(0.054092788979698554),\n",
       "   np.float64(0.05258941150482319),\n",
       "   np.float64(0.054625035391343564),\n",
       "   np.float64(0.055937067432287456),\n",
       "   np.float64(0.05169502247945444),\n",
       "   np.float64(0.05559316794163685),\n",
       "   np.float64(0.056065855764186705),\n",
       "   np.float64(0.05597852017557612),\n",
       "   np.float64(0.05239661093350842),\n",
       "   np.float64(0.05374860031233719),\n",
       "   np.float64(0.05763899467279567),\n",
       "   np.float64(0.05561043219121859),\n",
       "   np.float64(0.05409819593481294),\n",
       "   np.float64(0.05480259197210419),\n",
       "   np.float64(0.05643352087563798),\n",
       "   np.float64(0.05258385191652116),\n",
       "   np.float64(0.057165723659690415),\n",
       "   np.float64(0.05538106521601293),\n",
       "   np.float64(0.05490933846154273),\n",
       "   np.float64(0.05383430946014604),\n",
       "   np.float64(0.056896987599594065),\n",
       "   np.float64(0.05223975247151528),\n",
       "   np.float64(0.05501942446691595),\n",
       "   np.float64(0.05243914191372065),\n",
       "   np.float64(0.05443763858437437),\n",
       "   np.float64(0.0538133516583684),\n",
       "   np.float64(0.054062194069194214),\n",
       "   np.float64(0.05299268823786759),\n",
       "   np.float64(0.05916420220233059),\n",
       "   np.float64(0.05259214991352625),\n",
       "   np.float64(0.053313054635801216),\n",
       "   np.float64(0.053406579386242034),\n",
       "   np.float64(0.053397365436266685),\n",
       "   np.float64(0.05750032556172063),\n",
       "   np.float64(0.05729704579178379),\n",
       "   np.float64(0.056637702253678264),\n",
       "   np.float64(0.055688628392286964),\n",
       "   np.float64(0.05590601142574771),\n",
       "   np.float64(0.05691709646327214),\n",
       "   np.float64(0.05386493050690453),\n",
       "   np.float64(0.054142183970648676),\n",
       "   np.float64(0.05701674872101201),\n",
       "   np.float64(0.0580049853438083),\n",
       "   np.float64(0.054302589416873354),\n",
       "   np.float64(0.05902732498391878),\n",
       "   np.float64(0.052218051343202274),\n",
       "   np.float64(0.053292700383108564),\n",
       "   np.float64(0.05550420811742595),\n",
       "   np.float64(0.05681853979617872),\n",
       "   np.float64(0.05509550505906788),\n",
       "   np.float64(0.05461500242844515),\n",
       "   np.float64(0.05529401087942585),\n",
       "   np.float64(0.051323552746228386),\n",
       "   np.float64(0.05577959893369226),\n",
       "   np.float64(0.05574347793141139),\n",
       "   np.float64(0.05527973769080387),\n",
       "   np.float64(0.0562964927340913),\n",
       "   np.float64(0.05799728227431899),\n",
       "   np.float64(0.058685954412330946),\n",
       "   np.float64(0.058304693048450096),\n",
       "   np.float64(0.05487675983907545),\n",
       "   np.float64(0.057442708541896884),\n",
       "   np.float64(0.05394925594569678),\n",
       "   np.float64(0.055709688503387644),\n",
       "   np.float64(0.05573701933181668),\n",
       "   np.float64(0.054476715130142975),\n",
       "   np.float64(0.053824446245786015),\n",
       "   np.float64(0.05481346657017291),\n",
       "   np.float64(0.05523440328441584),\n",
       "   np.float64(0.05513797504732712),\n",
       "   np.float64(0.056798280108756136),\n",
       "   np.float64(0.05268513478202265),\n",
       "   np.float64(0.053889224513162415),\n",
       "   np.float64(0.05837122195866787),\n",
       "   np.float64(0.05599165486572306),\n",
       "   np.float64(0.05882721832459539),\n",
       "   np.float64(0.0544614339585099),\n",
       "   np.float64(0.055196395102549485),\n",
       "   np.float64(0.054565675614319874),\n",
       "   np.float64(0.0555768712548335),\n",
       "   np.float64(0.055889256775850694),\n",
       "   np.float64(0.055070275328180665),\n",
       "   np.float64(0.05546901344053706),\n",
       "   np.float64(0.053408039413286215),\n",
       "   np.float64(0.05473974955617406),\n",
       "   np.float64(0.05597215711989021),\n",
       "   np.float64(0.054267601461737223),\n",
       "   np.float64(0.05364991298364621),\n",
       "   np.float64(0.05482769298268099),\n",
       "   np.float64(0.053347700876517405),\n",
       "   np.float64(0.05273947864777165),\n",
       "   np.float64(0.05722302547849349),\n",
       "   np.float64(0.05635131659798539),\n",
       "   np.float64(0.053503717339715935),\n",
       "   np.float64(0.05375192317522345),\n",
       "   np.float64(0.05432589913826688),\n",
       "   np.float64(0.05457666304220501),\n",
       "   np.float64(0.05523679140282788),\n",
       "   np.float64(0.05225644069373382),\n",
       "   np.float64(0.055631233122561116),\n",
       "   np.float64(0.05502336751412088),\n",
       "   np.float64(0.05307588616455659),\n",
       "   np.float64(0.05451852357973493),\n",
       "   np.float64(0.05274208926534989),\n",
       "   np.float64(0.051628433557528705),\n",
       "   np.float64(0.05635508279865327),\n",
       "   np.float64(0.055089186178421005),\n",
       "   np.float64(0.0551856596848624),\n",
       "   np.float64(0.05241120246544147),\n",
       "   np.float64(0.05333188035350637),\n",
       "   np.float64(0.055891831615783164),\n",
       "   np.float64(0.052175956700641284),\n",
       "   np.float64(0.05273054252381593),\n",
       "   np.float64(0.05672012454234421),\n",
       "   np.float64(0.05400298904499799),\n",
       "   np.float64(0.05281210861758571),\n",
       "   np.float64(0.0533735438964258),\n",
       "   np.float64(0.05621679419583723),\n",
       "   np.float64(0.05428034413685248),\n",
       "   np.float64(0.05363537806414542),\n",
       "   np.float64(0.054552628255258154),\n",
       "   np.float64(0.05588444448467218),\n",
       "   np.float64(0.05374896087687322),\n",
       "   np.float64(0.05453849847975044),\n",
       "   np.float64(0.052484471167419486)]}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the data batch format again\n",
    "print(\"Check the data batch format:\")\n",
    "print(f\"Train batch size: {len(train_batch)}\")\n",
    "print(f\"Val batch size: {len(val_batch)}\")\n",
    "print(f\"First task in train batch support set size: {train_batch[0]['support']['labels'].shape}\")\n",
    "print(f\"First task in train batch query set size: {train_batch[0]['query']['labels'].shape}\")\n",
    "print(f\"First task in val batch support set size: {val_batch[0]['support']['labels'].shape}\")\n",
    "print(f\"First task in val batch query set size: {val_batch[0]['query']['labels'].shape}\")\n",
    "\n",
    "# initialize the MAMLRegressor\n",
    "print(\"\\nInitializing MAMLRegressor...\")\n",
    "maml_model = MAMLRegressor()\n",
    "maml_model.load_model('./models/gatedgcn.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Training...\n",
      "============================================================\n",
      "Training results:\n",
      "Total Loss: 0.9161\n",
      "MDN Loss: 0.9160\n",
      "Affinity Coefficient: 0.6065\n",
      "Atom Loss: 0.0022\n",
      "Bond Loss: 0.1107\n",
      "Total Affinity Coefficient: 0.6062\n",
      "Learning Rate: 0.001000\n",
      "Predicted values shape: (100,)\n"
     ]
    }
   ],
   "source": [
    "# perform a training iteration\n",
    "print(\"=\" * 60)\n",
    "print(\"Training...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# train an epoch\n",
    "epoch = 0\n",
    "train_losses, train_preds = maml_model.run_train_iter(train_batch, epoch)\n",
    "\n",
    "print(\"Training results:\")\n",
    "print(f\"Total Loss: {train_losses['total_loss']:.4f}\")\n",
    "print(f\"MDN Loss: {train_losses['mdn_loss']:.4f}\")\n",
    "print(f\"Affinity Coefficient: {train_losses['affi_coeffs']:.4f}\")\n",
    "print(f\"Atom Loss: {train_losses['atom_loss']:.4f}\")\n",
    "print(f\"Bond Loss: {train_losses['bond_loss']:.4f}\")\n",
    "print(f\"Total Affinity Coefficient: {train_losses['total_affi_coeff']:.4f}\")\n",
    "print(f\"Learning Rate: {train_losses['learning_rate']:.6f}\")\n",
    "print(f\"Predicted values shape: {train_preds.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Evaluating...\n",
      "============================================================\n",
      "Validation results:\n",
      "Total Loss: 1.2348\n",
      "MDN Loss: 1.2346\n",
      "Affinity Coefficient: 0.5213\n",
      "Atom Loss: 0.0017\n",
      "Bond Loss: 0.1535\n",
      "Total Affinity Coefficient: 0.5213\n",
      "Predicted values shape: (192,)\n"
     ]
    }
   ],
   "source": [
    "# perform validation iteration\n",
    "print(\"=\" * 60)\n",
    "print(\"Evaluating...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# validation\n",
    "val_losses, val_preds = maml_model.run_validation_iter(val_batch)\n",
    "\n",
    "print(\"Validation results:\")\n",
    "print(f\"Total Loss: {val_losses['total_loss']:.4f}\")\n",
    "print(f\"MDN Loss: {val_losses['mdn_loss']:.4f}\")\n",
    "print(f\"Affinity Coefficient: {val_losses['affi_coeffs']:.4f}\")\n",
    "print(f\"Atom Loss: {val_losses['atom_loss']:.4f}\")\n",
    "print(f\"Bond Loss: {val_losses['bond_loss']:.4f}\")\n",
    "print(f\"Total Affinity Coefficient: {val_losses['total_affi_coeff']:.4f}\")\n",
    "print(f\"Predicted values shape: {val_preds.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Demonstrate multiple epochs of training loop...\n",
      "============================================================\n",
      "\n",
      "Epoch 1/3\n",
      "----------------------------------------\n",
      "Train - Total Loss: 0.8793, Affinity Coeff: 0.5860, LR: 0.001000\n",
      "Val   - Total Loss: 1.2373, Affinity Coeff: 0.5264\n",
      "\n",
      "Epoch 2/3\n",
      "----------------------------------------\n",
      "Train - Total Loss: 0.8457, Affinity Coeff: 0.5949, LR: 0.001000\n",
      "Val   - Total Loss: 1.2400, Affinity Coeff: 0.5334\n",
      "\n",
      "Epoch 3/3\n",
      "----------------------------------------\n",
      "Train - Total Loss: 0.8192, Affinity Coeff: 0.5962, LR: 0.000998\n",
      "Val   - Total Loss: 1.2412, Affinity Coeff: 0.5317\n",
      "\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "# demonstrate multiple epochs of training loop\n",
    "print(\"=\" * 60)\n",
    "print(\"Demonstrate multiple epochs of training loop...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "num_epochs = 3\n",
    "train_history = []\n",
    "val_history = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # training\n",
    "    train_losses, train_preds = maml_model.run_train_iter(train_batch, epoch)\n",
    "    train_history.append(train_losses)\n",
    "    \n",
    "    # validation\n",
    "    val_losses, val_preds = maml_model.run_validation_iter(val_batch)\n",
    "    val_history.append(val_losses)\n",
    "    \n",
    "    print(f\"Train - Total Loss: {train_losses['total_loss']:.4f}, \"\n",
    "          f\"Affinity Coeff: {train_losses['total_affi_coeff']:.4f}, \"\n",
    "          f\"LR: {train_losses['learning_rate']:.6f}\")\n",
    "    print(f\"Val   - Total Loss: {val_losses['total_loss']:.4f}, \"\n",
    "          f\"Affinity Coeff: {val_losses['total_affi_coeff']:.4f}\")\n",
    "\n",
    "print(\"\\nTraining completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Training process analysis\n",
      "============================================================\n",
      "Training loss changes:\n",
      "  Epoch 1: 0.8793\n",
      "  Epoch 2: 0.8457\n",
      "  Epoch 3: 0.8192\n",
      "\n",
      "Validation loss changes:\n",
      "  Epoch 1: 1.2373\n",
      "  Epoch 2: 1.2400\n",
      "  Epoch 3: 1.2412\n",
      "\n",
      "Learnable inner-loop learning rates:\n",
      "  Parameter group 0: 0.003903\n",
      "  Parameter group 1: 5.531588\n",
      "  Parameter group 2: 3.829666\n",
      "\n",
      "Current outer-loop learning rate: 0.000998\n",
      "Total number of model parameters: 1050225\n"
     ]
    }
   ],
   "source": [
    "# analyze the training process and results\n",
    "print(\"=\" * 60)\n",
    "print(\"Training process analysis\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# extract the key metrics\n",
    "train_total_losses = [h['total_loss'].item() for h in train_history]\n",
    "val_total_losses = [h['total_loss'].item() for h in val_history]\n",
    "train_affi_coeffs = [h['total_affi_coeff'].item() for h in train_history]\n",
    "val_affi_coeffs = [h['total_affi_coeff'].item() for h in val_history]\n",
    "\n",
    "print(\"Training loss changes:\")\n",
    "for i, loss in enumerate(train_total_losses):\n",
    "    print(f\"  Epoch {i+1}: {loss:.4f}\")\n",
    "\n",
    "print(\"\\nValidation loss changes:\")\n",
    "for i, loss in enumerate(val_total_losses):\n",
    "    print(f\"  Epoch {i+1}: {loss:.4f}\")\n",
    "\n",
    "# check the learnable inner-loop learning rates\n",
    "flag = 0\n",
    "if maml_model.learnable_inner_opt_params:\n",
    "    print(f\"\\nLearnable inner-loop learning rates:\")\n",
    "    for i, lr in enumerate(maml_model.lrs):\n",
    "        print(f\"  Parameter group {i}: {lr.data.item():.6f}\")\n",
    "        flag += 1\n",
    "        if flag>=3:\n",
    "            break\n",
    "\n",
    "print(f\"\\nCurrent outer-loop learning rate: {maml_model.scheduler.get_lr()[0]:.6f}\")\n",
    "print(f\"Total number of model parameters: {sum(p.numel() for p in maml_model.parameters() if p.requires_grad)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### 总结\n",
    "\n",
    "1. **数据准备**: 使用`TaskSampler`采样任务，通过`transform`和`task_collate_fn`处理数据格式\n",
    "2. **模型初始化**: 创建`MAMLRegressor`实例并加载预训练权重\n",
    "3. **训练过程**: 使用`run_train_iter`执行训练，使用`run_validation_iter`执行验证\n",
    "4. **结果分析**: 监控损失变化、亲和力系数、学习率等关键指标\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metascore",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
