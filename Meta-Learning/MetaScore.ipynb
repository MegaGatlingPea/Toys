{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> 工程实现-MetaScore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 数据加载\n",
    "\n",
    "首先，训练所需要的数据全部来自于pdbbind并且已被处理为图格式，可以直接加载:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import hydra\n",
    "import higher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './dataset/pdbbind/v2020_train_dict.pt'\n",
    "data = torch.load(data_path,weights_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看一下数据组织格式：所有数据是一个大的嵌套字典，它的键是`pdb_id`，值是一个字典，其键`prot`,`lig`,`label`分别对应了蛋白质图、配体图和亲和力数据:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prot': Data(x=[83, 41], edge_index=[2, 2014], edge_attr=[2014, 5], pos=[83, 24, 3]),\n",
       " 'lig': Data(x=[33, 41], edge_index=[2, 68], edge_attr=[68, 10], pos=[33, 3]),\n",
       " 'label': np.str_('6.4')}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['10gs']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "与传统机器学习不同，元学习训练的数据单元是任务而非单个数据点，因此需要对pdbbind中的蛋白-配体对数据进行任务划分，在MetaScore中表现为对蛋白质结构相似度进行层次聚类，以聚类得到的簇为单个任务进行训练。对应的，`DataLoader`的写法也相应的改变：\n",
    "\n",
    "- `DataLoader`一次从所有任务中采样`batch_size`个任务\n",
    "- 每个任务应当采样`num_classes_per_task`个类别，即选取不同的聚类用于构建一个任务\n",
    "- 每个聚类中随机采样`k-shot`个样本用于构建支持集，`q-query`个样本用于构建查询集\n",
    "- 将一个任务中所有聚类中被选中的支持集样本和查询集样本聚合后，即可视为`meta-batch`中的一个任务\n",
    "- 考虑到显存问题，无法再将`meta-batch`中的任务也做一遍聚合然后一次算完（显存会爆炸），只能在一个批次中逐任务累计梯度\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "聚类的结果已经拿到，加载一下:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PDB_ID</th>\n",
       "      <th>Cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2z7i</td>\n",
       "      <td>257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3i7g</td>\n",
       "      <td>333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2v88</td>\n",
       "      <td>197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3wav</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6qlu</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  PDB_ID  Cluster\n",
       "0   2z7i      257\n",
       "1   3i7g      333\n",
       "2   2v88      197\n",
       "3   3wav      150\n",
       "4   6qlu        0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "cluster_path = './dataset/cluster/clustering_results.csv'\n",
    "cluster_df = pd.read_csv(cluster_path)\n",
    "cluster_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "而后我们根据聚类的结果来构建一个新的数据字典，在这个字典中，键应当改为`Cluster`，值应为列表，每个列表的元素代表位于该聚类下的一个数据点，以原数据中字典的格式存储（在原数据字典的基础上额外增加了`pdb_id`作为数据标识符）:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/19145 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19145/19145 [00:27<00:00, 695.37it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'prot': Data(x=[38, 41], edge_index=[2, 794], edge_attr=[794, 5], pos=[38, 24, 3]),\n",
       "  'lig': Data(x=[35, 41], edge_index=[2, 72], edge_attr=[72, 10], pos=[35, 3]),\n",
       "  'label': '5.7',\n",
       "  'pdb_id': '4z0u'},\n",
       " {'prot': Data(x=[83, 41], edge_index=[2, 2264], edge_attr=[2264, 5], pos=[83, 24, 3]),\n",
       "  'lig': Data(x=[21, 41], edge_index=[2, 46], edge_attr=[46, 10], pos=[21, 3]),\n",
       "  'label': '6.0',\n",
       "  'pdb_id': '3dxm'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tqdm\n",
    "dict_with_cluster = {}\n",
    "for pdb_id, value in tqdm.tqdm(data.items()):\n",
    "    value['pdb_id'] = pdb_id\n",
    "    row = cluster_df[cluster_df['PDB_ID'] == pdb_id]\n",
    "    cluster_id = row['Cluster'].values[0]\n",
    "    if f'class_{cluster_id}' not in dict_with_cluster:\n",
    "        dict_with_cluster[f'class_{cluster_id}'] = []\n",
    "    dict_with_cluster[f'class_{cluster_id}'].append(value)\n",
    "dict_with_cluster['class_1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在此结果基础上，考虑划分元学习的`meta-train`和`meta-test`集，在`MetaScore`中，考虑到降低实现难度，该集的划分以每个聚类中数据点的数量划分，具体的，不少于17个数据点的聚类会被分类为训练集，不少于7个数据点的聚类分类为测试集，其他聚类暂时丢弃，或考虑用于模型`zeroshot`性能测试（这部分聚类数量很多但是数据量很小，可以丢弃而不造成严重影响）。为此可以写一个`dict_with_split`，存储每个聚类属于的集合:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 186 classes, Val: 153 classes, Test: 808 classes\n",
      "data length of each split {'train': 15810, 'val': 1574, 'test': 1761}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "train_set_min_size = 17\n",
    "val_set_min_size = 7\n",
    "dict_with_split = {'train': [], 'val': [], 'test': []}\n",
    "for class_idx, value in dict_with_cluster.items():\n",
    "    data_count = len(value)\n",
    "    if data_count >= train_set_min_size:\n",
    "        dict_with_split['train'].append(class_idx)\n",
    "    elif data_count >= val_set_min_size and data_count < train_set_min_size:\n",
    "        dict_with_split['val'].append(class_idx)\n",
    "    else:\n",
    "        dict_with_split['test'].append(class_idx)\n",
    "print(f\"Train: {len(dict_with_split['train'])} classes, Val: {len(dict_with_split['val'])} classes, Test: {len(dict_with_split['test'])} classes\")\n",
    "dataset_size_dict = {\n",
    "    \"train\": {key: len(dict_with_cluster[key]) for key in dict_with_split[\"train\"]},\n",
    "    \"val\": {key: len(dict_with_cluster[key]) for key in dict_with_split[\"val\"]},\n",
    "    \"test\": {key: len(dict_with_cluster[key]) for key in dict_with_split[\"test\"]},\n",
    "}\n",
    "data_length = {name: np.sum(np.array(list(dataset_size_dict[name].values()))) for name in dict_with_split.keys()}\n",
    "print(\"data length of each split\", data_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "到此为止完成了数据单元（单个聚类）的数据集划分，接下来要考虑如何使用数据单元来构建`meta-batch`，这涉及到:\n",
    "- 任务构建逻辑，如何在所有聚类中选择`batch_size`个聚类构建一个任务\n",
    "- 聚类中的样本采样逻辑，如何在一个聚类中选取样本构建支持集和查询集\n",
    "这两点可以通过一个`TaskSampler`类来完成:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskSampler:\n",
    "    \"\"\"\n",
    "    Samples tasks based on specified rules.\n",
    "    Can select classes uniformly or based on class size using softmax probability.\n",
    "    Samples support/query items uniformly from within selected classes.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 dataset_size_dict: dict,\n",
    "                 train_num_classes_per_set: int,\n",
    "                 val_num_classes_per_set: int,\n",
    "                 train_num_support: int,\n",
    "                 val_num_support: int,\n",
    "                 train_num_query: int,\n",
    "                 val_num_query: int,\n",
    "                 sampling_rule: str = \"uniform\"): # Added sampling_rule parameter\n",
    "        \"\"\"\n",
    "        Initializes the TaskSampler.\n",
    "\n",
    "        Args:\n",
    "            dataset_size_dict: Dictionary mapping split names ('train', 'val') to dictionaries\n",
    "                               of {class_name: class_size}.\n",
    "            train_num_classes_per_set: Number of classes per task in the training set.\n",
    "            val_num_classes_per_set: Number of classes per task in the validation set.\n",
    "            train_num_support: Number of support examples per class in the training set.\n",
    "            val_num_support: Number of support examples per class in the validation set.\n",
    "            train_num_query: Number of query examples per class in the training set.\n",
    "            val_num_query: Number of query examples per class in the validation set.\n",
    "            sampling_rule: Rule for sampling classes ('uniform' or 'softmax'). Defaults to 'uniform'.\n",
    "        \"\"\"\n",
    "        self.dataset_size_dict = dataset_size_dict\n",
    "        self.sampling_rule = sampling_rule # Store the rule, for now only uniform sampling is implemented\n",
    "        # determine the number of clusters to sample for each task\n",
    "        self.num_classes_per_set = {\n",
    "            \"train\": train_num_classes_per_set,\n",
    "            \"val\": val_num_classes_per_set,\n",
    "        }\n",
    "        # determine the number of support samples to sample for each class\n",
    "        self.num_support = {\n",
    "            \"train\": train_num_support,\n",
    "            \"val\": val_num_support,\n",
    "        }\n",
    "        # determine the number of query samples to sample for each class\n",
    "        self.num_query = {\n",
    "            \"train\": train_num_query,\n",
    "            \"val\": val_num_query,\n",
    "        }\n",
    "        # check if the sampling rule is valid\n",
    "        if sampling_rule not in [\"uniform\", \"softmax\"]:\n",
    "            raise ValueError(\"Invalid sampling_rule. Must be 'uniform' or 'softmax'.\")\n",
    "\n",
    "    def sample_task(self, dataset_name: str, seed: int) -> dict:\n",
    "        \"\"\"\n",
    "        Samples a task based on the configured sampling rule.\n",
    "\n",
    "        Args:\n",
    "            dataset_name: 'train' or 'val'.\n",
    "            seed: Random seed for reproducibility.\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with sampled class names and their support/query indices.\n",
    "            Example: {'class_A': {'support_indices': [...], 'query_indices': [...]}, ...}\n",
    "        \"\"\"\n",
    "        if dataset_name not in [\"train\", \"val\"]:\n",
    "            raise ValueError(\"Invalid dataset_name. Must be 'train' or 'val'.\")\n",
    "        rng = np.random.RandomState(seed)\n",
    "        num_classes = self.num_classes_per_set[dataset_name]\n",
    "        num_support = self.num_support[dataset_name]\n",
    "        num_query = self.num_query[dataset_name]\n",
    "        num_samples_per_class = num_support + num_query\n",
    "        available_classes_dict = self.dataset_size_dict[dataset_name]\n",
    "        available_class_names = list(available_classes_dict.keys())\n",
    "        if len(available_class_names) < num_classes:\n",
    "            raise ValueError(\n",
    "                f\"Not enough classes in {dataset_name} split ({len(available_class_names)}) \"\n",
    "                f\"to sample {num_classes} classes.\"\n",
    "            )\n",
    "        # Select classes based on the sampling rule\n",
    "        if self.sampling_rule == \"uniform\":\n",
    "            selected_classes = rng.choice(\n",
    "                available_class_names,\n",
    "                size=num_classes,\n",
    "                replace=False,\n",
    "            )\n",
    "        elif self.sampling_rule == \"softmax\":\n",
    "            raise NotImplementedError(\"Softmax sampling rule not implemented\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown sampling rule: {self.sampling_rule}\")\n",
    "        task_info = {}\n",
    "        for class_name in selected_classes:\n",
    "            class_size = available_classes_dict[class_name] # Use the dict directly\n",
    "            if class_size < num_samples_per_class:\n",
    "                 raise ValueError(\n",
    "                    f\"Class '{class_name}' in '{dataset_name}' has only {class_size} samples, \"\n",
    "                    f\"but {num_samples_per_class} are required for support+query.\"\n",
    "                )\n",
    "            # Sample indices from 0 to class_size - 1\n",
    "            selected_indices = rng.choice(\n",
    "                class_size,\n",
    "                size=num_samples_per_class,\n",
    "                replace=False # Should not sample the same item twice for one task\n",
    "            )\n",
    "            task_info[class_name] = {\n",
    "                'support_indices': selected_indices[:num_support],\n",
    "                'query_indices': selected_indices[num_support : num_support + num_query]\n",
    "            }\n",
    "        return task_info\n",
    "\n",
    "task_sampler = TaskSampler(\n",
    "    dataset_size_dict=dataset_size_dict,\n",
    "    train_num_classes_per_set=5,\n",
    "    val_num_classes_per_set=16,\n",
    "    train_num_support=5,\n",
    "    val_num_support=3,\n",
    "    train_num_query=5,\n",
    "    val_num_query=3,\n",
    "    sampling_rule=\"uniform\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这是一个非常简单的采样实现（也是`MAML`论文中的实现），基于这个`TaskSampler`尝试采样一批任务:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 5, 3, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_train_task_info = task_sampler.sample_task(dataset_name='train', seed=1227);sampled_val_task_info = task_sampler.sample_task(dataset_name='val', seed=1227)\n",
    "train_support_set_data = [];train_query_set_data = [];val_support_set_data = [];val_query_set_data = []\n",
    "for sampled_train_class_name in sampled_train_task_info.keys():\n",
    "    train_class_data_list = dict_with_cluster[sampled_train_class_name]\n",
    "    support_indices = sampled_train_task_info[sampled_train_class_name]['support_indices']\n",
    "    query_indices = sampled_train_task_info[sampled_train_class_name]['query_indices']\n",
    "    per_class_support_data = [train_class_data_list[i] for i in support_indices]\n",
    "    train_support_set_data.append(per_class_support_data)\n",
    "    per_class_query_data = [train_class_data_list[i] for i in query_indices]\n",
    "    train_query_set_data.append(per_class_query_data)\n",
    "for sampled_val_class_name in sampled_val_task_info.keys():\n",
    "    val_class_data_list = dict_with_cluster[sampled_val_class_name]\n",
    "    support_indices = sampled_val_task_info[sampled_val_class_name]['support_indices']\n",
    "    query_indices = sampled_val_task_info[sampled_val_class_name]['query_indices']\n",
    "    per_class_support_data = [val_class_data_list[i] for i in support_indices]\n",
    "    val_support_set_data.append(per_class_support_data)\n",
    "    per_class_query_data = [val_class_data_list[i] for i in query_indices]\n",
    "    val_query_set_data.append(per_class_query_data)\n",
    "\n",
    "# thus a task is sampled, data is organized as follows:\n",
    "# (5,5,16,16), refer to the number of classes sampled \n",
    "len(train_support_set_data),len(train_query_set_data),len(val_support_set_data),len(val_query_set_data)\n",
    "# (5,5,3,3), refer to the number of support and query samples sampled for each class\n",
    "len(train_support_set_data[0]),len(train_query_set_data[0]),len(val_support_set_data[0]),len(val_query_set_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目前采样得到的任务组织比较混乱，考虑用字典使任务变得更加清晰，可以定义一个`transform`函数实现这一点，同时，该函数可以用于定义`__getitem__`方法，即获得一个数据单元，在此我们通过复制任务来得到一个`meta-batch`方便后续演示:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(data):\n",
    "    \"\"\"Transforms the raw data list from get_set into separate lists.\"\"\"\n",
    "    pdb_ids = []\n",
    "    labels = []\n",
    "    prots = []\n",
    "    ligs = []\n",
    "    for class_data_list in data:\n",
    "        for per_pdb_data_dict in class_data_list:\n",
    "            pdb_ids.append(per_pdb_data_dict['pdb_id'])\n",
    "            labels.append(float(per_pdb_data_dict['label']))\n",
    "            prots.append(per_pdb_data_dict['prot'])\n",
    "            ligs.append(per_pdb_data_dict['lig'])\n",
    "    return {\n",
    "        'pdb_ids': pdb_ids,\n",
    "        'labels': labels,\n",
    "        'prots': prots,\n",
    "        'ligs': ligs,\n",
    "    }\n",
    "# (25,25,48,48), 25 = 5*5, 48 = 16*3\n",
    "# train_support_set_data['pdb_ids'].__len__(),train_query_set_data['pdb_ids'].__len__(),val_support_set_data['pdb_ids'].__len__(),val_query_set_data['pdb_ids'].__len__()\n",
    "train_support_set_data = transform(train_support_set_data);train_query_set_data = transform(train_query_set_data)\n",
    "val_support_set_data = transform(val_support_set_data);val_query_set_data = transform(val_query_set_data)\n",
    "train_task = (train_support_set_data,train_query_set_data,1227); val_task = (val_support_set_data,val_query_set_data,1227)\n",
    "# BATCH_SIZE = 4, simply repeat the task for 4 times to get a batch\n",
    "train_batch = [train_task for _ in range(4)];val_batch = [val_task for _ in range(4)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了让数据格式更加工整，考虑写一个自定义的`task_collate_fn`来实现这一点，最后整合好的`meta-batch`是一个长度为`batch_size`的列表，列表的每个元素是一个字典:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Batch\n",
    "def task_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Collate function for creating task batches with graph data.\n",
    "    Processes and collates support and query sets into batches.\n",
    "    Input `batch`: A list of tuples, where each tuple is (support_set_dict, query_set_dict, seed)\n",
    "                    returned by FewShotLearningDatasetParallel.__getitem__.\n",
    "    \"\"\"\n",
    "    all_tasks = []\n",
    "    \n",
    "    for task in batch:\n",
    "        # Unpack the task tuple\n",
    "        support_set_dict, query_set_dict, seed = task\n",
    "\n",
    "        # Process Support Set\n",
    "        support_ligs = support_set_dict['ligs'] # List of ligand graph objects\n",
    "        support_prots = support_set_dict['prots'] # List of protein graph objects\n",
    "        support_labels = support_set_dict['labels']\n",
    "        support_pdb_ids = support_set_dict['pdb_ids']\n",
    "        \n",
    "        query_ligs = query_set_dict['ligs'] # List of ligand graph objects\n",
    "        query_prots = query_set_dict['prots'] # List of protein graph objects\n",
    "        query_labels = query_set_dict['labels']\n",
    "        query_pdb_ids = query_set_dict['pdb_ids']\n",
    "        \n",
    "        task_data = {\n",
    "            \"support\": {\n",
    "                \"prots\": Batch.from_data_list(support_prots),\n",
    "                \"ligs\": Batch.from_data_list(support_ligs),\n",
    "                \"labels\": torch.tensor(support_labels, dtype=torch.float).reshape(-1),\n",
    "                \"pdb_ids\": support_pdb_ids,\n",
    "            },\n",
    "            \"query\": {\n",
    "                \"prots\": Batch.from_data_list(query_prots),\n",
    "                \"ligs\": Batch.from_data_list(query_ligs),\n",
    "                \"labels\": torch.tensor(query_labels, dtype=torch.float).reshape(-1),\n",
    "                \"pdb_ids\": query_pdb_ids,\n",
    "            },\n",
    "            \"seed\": seed,\n",
    "        }\n",
    "        all_tasks.append(task_data)\n",
    "    return all_tasks\n",
    "\n",
    "# successfully get a meta-batch for training and validation\n",
    "train_batch = task_collate_fn(train_batch);val_batch = task_collate_fn(val_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 基础模型的定义\n",
    "\n",
    "该部分定义了神经网络的具体架构，实现与传统机器学习并无不同，后续考虑收录一些基于`MAML`设计模型的技巧在里面，目前相关的工作有:\n",
    "\n",
    "- [MetaMolGen: A Neural Graph Motif Generation Model for De Novo Molecular Design](https://arxiv.org/abs/2504.15587)\n",
    "- [Meta-MGNN: Few-Shot Graph Learning for Molecular Property Prediction](https://arxiv.org/abs/2102.07916)\n",
    "  \n",
    "该部分工作持续收录中~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此处我们就用预先写好的`./models/gatedgcn.py`中定义好的权重以及参数`./models/gatedgcn.pt`进行模型初始化，并且简单测试一下基础模型是否能够工作:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load meta-model\n",
      "Predicted Gaussian Mixture Weights: torch.Size([64953, 10])\n",
      "Predicted Gaussian Mixture Standard Deviations: torch.Size([64953, 10])\n",
      "Predicted Gaussian Mixture Means: torch.Size([64953, 10])\n",
      "Precalculated Closest Distance: torch.Size([64953, 1])\n",
      "Predicted Atom Types: torch.Size([763, 17])\n",
      "Predicted Bond Types: torch.Size([1638, 4])\n",
      "Batch Index: torch.Size([64953])\n",
      "Total Loss: 0.9442639807858056\n",
      "MDN Loss: 0.9440778493881226\n",
      "Affinity Pearson Correlation: 0.6901014447212219\n",
      "Atom Loss: 0.0011155897518619895\n",
      "Bond Loss: 0.18501216173171997\n",
      "Predicted Scores: tensor([205.3693, 218.8177, 196.0030, 135.6230, 209.4204, 103.4285, 158.9223,\n",
      "         54.2460, 172.2362, 174.9315,  84.5279,  51.5547,  70.0292,  89.8906,\n",
      "        202.3072, 125.1309, 172.8234, 152.8251, 119.3668, 170.6546,  60.1053,\n",
      "        120.5561, 181.4496,  78.5369, 122.1589], dtype=torch.float64)\n",
      "Batch Index: tensor([ 0,  0,  0,  ..., 24, 24, 24])\n"
     ]
    }
   ],
   "source": [
    "from models.gatedgcn import GenScore_GGCN, mdn_loss_fn\n",
    "import torch\n",
    "ft = False\n",
    "meta = True\n",
    "state_dict = torch.load('./models/gatedgcn.pt',weights_only=False)\n",
    "meta_model_state_dict = state_dict['network'] # meta_model_state_dict, with a 'regressor.' prefix\n",
    "# Correctly strip the prefix, ensuring the new key is not empty\n",
    "model_state_dict = {\n",
    "    new_key: v \n",
    "    for k, v in meta_model_state_dict.items() \n",
    "    if k.startswith('regressor.') and (new_key := k[len('regressor.'):])\n",
    "}\n",
    "base_model = GenScore_GGCN()\n",
    "if ft is False:\n",
    "    if meta is False:\n",
    "        print('load non-finetuned model')\n",
    "        base_model.load_state_dict(torch.load('./models/gatedgcn_re.pth',weights_only=False)['model_state_dict'])\n",
    "    else:\n",
    "        print('load meta-model')\n",
    "        base_model.load_state_dict(model_state_dict)\n",
    "else:\n",
    "    print('load ft non-meta-model')\n",
    "    base_model.load_state_dict(torch.load('./models/gatedgcn_nonmeta.pth',weights_only=False)['model_state_dict'])\n",
    "support_train_prot_eg = train_batch[0]['support']['prots']\n",
    "support_train_lig_eg = train_batch[0]['support']['ligs']\n",
    "pi, sigma, mu, dist, atom_types, bond_types, C_batch =base_model.net_forward(support_train_lig_eg,support_train_prot_eg)\n",
    "print(f\"Predicted Gaussian Mixture Weights: {pi.shape}\\nPredicted Gaussian Mixture Standard Deviations: {sigma.shape}\\nPredicted Gaussian Mixture Means: {mu.shape}\\nPrecalculated Closest Distance: {dist.shape}\\nPredicted Atom Types: {atom_types.shape}\\nPredicted Bond Types: {bond_types.shape}\\nBatch Index: {C_batch.shape}\")\n",
    "total_loss, mdn_loss, affi_loss, atom_loss, bond_loss, y, batch = \\\n",
    "    base_model.forward(train_batch[0]['support'])\n",
    "print(f\"Total Loss: {total_loss}\\nMDN Loss: {mdn_loss}\\nAffinity Pearson Correlation: {affi_loss}\\nAtom Loss: {atom_loss}\\nBond Loss: {bond_loss}\\nPredicted Scores: {y}\\nBatch Index: {batch}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent! 接下来就要上元学习的核心逻辑了，MetaScore采用的元学习方法是基于优化的`MAML`及其变体`MAML++`（后续考虑继续做变体，比如`Task Based Attention Mechanism`等），其介绍可以详细看同目录下的`MetaLearning.md`，下面主要强调其工程上的实现（主要基于`higher`库，比较方便，所有已经写好的并非用于元学习的模型通过`higher`的帮助均可以轻松改造为元模型）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 元学习框架设计"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了将元学习框架套用到`GenScore_GGCN`模型上，我们需要额外定义一个超类`MAMLRegressor`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_torch_seed(seed):\n",
    "    \"\"\"\n",
    "    Sets the pytorch seeds for current experiment run\n",
    "    :param seed: The seed (int)\n",
    "    :return: A random number generator to use\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(seed=seed)\n",
    "    torch_seed = rng.randint(0, 999999)\n",
    "    torch.manual_seed(seed=torch_seed)\n",
    "\n",
    "    return rng\n",
    "\n",
    "class MAMLRegressor(nn.Module):\n",
    "    def __init__(self, args, logs_filepath=None):\n",
    "        \"\"\"\n",
    "        Initializes a MAML few shot learning system\n",
    "        :param device: The device to use to use the model on.\n",
    "        :param args: A namedtuple of arguments specifying various hyperparameters.\n",
    "        :param logs_filepath: Path to the logs directory for storing csv files\n",
    "        \"\"\"\n",
    "        super(MAMLRegressor, self).__init__()\n",
    "\n",
    "        self.device = torch.device('cpu')\n",
    "        self.batch_size = 4\n",
    "        self.current_epoch = 0\n",
    "        self.logs_filepath = './tmp'\n",
    "\n",
    "        self.rng = set_torch_seed(seed=1227)\n",
    "        self.regressor = GenScore_GGCN() #.to(device=self.device)\n",
    "        \n",
    "        inner_opt_class_name = 'torch.optim.SGD'\n",
    "        inner_opt_class = hydra.utils.get_class(inner_opt_class_name)\n",
    "        kwargs = {'lr':0.01}\n",
    "\n",
    "        if args.meta_learning.options.learnable_inner_opt_params:\n",
    "            param_groups = [\n",
    "                {'params': p, 'lr': kwargs['lr']} for p in self.regressor.parameters()\n",
    "            ]\n",
    "            self.inner_opt = inner_opt_class(param_groups, **kwargs)\n",
    "            t = higher.optim.get_trainable_opt_params(self.inner_opt)\n",
    "            self.lrs = nn.ParameterList(map(\n",
    "                nn.Parameter,\n",
    "                t['lr']\n",
    "            ))\n",
    "            if 'Adam' in inner_opt_class_name:\n",
    "                self.betas = nn.ParameterList(list(map(\n",
    "                    nn.Parameter,\n",
    "                    [item for sublist in t['betas'] for item in sublist]\n",
    "                )))\n",
    "        else:\n",
    "            params = self.regressor.parameters()\n",
    "            self.inner_opt = inner_opt_class(params, **kwargs)\n",
    "\n",
    "        self.to(device=self.device)\n",
    "        print(\"Outer Loop parameters\")\n",
    "        param_shapes = []\n",
    "        for name, param in self.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                print(name, param.shape, param.device, param.requires_grad)\n",
    "                param_shapes.append(param.shape)\n",
    "        print(f'n_params: {sum(map(np.prod, param_shapes))}')\n",
    "\n",
    "        self.optimizer = optim.Adam(\n",
    "            self.trainable_parameters(), lr=args.meta_learning.learning_rate, amsgrad=False)\n",
    "        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer=self.optimizer, T_max=args.training.epochs.total,\n",
    "            eta_min=args.meta_learning.min_learning_rate\n",
    "        )\n",
    "        self.args = args\n",
    "\n",
    "    def get_per_step_loss_importance_vector(self):\n",
    "        \"\"\"\n",
    "        Generates a tensor of dimensionality (num_inner_loop_steps) indicating the importance of each step's target\n",
    "        loss towards the optimization loss.\n",
    "        :return: A tensor to be used to compute the weighted average of the loss, useful for\n",
    "        the MSL (Multi Step Loss) mechanism.\n",
    "        \"\"\"\n",
    "        loss_weights = np.ones(shape=(self.args.meta_learning.steps.training_per_iter)) * (\n",
    "                1.0 / self.args.meta_learning.steps.training_per_iter)\n",
    "        decay_rate = 1.0 / self.args.meta_learning.steps.training_per_iter / self.args.training.epochs.multi_step_loss_num\n",
    "        min_value_for_non_final_losses = 0.03 / self.args.meta_learning.steps.training_per_iter\n",
    "        for i in range(len(loss_weights) - 1):\n",
    "            curr_value = np.maximum(loss_weights[i] - (self.current_epoch * decay_rate), min_value_for_non_final_losses)\n",
    "            loss_weights[i] = curr_value\n",
    "\n",
    "        curr_value = np.minimum(\n",
    "            loss_weights[-1] + (self.current_epoch * (self.args.meta_learning.steps.training_per_iter - 1) * decay_rate),\n",
    "            1.0 - ((self.args.meta_learning.steps.training_per_iter - 1) * min_value_for_non_final_losses))\n",
    "        loss_weights[-1] = curr_value\n",
    "        loss_weights = torch.Tensor(loss_weights).to(device=self.device)\n",
    "        return loss_weights\n",
    "\n",
    "    def get_inner_loop_parameter_dict(self, params):\n",
    "        \"\"\"\n",
    "        Returns a dictionary with the parameters to use for inner loop updates.\n",
    "        :param params: A dictionary of the network's parameters.\n",
    "        :return: A dictionary of the parameters to use for the inner loop optimization process.\n",
    "        \"\"\"\n",
    "        param_dict = dict()\n",
    "        for name, param in params:\n",
    "            #print(name, param.shape, param.device, param.requires_grad)\n",
    "            if param.requires_grad:\n",
    "                if self.args.meta_learning.options.enable_inner_loop_optimizable_bn_params:\n",
    "                    param_dict[name] = param.to(device=self.device)\n",
    "                else:\n",
    "                    if \"norm_layer\" not in name:\n",
    "                        param_dict[name] = param.to(device=self.device)\n",
    "\n",
    "        return param_dict\n",
    "\n",
    "    def forward(\n",
    "            self, \n",
    "            data_batch, \n",
    "            epoch, \n",
    "            use_second_order,\n",
    "            use_multi_step_loss_optimization, \n",
    "            num_steps, \n",
    "            training_phase\n",
    "        ):\n",
    "        if training_phase:\n",
    "            dist_threhold = None \n",
    "        else:\n",
    "            dist_threhold = self.args.model.architecture.val_dist_threhold\n",
    "        self.regressor.zero_grad()\n",
    "        total_losses = []\n",
    "        mdn_losses = []\n",
    "        atom_losses = []\n",
    "        bond_losses = []\n",
    "        affi_coeffs = []\n",
    "        per_task_query_preds = []\n",
    "        per_task_query_labels = []\n",
    "        for task_id, task in enumerate(data_batch):\n",
    "            task_losses = []\n",
    "            task_mdn_losses = []\n",
    "            task_affi_coeffs = []\n",
    "            task_atom_losses = []\n",
    "            task_bond_losses = []\n",
    "            # We only need the predictions after the final inner loop step\n",
    "            final_query_preds_for_task = None \n",
    "\n",
    "            support_task = task[\"support\"]\n",
    "            query_task = task[\"query\"]\n",
    "            per_task_query_labels.append(query_task[\"labels\"])\n",
    "            per_step_loss_importance_vectors = self.get_per_step_loss_importance_vector()\n",
    "            with higher.innerloop_ctx(\n",
    "                self.regressor, self.inner_opt, copy_initial_weights=False,\n",
    "                track_higher_grads=training_phase,\n",
    "            ) as (fnet, diffopt):\n",
    "                # wtf code it is?\n",
    "                #for p in self.regressor.parameters():\n",
    "                #    self.inner_opt.state[p] = copy.deepcopy(self.optimizer.state[p])\n",
    "                for num_step in range(num_steps):\n",
    "                    (support_loss, \n",
    "                     support_mdn_loss, \n",
    "                     support_affi_coeff, \n",
    "                     support_atom_loss, \n",
    "                     support_bond_loss, \n",
    "                     support_preds, \n",
    "                     support_batch) = fnet(task=support_task,dist_threhold=dist_threhold)\n",
    "\n",
    "                    if self.args.meta_learning.options.learnable_inner_opt_params:\n",
    "                        if 'Adam' in self.args.optimizers.inner['class']:\n",
    "                            betas = [self.betas[i:i+2] for i in range(0, len(self.betas), 2)]\n",
    "                            diffopt.step(\n",
    "                                support_loss, override={'lr': self.lrs, 'betas': betas}\n",
    "                            )\n",
    "                        else:\n",
    "                            diffopt.step(\n",
    "                                support_loss, override={'lr': self.lrs},\n",
    "                            )\n",
    "                    else:\n",
    "                        diffopt.step(support_loss)\n",
    "\n",
    "                    if use_multi_step_loss_optimization and training_phase and \\\n",
    "                            epoch < self.args.training.epochs.multi_step_loss_num:\n",
    "                        (query_loss, \n",
    "                         query_mdn_loss, \n",
    "                         query_affi_coeff, \n",
    "                         query_atom_loss, \n",
    "                         query_bond_loss, \n",
    "                         query_preds, \n",
    "                         query_batch) = fnet(query_task,dist_threhold=dist_threhold)\n",
    "                        \n",
    "                        task_losses.append(\n",
    "                            per_step_loss_importance_vectors[num_step] * query_loss)\n",
    "                        task_mdn_losses.append(\n",
    "                            per_step_loss_importance_vectors[num_step] * query_mdn_loss)\n",
    "                        task_affi_coeffs.append(\n",
    "                            per_step_loss_importance_vectors[num_step] * query_affi_coeff)\n",
    "                        task_atom_losses.append(\n",
    "                            per_step_loss_importance_vectors[num_step] * query_atom_loss)\n",
    "                        task_bond_losses.append(\n",
    "                            per_step_loss_importance_vectors[num_step] * query_bond_loss)\n",
    "                        # Store prediction only from the last step\n",
    "                        if num_step == (num_steps - 1):\n",
    "                            final_query_preds_for_task = query_preds\n",
    "\n",
    "                    else:\n",
    "                        if num_step == (self.args.meta_learning.steps.training_per_iter - 1):\n",
    "                            (query_loss, \n",
    "                             query_mdn_loss, \n",
    "                             query_affi_coeff, \n",
    "                             query_atom_loss, \n",
    "                             query_bond_loss, \n",
    "                             query_preds, \n",
    "                             query_batch) = fnet(query_task,dist_threhold=dist_threhold)\n",
    "\n",
    "                            task_losses.append(query_loss)\n",
    "                            task_mdn_losses.append(query_mdn_loss)\n",
    "                            task_affi_coeffs.append(query_affi_coeff)\n",
    "                            task_atom_losses.append(query_atom_loss)\n",
    "                            task_bond_losses.append(query_bond_loss)\n",
    "                            # Store prediction from the last step\n",
    "                            final_query_preds_for_task = query_preds\n",
    "\n",
    "            total_losses.append(torch.sum(torch.stack(task_losses)))\n",
    "            mdn_losses.append(torch.sum(torch.stack(task_mdn_losses)))\n",
    "            atom_losses.append(torch.sum(torch.stack(task_atom_losses)))\n",
    "            bond_losses.append(torch.sum(torch.stack(task_bond_losses)))\n",
    "            affi_coeffs.append(torch.sum(torch.stack(task_affi_coeffs)))\n",
    "            # Append the final prediction for this task\n",
    "            per_task_query_preds.append(final_query_preds_for_task) \n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # Concatenate final predictions from all tasks\n",
    "        all_query_preds = torch.cat(per_task_query_preds, dim=0) \n",
    "        all_query_labels = torch.cat(per_task_query_labels, dim=0)\n",
    "        all_query_labels = all_query_labels.to(device=self.device)\n",
    "\n",
    "        # Now shapes should match: [batch_size * num_query]\n",
    "        total_affi_coeff = torch.corrcoef(torch.stack([all_query_preds, all_query_labels]))[1, 0]\n",
    "\n",
    "        losses = {\n",
    "            \"total_loss\": torch.mean(torch.stack(total_losses)),\n",
    "            \"mdn_loss\": torch.mean(torch.stack(mdn_losses)),\n",
    "            \"affi_coeffs\": torch.mean(torch.stack(affi_coeffs)),\n",
    "            \"atom_loss\": torch.mean(torch.stack(atom_losses)),\n",
    "            \"bond_loss\": torch.mean(torch.stack(bond_losses)),\n",
    "            \"total_affi_coeff\": total_affi_coeff,\n",
    "        }\n",
    "        torch.cuda.empty_cache()\n",
    "        return losses, all_query_preds.cpu().numpy()\n",
    "\n",
    "    def adaptation(self, support_task, num_steps=None, dist_threhold=None):\n",
    "        \"\"\"\n",
    "        Based on a small number of samples, adapt the model parameters, and return the adapted model for virtual screening tasks.\n",
    "        \n",
    "        Args:\n",
    "            support_task: The support set data, used to adapt the model\n",
    "            num_steps: The number of inner loop update steps, default using evaluation_per_iter\n",
    "            dist_threhold: The distance threshold, used in the model for distance calculation\n",
    "            \n",
    "        Returns:\n",
    "            adapted_model: The adapted model instance, ready for virtual screening tasks\n",
    "        \"\"\"\n",
    "        if num_steps is None:\n",
    "            num_steps = self.args.meta_learning.steps.evaluation_per_iter\n",
    "        \n",
    "        if dist_threhold is None:\n",
    "            dist_threhold = self.args.model.architecture.val_dist_threhold\n",
    "            \n",
    "        # Create a model copy, to avoid modifying the original model\n",
    "        adapted_model = copy.deepcopy(self.regressor)\n",
    "        # support_task = copy.deepcopy(support_task) # Comment out this deepcopy\n",
    "        adapted_model.train()  # Set to training mode for adaptation\n",
    "        \n",
    "        # Create an optimizer\n",
    "        inner_opt_class_name = self.args.optimizers.inner['class']\n",
    "        inner_opt_class = hydra.utils.get_class(inner_opt_class_name)\n",
    "        kwargs = dict(self.args.optimizers.inner.params)\n",
    "        if 'Adam' in inner_opt_class_name:\n",
    "            kwargs['betas'] = (kwargs['beta1'], kwargs['beta2'])\n",
    "            del kwargs['beta1'], kwargs['beta2']\n",
    "        \n",
    "        # Use the learned inner loop optimizer parameters\n",
    "        if self.args.meta_learning.options.learnable_inner_opt_params:\n",
    "            param_groups = [\n",
    "                {'params': p, 'lr': kwargs['lr']} for p in adapted_model.parameters()\n",
    "            ]\n",
    "            inner_opt = inner_opt_class(param_groups, **kwargs)\n",
    "            \n",
    "            # Use higher for updates, get the adapted model\n",
    "            with higher.innerloop_ctx(\n",
    "                adapted_model, inner_opt, copy_initial_weights=True,\n",
    "                track_higher_grads=False  # No need to track gradients, because it's only for inference\n",
    "            ) as (fnet, diffopt):\n",
    "                for step in range(num_steps):\n",
    "                    support_task_copy = {\n",
    "                        \"pdb_ids\": copy.deepcopy(support_task[\"pdb_ids\"]),\n",
    "                        \"ligs\": copy.deepcopy(support_task[\"ligs\"]),\n",
    "                        \"prots\": copy.deepcopy(support_task[\"prots\"]),\n",
    "                        \"labels\": copy.deepcopy(support_task[\"labels\"])\n",
    "                    }\n",
    "                    (support_loss, \n",
    "                    support_mdn_loss, \n",
    "                    support_affi_coeff, \n",
    "                    support_atom_loss, \n",
    "                    support_bond_loss, \n",
    "                    support_preds, \n",
    "                    support_batch) = fnet(task=support_task_copy, dist_threhold=dist_threhold)\n",
    "                    \n",
    "                    # Use the learned learning rate and beta parameters\n",
    "                    if 'Adam' in inner_opt_class_name:\n",
    "                        betas = [self.betas[i:i+2] for i in range(0, len(self.betas), 2)]\n",
    "                        torch.nn.utils.clip_grad_norm_(adapted_model.parameters(), max_norm=10)\n",
    "                        diffopt.step(\n",
    "                            support_loss, override={'lr': self.lrs, 'betas': betas}\n",
    "                        )\n",
    "                    else:\n",
    "                        torch.nn.utils.clip_grad_norm_(adapted_model.parameters(), max_norm=10)\n",
    "                        diffopt.step(\n",
    "                            support_loss, override={'lr': self.lrs}\n",
    "                        )\n",
    "                \n",
    "                # Get the adapted model\n",
    "                adapted_model = fnet\n",
    "        else:\n",
    "            # If not using learnable inner loop parameters, use regular optimization method\n",
    "            inner_opt = inner_opt_class(adapted_model.parameters(), **kwargs)\n",
    "            \n",
    "            for step in range(num_steps):\n",
    "                adapted_model.zero_grad()\n",
    "                support_task_copy = {\n",
    "                    \"pdb_ids\": copy.deepcopy(support_task[\"pdb_ids\"]),\n",
    "                    \"ligs\": copy.deepcopy(support_task[\"ligs\"]),\n",
    "                    \"prots\": copy.deepcopy(support_task[\"prots\"]),\n",
    "                    \"labels\": copy.deepcopy(support_task[\"labels\"])\n",
    "                }\n",
    "                (support_loss, _, _, _, _, _, _) = adapted_model(task=support_task_copy, dist_threhold=dist_threhold)\n",
    "                support_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(adapted_model.parameters(), max_norm=10)\n",
    "                inner_opt.step()\n",
    "        \n",
    "        # Set to evaluation mode\n",
    "        adapted_model.eval()\n",
    "        return adapted_model\n",
    "\n",
    "    def trainable_parameters(self):\n",
    "        \"\"\"\n",
    "        Returns an iterator over the trainable parameters of the model.\n",
    "        \"\"\"\n",
    "        for param in self.parameters():\n",
    "            if param.requires_grad:\n",
    "                yield param\n",
    "\n",
    "    def train_forward_prop(self, data_batch, epoch):\n",
    "        \"\"\"\n",
    "        Runs an outer loop forward prop using the meta-model and base-model.\n",
    "        :param data_batch: A data batch containing the support set and target set input, output pairs.\n",
    "        :param epoch: The training epoch's index\n",
    "        :return: A dictionary of losses for the current step.\n",
    "        \"\"\"\n",
    "        losses, all_query_preds = self.forward(\n",
    "            data_batch=data_batch, epoch=epoch,\n",
    "            use_second_order=self.args.meta_learning.options.second_order and\n",
    "            epoch > self.args.meta_learning.options.first_order_to_second_order_epoch,\n",
    "            use_multi_step_loss_optimization=self.args.meta_learning.options.use_multi_step_loss_optimization,\n",
    "            num_steps=self.args.meta_learning.steps.training_per_iter,\n",
    "            training_phase=True)\n",
    "        return losses, all_query_preds\n",
    "\n",
    "    def evaluation_forward_prop(self, data_batch, epoch):\n",
    "        \"\"\"\n",
    "        Runs an outer loop evaluation forward prop using the meta-model and base-model.\n",
    "        :param data_batch: A data batch containing the support set and target set input, output pairs.\n",
    "        :param epoch: The training epoch's index\n",
    "        :return: A dictionary of losses for the current step.\n",
    "        \"\"\"\n",
    "        losses, all_query_preds = self.forward(\n",
    "            data_batch=data_batch, epoch=epoch, use_second_order=False,\n",
    "            use_multi_step_loss_optimization=True,\n",
    "            num_steps=self.args.meta_learning.steps.evaluation_per_iter,\n",
    "            training_phase=False)\n",
    "\n",
    "        return losses, all_query_preds\n",
    "\n",
    "    def meta_update(self, loss):\n",
    "        \"\"\"\n",
    "        Applies an outer loop update on the meta-parameters of the model.\n",
    "        :param loss: The current crossentropy loss.\n",
    "        \"\"\"\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        if 'pdbbind' in self.args.dataset.name:\n",
    "            for name, param in self.regressor.named_parameters():\n",
    "                if param.requires_grad and param.grad is not None:\n",
    "                    param.grad.data.clamp_(-10, 10)  # not sure if this is necessary, more experiments are needed\n",
    "        self.optimizer.step()\n",
    "\n",
    "        if self.args.meta_learning.options.learnable_inner_opt_params:\n",
    "            for lr in self.lrs:\n",
    "                lr.data[lr < 1e-4] = 1e-4\n",
    "            if 'Adam' in self.args.optimizers.inner['class']:\n",
    "                for beta in self.betas:\n",
    "                    beta.data[beta < 1e-4] = 1e-4\n",
    "                    beta.data[beta > 0.99] = 0.99\n",
    "\n",
    "\n",
    "    def run_train_iter(self, data_batch, epoch):\n",
    "        \"\"\"\n",
    "        Runs an outer loop update step on the meta-model's parameters.\n",
    "        :param data_batch: input data batch containing the support set and target set input, output pairs\n",
    "        :param epoch: the index of the current epoch\n",
    "        :return: The losses of the ran iteration.\n",
    "        \"\"\"\n",
    "        epoch = int(epoch)\n",
    "        self.scheduler.step(epoch=epoch)\n",
    "        if self.current_epoch != epoch:\n",
    "            self.current_epoch = epoch\n",
    "\n",
    "        self.train()\n",
    "\n",
    "        losses, all_query_preds = self.train_forward_prop(\n",
    "            data_batch=data_batch, epoch=epoch)\n",
    "\n",
    "        self.meta_update(loss=losses['total_loss'])\n",
    "        losses['learning_rate'] = self.scheduler.get_lr()[0]\n",
    "        self.optimizer.zero_grad()\n",
    "        self.zero_grad()\n",
    "        losses['total_loss'] = losses['total_loss'].detach().cpu()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        return losses, all_query_preds\n",
    "\n",
    "    def write_inner_opt_stats(self):\n",
    "        if self.args.meta_learning.options.learnable_inner_opt_params:\n",
    "            lrs = torch.stack(list(self.lrs)).cpu().detach().numpy()\n",
    "            \n",
    "            if self.logs_filepath:\n",
    "                lrs_path = os.path.join(self.logs_filepath, 'lrs.csv')\n",
    "            else:\n",
    "                lrs_path = 'lrs.csv'\n",
    "            \n",
    "            f = open(lrs_path, 'a')\n",
    "            f.write(','.join(map(str, lrs)) + '\\n')\n",
    "            f.close()\n",
    "            \n",
    "            if 'Adam' in self.args.optimizers.inner['class']:\n",
    "                betas = torch.stack(list(self.betas)).cpu().detach().numpy()\n",
    "                \n",
    "                if self.logs_filepath:\n",
    "                    betas_path = os.path.join(self.logs_filepath, 'betas.csv')\n",
    "                else:\n",
    "                    betas_path = 'betas.csv'\n",
    "                \n",
    "                f = open(betas_path, 'a')\n",
    "                f.write(','.join(map(str, betas)) + '\\n')\n",
    "                f.close()\n",
    "\n",
    "    def run_validation_iter(self, data_batch):\n",
    "        \"\"\"\n",
    "        Runs an outer loop evaluation step on the meta-model's parameters.\n",
    "        :param data_batch: input data batch containing the support set and target set input, output pairs\n",
    "        :param epoch: the index of the current epoch\n",
    "        :return: The losses of the ran iteration.\n",
    "        \"\"\"\n",
    "\n",
    "        # if self.training:\n",
    "        #     self.eval()\n",
    "        self.train()\n",
    "\n",
    "        losses, all_query_preds = self.evaluation_forward_prop(\n",
    "            data_batch=data_batch, epoch=self.current_epoch)\n",
    "\n",
    "        # losses['loss'].backward() # uncomment if you get the weird memory error\n",
    "        # self.zero_grad()\n",
    "        # self.optimizer.zero_grad()\n",
    "        losses['total_loss'] = losses['total_loss'].detach().cpu()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        return losses, all_query_preds\n",
    "\n",
    "    def save_model(self, model_save_dir, state):\n",
    "        \"\"\"\n",
    "        Save the network parameter state and experiment state dictionary.\n",
    "        :param model_save_dir: The directory to store the state at.\n",
    "        :param state: The state containing the experiment state and the network. It's in the form of a dictionary\n",
    "        object.\n",
    "        \"\"\"\n",
    "        state['network'] = self.state_dict()\n",
    "        torch.save(state, f=model_save_dir)\n",
    "\n",
    "    def load_model(self, model_save_dir, model_name, model_idx):\n",
    "        \"\"\"\n",
    "        Load checkpoint and return the state dictionary containing the network state params and experiment state.\n",
    "        :param model_save_dir: The directory from which to load the files.\n",
    "        :param model_name: The model_name to be loaded from the direcotry.\n",
    "        :param model_idx: The index of the model (i.e. epoch number or 'latest' for the latest saved model of the current\n",
    "        experiment)\n",
    "        :return: A dictionary containing the experiment state and the saved model parameters.\n",
    "        \"\"\"\n",
    "        filepath = os.path.join(model_save_dir, \"{}_{}\".format(model_name, model_idx))\n",
    "        state = torch.load(filepath, map_location=self.device)\n",
    "        state_dict_loaded = state['network']\n",
    "        self.load_state_dict(state_dict=state_dict_loaded)\n",
    "        return state"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
