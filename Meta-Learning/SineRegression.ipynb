{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> SineRegression Task </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable as V\n",
    "import pandas as pd\n",
    "import random\n",
    "import seaborn as sbs\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "sbs.set_style('darkgrid')\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "**SineRegression** 是元学习领域中的基准任务之一。给定正弦函数形式\n",
    "\n",
    "$$y = A \\sin(x + \\phi)$$\n",
    "\n",
    "按照以下步骤完成元学习算法的评估:\n",
    "\n",
    "1. 构建元任务:学习适应不同的正弦函数对上式赋予多组不同的$(A,\\phi)$值,一组不同的值代表元学习中的一个不同任务.例如:\n",
    "\n",
    "- 任务1：y = 2sin(x + 0.5)     \n",
    "- 任务2：y = 1.5sin(x + 1.2)   \n",
    "- 任务3：y = 3sin(x - 0.8)\n",
    "- ...\n",
    "\n",
    "2. 选取数据点:在每个任务中,随机采一系列数量的点用作支持集(support set)和查询集(query set).\n",
    "   \n",
    "3. 元测试评估:在任务域上训练完以后选取未出现过的$(A,\\phi)$值用作元测试任务,仍旧在任务中随机采一系列点作为支持集和查询集,评估元模型拟合正弦函数的能力.例如在新任务：y = 2.5sin(x + 2.1)上给定五个点用于拟合函数.\n",
    "\n",
    "据此就可以写出我们采样的Task了:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SineWaveTask:\n",
    "    def __init__(self):\n",
    "        self.a = np.random.uniform(0.1, 5.0)\n",
    "        self.b = np.random.uniform(0, 2*np.pi)\n",
    "        self.train_x = None\n",
    "        \n",
    "    def f(self, x):\n",
    "        return self.a * np.sin(x + self.b)\n",
    "        \n",
    "    def training_set(self, size=10, force_new=False):\n",
    "        if self.train_x is None and not force_new:\n",
    "            self.train_x = np.random.uniform(-5, 5, size)\n",
    "            x = self.train_x\n",
    "        elif not force_new:\n",
    "            x = self.train_x\n",
    "        else:\n",
    "            x = np.random.uniform(-5, 5, size)\n",
    "        y = self.f(x)\n",
    "        return torch.Tensor(x), torch.Tensor(y)\n",
    "    \n",
    "    def test_set(self, size=50):\n",
    "        x = np.linspace(-5, 5, size)\n",
    "        y = self.f(x)\n",
    "        return torch.Tensor(x), torch.Tensor(y)\n",
    "    \n",
    "    def plot(self, *args, **kwargs):\n",
    "        x, y = self.test_set(size=100)\n",
    "        return plt.plot(x.numpy(), y.numpy(), *args, **kwargs)\n",
    "    \n",
    "SineWaveTask().plot()\n",
    "SineWaveTask().plot()\n",
    "SineWaveTask().plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来要实现我们的模型,实现过程中有几点需要注意:\n",
    "\n",
    "1. 元学习实操时涉及到模型参数的手动指定(比如适应后的参数)\n",
    "2. 1中提到的部分in-place操作可能导致backward出错,需要手动指定leave tensor\n",
    "3. 在元更新过程中可能涉及到高阶导数,不能使用默认值(PyTorch默认一阶)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One can define a ModifiableModule to be a nn.Module that can be modified in-place\n",
    "# This is useful for meta-learning, where we want to update the model in-place\n",
    "# without having to re-instantiate the model\n",
    "class ModifiableModule(nn.Module):\n",
    "    def params(self):\n",
    "        return [p for _, p in self.named_params()]\n",
    "    \n",
    "    # PyTorch will track leave tensors, so we need to override this method\n",
    "    # to return the leaves of the model\n",
    "    def named_leaves(self):\n",
    "        return []\n",
    "    \n",
    "    def named_submodules(self):\n",
    "        return []\n",
    "    \n",
    "    def named_params(self):\n",
    "        subparams = []\n",
    "        for name, mod in self.named_submodules():\n",
    "            for subname, param in mod.named_params():\n",
    "                subparams.append((name + '.' + subname, param))\n",
    "        return self.named_leaves() + subparams\n",
    "    \n",
    "    # set_param is used to set the parameter of the model\n",
    "    # it is used to update the model in-place\n",
    "    # without having to re-instantiate the model\n",
    "    def set_param(self, name, param):\n",
    "        if '.' in name:\n",
    "            n = name.split('.')\n",
    "            module_name = n[0]\n",
    "            rest = '.'.join(n[1:])\n",
    "            for name, mod in self.named_submodules():\n",
    "                if module_name == name:\n",
    "                    mod.set_param(rest, param)\n",
    "                    break\n",
    "        else:\n",
    "            setattr(self, name, param)\n",
    "    \n",
    "    # to cut the computational graph\n",
    "    def copy(self, other, same_var=False):\n",
    "        for name, param in other.named_params():\n",
    "            if not same_var:\n",
    "                param = V(param.data.clone(), requires_grad=True)\n",
    "            self.set_param(name, param)\n",
    "\n",
    "# GradLinear is a linear layer that can be modified in-place\n",
    "# and also support higher order gradient\n",
    "class GradLinear(ModifiableModule):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        ignore = nn.Linear(*args, **kwargs)\n",
    "        self.weights = V(ignore.weight.data, requires_grad=True)\n",
    "        self.bias = V(ignore.bias.data, requires_grad=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return F.linear(x, self.weights, self.bias)\n",
    "    \n",
    "    def named_leaves(self):\n",
    "        return [('weights', self.weights), ('bias', self.bias)]\n",
    "\n",
    "class SineModel(ModifiableModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden1 = GradLinear(1, 40)\n",
    "        self.hidden2 = GradLinear(40, 40)\n",
    "        self.out = GradLinear(40, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden1(x))\n",
    "        x = F.relu(self.hidden2(x))\n",
    "        return self.out(x)\n",
    "    \n",
    "    def named_submodules(self):\n",
    "        return [('hidden1', self.hidden1), ('hidden2', self.hidden2), ('out', self.out)]\n",
    "\n",
    "# A one sided example is used to test the model\n",
    "# all positive or all negative\n",
    "ONE_SIDED_EXAMPLE = None\n",
    "while ONE_SIDED_EXAMPLE is None:\n",
    "    cur = SineWaveTask()\n",
    "    x, _ = cur.training_set()\n",
    "    x = x.numpy()\n",
    "    if np.max(x) < 0 or np.min(x) > 0:\n",
    "        ONE_SIDED_EXAMPLE = cur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上代码主要实现了支持inplace操作和追踪高阶梯度的神经网络,接下来我们插入几个用于评估和可视化的函数,后续有用:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just normal training process, should be useful\n",
    "# when doing transfer learning\n",
    "def sine_fit1(net, wave, optim=None, get_test_loss=False, create_graph=False, force_new=False):\n",
    "    net.train()\n",
    "    if optim is not None:\n",
    "        optim.zero_grad()\n",
    "    x, y = wave.training_set(force_new=force_new)\n",
    "    loss = F.mse_loss(net(V(x[:, None])), V(y).unsqueeze(1))\n",
    "    loss.backward(create_graph=create_graph, retain_graph=True)\n",
    "    if optim is not None:\n",
    "        optim.step()\n",
    "    if get_test_loss:\n",
    "        net.eval()\n",
    "        x, y = wave.test_set()\n",
    "        loss_test = F.mse_loss(net(V(x[:, None])), V(y))\n",
    "        return loss.data.cpu().numpy()[0], loss_test.data.cpu().numpy()[0]\n",
    "    return loss.data.cpu().numpy()#[0]\n",
    "\n",
    "# copy the model, used for meta-learning\n",
    "def copy_sine_model(model):\n",
    "    m = SineModel()\n",
    "    m.copy(model)\n",
    "    return m\n",
    "\n",
    "# evaluate the model\n",
    "def eval_sine_test(model, test, fits=(0, 1), lr=0.01):\n",
    "    xtest, ytest = test.test_set()\n",
    "    xtrain, ytrain = test.training_set()\n",
    "\n",
    "    model = copy_sine_model(model)\n",
    "    # Not sure if this should be Adam or SGD.\n",
    "    optim = torch.optim.SGD(model.params(), lr)\n",
    "        \n",
    "    def get_loss(res):\n",
    "        return F.mse_loss(res, V(ytest[:, None])).cpu().data.numpy()#[0]\n",
    "    \n",
    "    fit_res = []\n",
    "    if 0 in fits:\n",
    "        results = model(V(xtest[:, None]))\n",
    "        fit_res.append((0, results, get_loss(results)))\n",
    "    for i in range(np.max(fits)):\n",
    "        sine_fit1(model, test, optim)\n",
    "        if i + 1 in fits:\n",
    "            results = model(V(xtest[:, None]))\n",
    "            fit_res.append(\n",
    "                (\n",
    "                    i + 1, \n",
    "                    results,\n",
    "                    get_loss(results)\n",
    "                )\n",
    "            )\n",
    "\n",
    "    return fit_res\n",
    "\n",
    "# doing the evaluation and plot the result\n",
    "def plot_sine_test(model, test, fits=(0, 1), lr=0.01):\n",
    "    xtest, ytest = test.test_set()\n",
    "    xtrain, ytrain = test.training_set()\n",
    "\n",
    "    fit_res = eval_sine_test(model, test, fits, lr)\n",
    "    \n",
    "    train, = plt.plot(xtrain.numpy(), ytrain.numpy(), '^')\n",
    "    ground_truth, = plt.plot(xtest.numpy(), ytest.numpy())\n",
    "    plots = [train, ground_truth]\n",
    "    legend = ['Training Points', 'True Function']\n",
    "    for n, res, loss in fit_res:\n",
    "        cur, = plt.plot(xtest.numpy(), res.cpu().data.numpy()[:, 0], '--')\n",
    "        plots.append(cur)\n",
    "        legend.append(f'After {n} Steps')\n",
    "    plt.legend(plots, legend)\n",
    "    plt.show()\n",
    "\n",
    "# visualize more concrete results\n",
    "def plot_sine_learning(models, sine_test, fits=(0, 1), lr=0.01, marker='s', linestyle='--'):\n",
    "    data = {'model': [], 'fits': [], 'loss': [], 'set': []}\n",
    "    for name, models in models:\n",
    "        if not isinstance(models, list):\n",
    "            models = [models]\n",
    "        for n_model, model in enumerate(models):\n",
    "            for n_test, test in enumerate(sine_test):\n",
    "                n_test = n_model * len(sine_test) + n_test\n",
    "                fit_res = eval_sine_test(model, test, fits, lr)\n",
    "                for n, _, loss in fit_res:\n",
    "                    data['model'].append(name)\n",
    "                    data['fits'].append(n)\n",
    "                    data['loss'].append(loss)\n",
    "                    data['set'].append(n_test)\n",
    "    data['loss'] = list(np.array(data['loss']))\n",
    "    sbs.lineplot(data=pd.DataFrame(data), x='fits', \n",
    "             y='loss', hue='model', \n",
    "             linestyle=linestyle,\n",
    "             markers=marker,estimator=np.mean)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下代码用于构建元任务、开展元训练并执行元测试,具体而言,在元训练和元测试阶段,操作流程如下:\n",
    "- 首先创建一个初始权重的副本用于赋值,同时`.copy()`方法可以切断复制后用于内循环的权重和元模型权重的计算图\n",
    "- 在副本上对随机任务运行一次梯度下降迭代\n",
    "- 将测试集上的损失通过梯度下降迭代反向传播回初始权重，这样我们就可以在使初始权重更容易更新的方向上更新初始权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 1. Build Meta-Learning Task\n",
    "# ===============================\n",
    "\n",
    "TRAIN_SIZE = 10000\n",
    "TEST_SIZE = 1000\n",
    "\n",
    "SINE_TRAIN = [SineWaveTask() for _ in range(TRAIN_SIZE)]\n",
    "SINE_TEST = [SineWaveTask() for _ in range(TEST_SIZE)]\n",
    "\n",
    "# ===============================\n",
    "# 2. Perform Meta-Training\n",
    "# ===============================\n",
    "\n",
    "def maml_sine(model, epochs, lr_inner=0.01, batch_size=1, first_order=False):\n",
    "    optimizer = torch.optim.Adam(model.params())\n",
    "    \n",
    "    for _ in tqdm(range(epochs)):\n",
    "        # Note: the paper doesn't specify the meta-batch size for this task,\n",
    "        # so I just use 1 for now.\n",
    "        for i, t in enumerate(random.sample(SINE_TRAIN, len(SINE_TRAIN))):\n",
    "            new_model = SineModel()\n",
    "            new_model.copy(model, same_var=True)\n",
    "            loss = sine_fit1(new_model, t, create_graph=not first_order)\n",
    "            for name, param in new_model.named_params():\n",
    "                grad = param.grad\n",
    "                if first_order:\n",
    "                    grad = V(grad.detach().data)\n",
    "                new_model.set_param(name, param - lr_inner * grad)\n",
    "                        \n",
    "            sine_fit1(new_model, t, force_new=True)\n",
    "\n",
    "            if (i + 1) % batch_size == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                    \n",
    "SINE_MAML = [SineModel() for _ in range(5)]\n",
    "\n",
    "for m in SINE_MAML:\n",
    "    maml_sine(m, 4)\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# 3. Perform Meta-Testing and Visualize Results\n",
    "# ===============================\n",
    "\n",
    "plot_sine_test(SINE_MAML[0], SINE_TEST[0], fits=[0, 1, 10], lr=0.01)\n",
    "plt.show()\n",
    "\n",
    "plot_sine_learning(\n",
    "    [('MAML', SINE_MAML[0]), ('Random', SineModel())],\n",
    "    SINE_TEST,\n",
    "    list(range(10)),\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "plot_sine_test(SINE_MAML[0], ONE_SIDED_EXAMPLE, fits=[0, 1, 10], lr=0.01)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上是元学习在**SineRegression**任务上的表现,在`outline.md`当中我们同时还提到了迁移学习,似乎两者都致力于学习到一个蕴含丰富知识的初始化,不过可以预期的是,`Transfer Learning`的方法完全不适用于**SineRegression**任务,我们可以简单的验证一下:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# 1. Build Transfer Learning Model\n",
    "# ===============================\n",
    "\n",
    "SINE_TRANSFER = SineModel()\n",
    "            \n",
    "# ===============================\n",
    "# 2. Perform Transfer Learning\n",
    "# ===============================\n",
    "\n",
    "def fit_transfer(epochs=1):\n",
    "    optim = torch.optim.Adam(SINE_TRANSFER.params())\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        for t in random.sample(SINE_TRAIN, len(SINE_TRAIN)):\n",
    "            sine_fit1(SINE_TRANSFER, t, optim)\n",
    "            \n",
    "fit_transfer()\n",
    "\n",
    "# ===============================\n",
    "# 3. Perform Transfer Learning and Visualize Results\n",
    "# ===============================\n",
    "\n",
    "plot_sine_test(SINE_TRANSFER, SINE_TEST[0], fits=[0, 1, 10], lr=0.02)\n",
    "\n",
    "data = plot_sine_learning(\n",
    "    [('Transfer', SINE_TRANSFER), ('MAML', SINE_MAML[0]), ('Random', SineModel())],\n",
    "    SINE_TEST,\n",
    "    list(range(100)),\n",
    "    marker='',\n",
    "    linestyle='-'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正如`outline.md`中所指出的那样,迁移学习更倾向于学习`平均化`的知识,其实这是显而易见的:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(1000):\n",
    "    SineWaveTask().plot(color='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于跨多个任务的每个$x$都有多个可能的值，如果我们训练一个单一的神经网络来同时处理多个任务，它的最佳策略就是简单地为每个$x$返回所有任务中$y$值的平均值。这会是什么样子呢？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_x, all_y = [], []\n",
    "\n",
    "for _ in range(10000):\n",
    "    curx, cury = SineWaveTask().test_set(size=100)\n",
    "    all_x.append(curx.numpy())\n",
    "    all_y.append(cury.numpy())\n",
    "\n",
    "avg, = plt.plot(all_x[0], np.mean(all_y, axis=0))\n",
    "rand, = SineWaveTask().plot()\n",
    "plt.legend([avg, rand], ['Average', 'Random'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "平均值基本上是0，这意味着在大量任务上训练的神经网络会在所有地方都简单地返回0！所以对于这个问题，MAML比迁移学习或随机初始化要好得多。\n",
    "\n",
    "然而，在之前的视线中我们使用了二阶导数,这会减慢运行的速度（根据论文，大约慢33%，这与我们这里将要看到的情况相符）。\n",
    "\n",
    "在`outline.md`中的推导部分我们已经看见一阶导数近似的方法,很巧的是,我们只要让`first_order=True`就可以使得二者数学形式匹配了.\n",
    "\n",
    "那么这个一阶近似的效果如何呢？事实证明，几乎和原始的MAML一样好！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SINE_MAML_FIRST_ORDER = [SineModel() for _ in range(5)]\n",
    "\n",
    "for m in SINE_MAML_FIRST_ORDER:\n",
    "    maml_sine(m, 4, first_order=True)\n",
    "\n",
    "plot_sine_test(SINE_MAML_FIRST_ORDER[0], SINE_TEST[0], fits=[0, 1, 10], lr=0.01)\n",
    "plt.show()\n",
    "\n",
    "plot_sine_learning(\n",
    "    [('MAML', SINE_MAML), ('MAML First Order', SINE_MAML_FIRST_ORDER)],\n",
    "    list(range(10)),\n",
    "    SINE_TEST\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "plot_sine_test(SINE_MAML_FIRST_ORDER[0], ONE_SIDED_EXAMPLE, fits=[0, 1, 10], lr=0.01)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metascore",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
