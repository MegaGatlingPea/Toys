{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> 工程实现-MetaScore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 数据加载\n",
    "\n",
    "首先，训练所需要的数据全部来自于pdbbind并且已被处理为图格式，可以直接加载:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "data_path = './dataset/pdbbind/v2020_train_dict.pt'\n",
    "data = torch.load(data_path,weights_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看一下数据组织格式：所有数据是一个大的嵌套字典，它的键是`pdb_id`，值是一个字典，其键`prot`,`lig`,`label`分别对应了蛋白质图、配体图和亲和力数据:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prot': Data(x=[83, 41], edge_index=[2, 2014], edge_attr=[2014, 5], pos=[83, 24, 3]),\n",
       " 'lig': Data(x=[33, 41], edge_index=[2, 68], edge_attr=[68, 10], pos=[33, 3]),\n",
       " 'label': '6.4'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['10gs']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "与传统机器学习不同，元学习训练的数据单元是任务而非单个数据点，因此需要对pdbbind中的蛋白-配体对数据进行任务划分，在MetaScore中表现为对蛋白质结构相似度进行层次聚类，以聚类得到的簇为单个任务进行训练。对应的，`DataLoader`的写法也相应的改变：\n",
    "\n",
    "- `DataLoader`一次从所有任务中采样`batch_size`个任务\n",
    "- 每个任务应当采样`num_classes_per_task`个类别，即选取不同的聚类用于构建一个任务\n",
    "- 每个聚类中随机采样`k-shot`个样本用于构建支持集，`q-query`个样本用于构建查询集\n",
    "- 将一个任务中所有聚类中被选中的支持集样本和查询集样本聚合后，即可视为`meta-batch`中的一个任务\n",
    "- 考虑到显存问题，无法再将`meta-batch`中的任务也做一遍聚合然后一次算完（显存会爆炸），只能在一个批次中逐任务累计梯度\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "聚类的结果已经拿到，加载一下:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PDB_ID</th>\n",
       "      <th>Cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2z7i</td>\n",
       "      <td>257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3i7g</td>\n",
       "      <td>333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2v88</td>\n",
       "      <td>197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3wav</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6qlu</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  PDB_ID  Cluster\n",
       "0   2z7i      257\n",
       "1   3i7g      333\n",
       "2   2v88      197\n",
       "3   3wav      150\n",
       "4   6qlu        0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "cluster_path = './dataset/cluster/clustering_results.csv'\n",
    "cluster_df = pd.read_csv(cluster_path)\n",
    "cluster_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "而后我们根据聚类的结果来构建一个新的数据字典，在这个字典中，键应当改为`Cluster`，值应为列表，每个列表的元素代表位于该聚类下的一个数据点，以原数据中字典的格式存储（在原数据字典的基础上额外增加了`pdb_id`作为数据标识符）:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19145/19145 [00:27<00:00, 693.98it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'prot': Data(x=[38, 41], edge_index=[2, 794], edge_attr=[794, 5], pos=[38, 24, 3]),\n",
       "  'lig': Data(x=[35, 41], edge_index=[2, 72], edge_attr=[72, 10], pos=[35, 3]),\n",
       "  'label': '5.7',\n",
       "  'pdb_id': '4z0u'},\n",
       " {'prot': Data(x=[83, 41], edge_index=[2, 2264], edge_attr=[2264, 5], pos=[83, 24, 3]),\n",
       "  'lig': Data(x=[21, 41], edge_index=[2, 46], edge_attr=[46, 10], pos=[21, 3]),\n",
       "  'label': '6.0',\n",
       "  'pdb_id': '3dxm'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tqdm\n",
    "dict_with_cluster = {}\n",
    "for pdb_id, value in tqdm.tqdm(data.items()):\n",
    "    value['pdb_id'] = pdb_id\n",
    "    row = cluster_df[cluster_df['PDB_ID'] == pdb_id]\n",
    "    cluster_id = row['Cluster'].values[0]\n",
    "    if f'class_{cluster_id}' not in dict_with_cluster:\n",
    "        dict_with_cluster[f'class_{cluster_id}'] = []\n",
    "    dict_with_cluster[f'class_{cluster_id}'].append(value)\n",
    "dict_with_cluster['class_1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在此结果基础上，考虑划分元学习的`meta-train`和`meta-test`集，在`MetaScore`中，考虑到降低实现难度，该集的划分以每个聚类中数据点的数量划分，具体的，不少于17个数据点的聚类会被分类为训练集，不少于7个数据点的聚类分类为测试集，其他聚类暂时丢弃，或考虑用于模型`zeroshot`性能测试（这部分聚类数量很多但是数据量很小，可以丢弃而不造成严重影响）。为此可以写一个`dict_with_split`，存储每个聚类属于的集合:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 186 classes, Val: 153 classes, Test: 808 classes\n",
      "data length of each split {'train': 15810, 'val': 1574, 'test': 1761}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "train_set_min_size = 17\n",
    "val_set_min_size = 7\n",
    "dict_with_split = {'train': [], 'val': [], 'test': []}\n",
    "for class_idx, value in dict_with_cluster.items():\n",
    "    data_count = len(value)\n",
    "    if data_count >= train_set_min_size:\n",
    "        dict_with_split['train'].append(class_idx)\n",
    "    elif data_count >= val_set_min_size and data_count < train_set_min_size:\n",
    "        dict_with_split['val'].append(class_idx)\n",
    "    else:\n",
    "        dict_with_split['test'].append(class_idx)\n",
    "print(f\"Train: {len(dict_with_split['train'])} classes, Val: {len(dict_with_split['val'])} classes, Test: {len(dict_with_split['test'])} classes\")\n",
    "dataset_size_dict = {\n",
    "    \"train\": {key: len(dict_with_cluster[key]) for key in dict_with_split[\"train\"]},\n",
    "    \"val\": {key: len(dict_with_cluster[key]) for key in dict_with_split[\"val\"]},\n",
    "    \"test\": {key: len(dict_with_cluster[key]) for key in dict_with_split[\"test\"]},\n",
    "}\n",
    "data_length = {name: np.sum(np.array(list(dataset_size_dict[name].values()))) for name in dict_with_split.keys()}\n",
    "print(\"data length of each split\", data_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "到此为止完成了数据单元（单个聚类）的数据集划分，接下来要考虑如何使用数据单元来构建`meta-batch`，这涉及到:\n",
    "- 任务构建逻辑，如何在所有聚类中选择`batch_size`个聚类构建一个任务\n",
    "- 聚类中的样本采样逻辑，如何在一个聚类中选取样本构建支持集和查询集\n",
    "这两点可以通过一个`TaskSampler`类来完成:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskSampler:\n",
    "    \"\"\"\n",
    "    Samples tasks based on specified rules.\n",
    "    Can select classes uniformly or based on class size using softmax probability.\n",
    "    Samples support/query items uniformly from within selected classes.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 dataset_size_dict: dict,\n",
    "                 train_num_classes_per_set: int,\n",
    "                 val_num_classes_per_set: int,\n",
    "                 train_num_support: int,\n",
    "                 val_num_support: int,\n",
    "                 train_num_query: int,\n",
    "                 val_num_query: int,\n",
    "                 sampling_rule: str = \"uniform\"): # Added sampling_rule parameter\n",
    "        \"\"\"\n",
    "        Initializes the TaskSampler.\n",
    "\n",
    "        Args:\n",
    "            dataset_size_dict: Dictionary mapping split names ('train', 'val') to dictionaries\n",
    "                               of {class_name: class_size}.\n",
    "            train_num_classes_per_set: Number of classes per task in the training set.\n",
    "            val_num_classes_per_set: Number of classes per task in the validation set.\n",
    "            train_num_support: Number of support examples per class in the training set.\n",
    "            val_num_support: Number of support examples per class in the validation set.\n",
    "            train_num_query: Number of query examples per class in the training set.\n",
    "            val_num_query: Number of query examples per class in the validation set.\n",
    "            sampling_rule: Rule for sampling classes ('uniform' or 'softmax'). Defaults to 'uniform'.\n",
    "        \"\"\"\n",
    "        self.dataset_size_dict = dataset_size_dict\n",
    "        self.sampling_rule = sampling_rule # Store the rule, for now only uniform sampling is implemented\n",
    "        # determine the number of clusters to sample for each task\n",
    "        self.num_classes_per_set = {\n",
    "            \"train\": train_num_classes_per_set,\n",
    "            \"val\": val_num_classes_per_set,\n",
    "        }\n",
    "        # determine the number of support samples to sample for each class\n",
    "        self.num_support = {\n",
    "            \"train\": train_num_support,\n",
    "            \"val\": val_num_support,\n",
    "        }\n",
    "        # determine the number of query samples to sample for each class\n",
    "        self.num_query = {\n",
    "            \"train\": train_num_query,\n",
    "            \"val\": val_num_query,\n",
    "        }\n",
    "        # check if the sampling rule is valid\n",
    "        if sampling_rule not in [\"uniform\", \"softmax\"]:\n",
    "            raise ValueError(\"Invalid sampling_rule. Must be 'uniform' or 'softmax'.\")\n",
    "\n",
    "    def sample_task(self, dataset_name: str, seed: int) -> dict:\n",
    "        \"\"\"\n",
    "        Samples a task based on the configured sampling rule.\n",
    "\n",
    "        Args:\n",
    "            dataset_name: 'train' or 'val'.\n",
    "            seed: Random seed for reproducibility.\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with sampled class names and their support/query indices.\n",
    "            Example: {'class_A': {'support_indices': [...], 'query_indices': [...]}, ...}\n",
    "        \"\"\"\n",
    "        if dataset_name not in [\"train\", \"val\"]:\n",
    "            raise ValueError(\"Invalid dataset_name. Must be 'train' or 'val'.\")\n",
    "        rng = np.random.RandomState(seed)\n",
    "        num_classes = self.num_classes_per_set[dataset_name]\n",
    "        num_support = self.num_support[dataset_name]\n",
    "        num_query = self.num_query[dataset_name]\n",
    "        num_samples_per_class = num_support + num_query\n",
    "        available_classes_dict = self.dataset_size_dict[dataset_name]\n",
    "        available_class_names = list(available_classes_dict.keys())\n",
    "        if len(available_class_names) < num_classes:\n",
    "            raise ValueError(\n",
    "                f\"Not enough classes in {dataset_name} split ({len(available_class_names)}) \"\n",
    "                f\"to sample {num_classes} classes.\"\n",
    "            )\n",
    "        # Select classes based on the sampling rule\n",
    "        if self.sampling_rule == \"uniform\":\n",
    "            selected_classes = rng.choice(\n",
    "                available_class_names,\n",
    "                size=num_classes,\n",
    "                replace=False,\n",
    "            )\n",
    "        elif self.sampling_rule == \"softmax\":\n",
    "            raise NotImplementedError(\"Softmax sampling rule not implemented\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown sampling rule: {self.sampling_rule}\")\n",
    "        task_info = {}\n",
    "        for class_name in selected_classes:\n",
    "            class_size = available_classes_dict[class_name] # Use the dict directly\n",
    "            if class_size < num_samples_per_class:\n",
    "                 raise ValueError(\n",
    "                    f\"Class '{class_name}' in '{dataset_name}' has only {class_size} samples, \"\n",
    "                    f\"but {num_samples_per_class} are required for support+query.\"\n",
    "                )\n",
    "            # Sample indices from 0 to class_size - 1\n",
    "            selected_indices = rng.choice(\n",
    "                class_size,\n",
    "                size=num_samples_per_class,\n",
    "                replace=False # Should not sample the same item twice for one task\n",
    "            )\n",
    "            task_info[class_name] = {\n",
    "                'support_indices': selected_indices[:num_support],\n",
    "                'query_indices': selected_indices[num_support : num_support + num_query]\n",
    "            }\n",
    "        return task_info\n",
    "\n",
    "task_sampler = TaskSampler(\n",
    "    dataset_size_dict=dataset_size_dict,\n",
    "    train_num_classes_per_set=5,\n",
    "    val_num_classes_per_set=16,\n",
    "    train_num_support=5,\n",
    "    val_num_support=3,\n",
    "    train_num_query=5,\n",
    "    val_num_query=3,\n",
    "    sampling_rule=\"uniform\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这是一个非常简单的采样实现（也是`MAML`论文中的实现），基于这个`TaskSampler`尝试采样一批任务:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 5, 3, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_train_task_info = task_sampler.sample_task(dataset_name='train', seed=1227);sampled_val_task_info = task_sampler.sample_task(dataset_name='val', seed=1227)\n",
    "train_support_set_data = [];train_query_set_data = [];val_support_set_data = [];val_query_set_data = []\n",
    "for sampled_train_class_name in sampled_train_task_info.keys():\n",
    "    train_class_data_list = dict_with_cluster[sampled_train_class_name]\n",
    "    support_indices = sampled_train_task_info[sampled_train_class_name]['support_indices']\n",
    "    query_indices = sampled_train_task_info[sampled_train_class_name]['query_indices']\n",
    "    per_class_support_data = [train_class_data_list[i] for i in support_indices]\n",
    "    train_support_set_data.append(per_class_support_data)\n",
    "    per_class_query_data = [train_class_data_list[i] for i in query_indices]\n",
    "    train_query_set_data.append(per_class_query_data)\n",
    "for sampled_val_class_name in sampled_val_task_info.keys():\n",
    "    val_class_data_list = dict_with_cluster[sampled_val_class_name]\n",
    "    support_indices = sampled_val_task_info[sampled_val_class_name]['support_indices']\n",
    "    query_indices = sampled_val_task_info[sampled_val_class_name]['query_indices']\n",
    "    per_class_support_data = [val_class_data_list[i] for i in support_indices]\n",
    "    val_support_set_data.append(per_class_support_data)\n",
    "    per_class_query_data = [val_class_data_list[i] for i in query_indices]\n",
    "    val_query_set_data.append(per_class_query_data)\n",
    "\n",
    "# thus a task is sampled, data is organized as follows:\n",
    "# (5,5,16,16), refer to the number of classes sampled \n",
    "len(train_support_set_data),len(train_query_set_data),len(val_support_set_data),len(val_query_set_data)\n",
    "# (5,5,3,3), refer to the number of support and query samples sampled for each class\n",
    "len(train_support_set_data[0]),len(train_query_set_data[0]),len(val_support_set_data[0]),len(val_query_set_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目前采样得到的任务组织比较混乱，考虑用字典使任务变得更加清晰，可以定义一个`transform`函数实现这一点，同时，该函数可以用于定义`__getitem__`方法，即获得一个数据单元，在此我们通过复制任务来得到一个`meta-batch`方便后续演示:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(data):\n",
    "    \"\"\"Transforms the raw data list from get_set into separate lists.\"\"\"\n",
    "    pdb_ids = []\n",
    "    labels = []\n",
    "    prots = []\n",
    "    ligs = []\n",
    "    for class_data_list in data:\n",
    "        for per_pdb_data_dict in class_data_list:\n",
    "            pdb_ids.append(per_pdb_data_dict['pdb_id'])\n",
    "            labels.append(float(per_pdb_data_dict['label']))\n",
    "            prots.append(per_pdb_data_dict['prot'])\n",
    "            ligs.append(per_pdb_data_dict['lig'])\n",
    "    return {\n",
    "        'pdb_ids': pdb_ids,\n",
    "        'labels': labels,\n",
    "        'prots': prots,\n",
    "        'ligs': ligs,\n",
    "    }\n",
    "# (25,25,48,48), 25 = 5*5, 48 = 16*3\n",
    "# train_support_set_data['pdb_ids'].__len__(),train_query_set_data['pdb_ids'].__len__(),val_support_set_data['pdb_ids'].__len__(),val_query_set_data['pdb_ids'].__len__()\n",
    "train_support_set_data = transform(train_support_set_data);train_query_set_data = transform(train_query_set_data)\n",
    "val_support_set_data = transform(val_support_set_data);val_query_set_data = transform(val_query_set_data)\n",
    "train_task = (train_support_set_data,train_query_set_data,1227); val_task = (val_support_set_data,val_query_set_data,1227)\n",
    "# BATCH_SIZE = 4, simply repeat the task for 4 times to get a batch\n",
    "train_batch = [train_task for _ in range(4)];val_batch = [val_task for _ in range(4)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了让数据格式更加工整，考虑写一个自定义的`task_collate_fn`来实现这一点，最后整合好的`meta-batch`是一个长度为`batch_size`的列表，列表的每个元素是一个字典:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Batch\n",
    "def task_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Collate function for creating task batches with graph data.\n",
    "    Processes and collates support and query sets into batches.\n",
    "    Input `batch`: A list of tuples, where each tuple is (support_set_dict, query_set_dict, seed)\n",
    "                    returned by FewShotLearningDatasetParallel.__getitem__.\n",
    "    \"\"\"\n",
    "    all_tasks = []\n",
    "    \n",
    "    for task in batch:\n",
    "        # Unpack the task tuple\n",
    "        support_set_dict, query_set_dict, seed = task\n",
    "\n",
    "        # Process Support Set\n",
    "        support_ligs = support_set_dict['ligs'] # List of ligand graph objects\n",
    "        support_prots = support_set_dict['prots'] # List of protein graph objects\n",
    "        support_labels = support_set_dict['labels']\n",
    "        support_pdb_ids = support_set_dict['pdb_ids']\n",
    "        \n",
    "        query_ligs = query_set_dict['ligs'] # List of ligand graph objects\n",
    "        query_prots = query_set_dict['prots'] # List of protein graph objects\n",
    "        query_labels = query_set_dict['labels']\n",
    "        query_pdb_ids = query_set_dict['pdb_ids']\n",
    "        \n",
    "        task_data = {\n",
    "            \"support\": {\n",
    "                \"prots\": Batch.from_data_list(support_prots),\n",
    "                \"ligs\": Batch.from_data_list(support_ligs),\n",
    "                \"labels\": torch.tensor(support_labels, dtype=torch.float).reshape(-1),\n",
    "                \"pdb_ids\": support_pdb_ids,\n",
    "            },\n",
    "            \"query\": {\n",
    "                \"prots\": Batch.from_data_list(query_prots),\n",
    "                \"ligs\": Batch.from_data_list(query_ligs),\n",
    "                \"labels\": torch.tensor(query_labels, dtype=torch.float).reshape(-1),\n",
    "                \"pdb_ids\": query_pdb_ids,\n",
    "            },\n",
    "            \"seed\": seed,\n",
    "        }\n",
    "        all_tasks.append(task_data)\n",
    "    return all_tasks\n",
    "\n",
    "# successfully get a meta-batch for training and validation\n",
    "train_batch = task_collate_fn(train_batch);val_batch = task_collate_fn(val_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 基础模型的定义\n",
    "\n",
    "该部分定义了神经网络的具体架构，实现与传统机器学习并无不同，后续考虑收录一些基于`MAML`设计模型的技巧在里面，目前相关的工作有:\n",
    "\n",
    "- [MetaMolGen: A Neural Graph Motif Generation Model for De Novo Molecular Design](https://arxiv.org/abs/2504.15587)\n",
    "- [Meta-MGNN: Few-Shot Graph Learning for Molecular Property Prediction](https://arxiv.org/abs/2102.07916)\n",
    "  \n",
    "该部分工作持续收录中~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此处我们就用预先写好的`./models/gatedgcn.py`中定义好的权重以及参数`./models/gatedgcn.pt`进行模型初始化，并且简单测试一下基础模型是否能够工作:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load meta-model\n",
      "Predicted Gaussian Mixture Weights: torch.Size([64953, 10])\n",
      "Predicted Gaussian Mixture Standard Deviations: torch.Size([64953, 10])\n",
      "Predicted Gaussian Mixture Means: torch.Size([64953, 10])\n",
      "Precalculated Closest Distance: torch.Size([64953, 1])\n",
      "Predicted Atom Types: torch.Size([763, 17])\n",
      "Predicted Bond Types: torch.Size([1638, 4])\n",
      "Batch Index: torch.Size([64953])\n",
      "Total Loss: 0.9454381367967772\n",
      "MDN Loss: 0.9453015327453613\n",
      "Affinity Pearson Correlation: 0.6828477382659912\n",
      "Atom Loss: 0.0032243274617940187\n",
      "Bond Loss: 0.13338115811347961\n",
      "Predicted Scores: tensor([208.2487, 230.9851, 212.2064, 135.1499, 214.3612, 107.6525, 157.2823,\n",
      "         51.0062, 168.1118, 168.4160,  78.0566,  50.8879,  69.3069,  93.9391,\n",
      "        208.8820, 115.5764, 169.4495, 150.0120, 112.4466, 166.9707,  55.4113,\n",
      "        129.6583, 181.6872,  73.9615, 121.3966], dtype=torch.float64)\n",
      "Batch Index: tensor([ 0,  0,  0,  ..., 24, 24, 24])\n"
     ]
    }
   ],
   "source": [
    "from models.gatedgcn import GenScore_GGCN, mdn_loss_fn\n",
    "import torch\n",
    "ft = False\n",
    "state_dict = torch.load('./models/gatedgcn.pt',weights_only=False)\n",
    "meta_model_state_dict = state_dict['network'] # meta_model_state_dict, with a 'regressor.' prefix\n",
    "# Correctly strip the prefix, ensuring the new key is not empty\n",
    "model_state_dict = {\n",
    "    new_key: v \n",
    "    for k, v in meta_model_state_dict.items() \n",
    "    if k.startswith('regressor.') and (new_key := k[len('regressor.'):])\n",
    "}\n",
    "base_model = GenScore_GGCN()\n",
    "if ft is False:\n",
    "    print('load meta-model')\n",
    "    base_model.load_state_dict(model_state_dict)\n",
    "else:\n",
    "    print('load non-meta-model')\n",
    "    base_model.load_state_dict(torch.load('./models/gatedgcn_nonmeta.pth',weights_only=False)['model_state_dict'])\n",
    "support_train_prot_eg = train_batch[0]['support']['prots']\n",
    "support_train_lig_eg = train_batch[0]['support']['ligs']\n",
    "pi, sigma, mu, dist, atom_types, bond_types, C_batch =base_model.net_forward(support_train_lig_eg,support_train_prot_eg)\n",
    "print(f\"Predicted Gaussian Mixture Weights: {pi.shape}\\nPredicted Gaussian Mixture Standard Deviations: {sigma.shape}\\nPredicted Gaussian Mixture Means: {mu.shape}\\nPrecalculated Closest Distance: {dist.shape}\\nPredicted Atom Types: {atom_types.shape}\\nPredicted Bond Types: {bond_types.shape}\\nBatch Index: {C_batch.shape}\")\n",
    "total_loss, mdn_loss, affi_loss, atom_loss, bond_loss, y, batch = \\\n",
    "    base_model.forward(train_batch[0]['support'])\n",
    "print(f\"Total Loss: {total_loss}\\nMDN Loss: {mdn_loss}\\nAffinity Pearson Correlation: {affi_loss}\\nAtom Loss: {atom_loss}\\nBond Loss: {bond_loss}\\nPredicted Scores: {y}\\nBatch Index: {batch}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent! 接下来就要上元学习的核心逻辑了，MetaScore采用的元学习方法是基于优化的`MAML`及其变体`MAML++`（后续考虑继续做变体，比如`Task Based Attention Mechanism`等），其介绍可以详细看同目录下的`MetaLearning.md`，下面主要强调其工程上的实现（主要基于`higher`库，比较方便，所有已经写好的并非用于元学习的模型通过`higher`的帮助均可以轻松改造为元模型）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 元学习框架设计"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了将元学习框架套用到`GenScore_GGCN`模型上，我们需要额外定义一个超类`MAMLRegressor`:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metascore",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
